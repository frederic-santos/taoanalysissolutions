\documentclass[11pt]{article}
\title{Propositions of solutions for \textit{Analysis II} by Terence Tao}
\author{Frédéric Santos}
% General packages:
\usepackage{a4wide}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{enumitem}
% Fonts and math packages:
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\numberwithin{equation}{section}
\usepackage[matha,mathb,mathx]{mathabx}
\usepackage{mbboard}
\usepackage{stmaryrd}
\usepackage{hyperref}
% Macros:
\newcommand{\successor}[1]{#1 \! +\!\!\!+}
\newcommand{\aval}[1]{\left\lvert #1 \right\rvert}
\newcommand{\intset}[2]{\llbracket #1, #2 \rrbracket}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\partsof}[1]{\mathcal{P}\left( #1 \right)}
\newcommand{\minus}{\, \textrm{---}\!\textrm{--} \:}
\newcommand{\quot}{\, \textrm{/}\!\textrm{/} \:}
\newcommand{\nn}{\mathbb{N}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\qq}{\mathbb{Q}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\formallimit}[1]{\text{LIM}_{n \to \infty} #1}
\newcommand{\seq}[2]{(#1)_{n=#2}^\infty}
\newcommand{\limit}[1]{\text{lim}_{n \to \infty} #1}
\newcommand{\extrr}{\overline{\rr}}
\newcommand{\adh}[1]{\overline{#1}}
\newcommand{\liminfp}[2]{\inf (#1^+_N)_{N=#2}^{\infty}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\Q}{\mathbf{Q}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\infint}[2]{\underline{\int}_{#2} \, #1}
\newcommand{\supint}[2]{\overline{\int}_{#2} \, #1}
\newcommand{\ddisc}{d_{\text{disc}}}
\newcommand{\inter}{\text{int}}
\newcommand{\ext}{\text{ext}}
\newcommand{\ninf}[1]{\|#1\|_\infty}
\newcommand{\vecn}[1]{(#1_i)_{1 \leq i \leq n}}
% Lemmas:
\usepackage{amsthm}
\newtheorem*{lem}{Lemma}
\newtheorem*{theorem}{Theorem}
% Environment:
\newenvironment{exo}[2]{\noindent \textsc{Exercise #1}. ---
  \textit{#2} \vspace{3mm}}

%%%%%%%%%%%%%%
%%% Begin doc:
\begin{document}
\maketitle
\tableofcontents

\vskip 15mm

\noindent \textbf{Remarks.} The numbering of the Exercises follows the
fourth edition of \textit{Analysis II}. In order to make the
references to \textit{Analysis I} easier, we consider that we begin
with Chapter 12 here, as in earlier editions of the textbook. Thus, in
particular, a reference to ``Exercise 4.3.3'' (for instance) will
always mean ``Exercise 4.3.3 from \textit{Analysis I}''.

\pagebreak
\setcounter{section}{11}
\section{Metric spaces}
\label{sec:metric-spaces}
\begin{exo}{12.1.1}{Prove Lemma 12.1.1}

  Consider the sequence $\seq{a_n}{m}$ defined by
  $a_n := d(x_n, x) = |x_n - x|$ for all $n \geq m$. We have to prove
  that $\lim_{n \to \infty} a_n = 0$ if and only if
  $\lim_{n \to \infty} x_n = x$.

  \begin{itemize}
  \item Let be $\epsilon > 0$. If $\lim_{n \to \infty} a_n = 0$, then
    there exists an $N \geq m$ such that $|a_n| < \epsilon$ whenever
    $n \geq N$. Thus, there exists an $N \geq m$ such that
    $|x_n - x| < \epsilon$ whenever $n \geq N$, which means that
    $\lim_{n \to \infty} x_n = x$.
  \item Let be $\epsilon > 0$. Conversely, if $\lim_{n \to \infty} x_n
    = x$, then there exists an $N \geq m$ such that $|x_n - x| <
    \epsilon$ whenever $n \geq N$. But since $|a_n| := |x_n - x|$, it
    means that $\lim_{n \to \infty} a_n = 0$, as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.2}{Show that the real line with the metric $d(x, y) :=
    |x-y|$ is indeed a metric space.}

  Using Proposition 4.3.3, this claim is obvious. All claims (a)--(d)
  of Definition 12.1.2 are satisfied because:
  \begin{enumerate}[label=(\alph*)]
  \item comes from Proposition 4.3.3(e)
  \item also comes from Proposition 4.3.3(e)
  \item comes from Proposition 4.3.3(f)
  \item comes from Proposition 4.3.3(g).
  \end{enumerate}
\end{exo}

\begin{exo}{12.1.3}{Let $X$ be a set, and let
    $d : X \times X \to [0, \infty)$ be a function. With respect to
    Definition 12.1.2, give an example of a pair $(X,d)$ which...
    \vskip -4mm}
  
  \begin{enumerate}[label=(\alph*)]
  \item obeys the axioms (bcd) but not (a).

    Consider $X = \rr$, and $d$ defined by $d(x,x) = 1$ and $d(x,y) =
    5$ for all $x \neq y \in \rr$.
  \item obeys the axioms (acd) but not (b).

    Consider $X = \rr$, and $d$ defined by $d(x,y) = 0$ for all $x, y
    \in \rr$.
  \item obeys the axioms (abd) but not (c).

    Consider $X = \rr$, and $d$ defined by $d(x,y) = \max(x-y,0)$ for
    all $x,y \in \rr$.
  \item obeys the axioms (abc) but not (d).

    Consider the finite set $X := \{1,2,3\}$ and the application $d$
    defined by $d(1,2) = d(2,1) = d(2,3) = d(3,2) := 1$, and
    $d(1,3) = d(3,1) := 5$, and $d(x,x) = 0$ for all $x \in X$.
  \end{enumerate}  
\end{exo}

\begin{exo}{12.1.4}{Show that the pair $(Y, d|_{Y \times Y})$ deﬁned
    in Example 12.1.5 is indeed a metric space.}

  By definition, since $Y \subseteq X$, we have $x,y \in X$ whenever
  $x,y \in Y$. And furthermore, since $d|_{Y \times Y}(x,y) :=
  d(x,y)$, then the application $d|_{Y \times Y}$ obeys all four
  statements (a)--(d) of Definition 12.1.2. Thus, $(Y, d|_{Y \times
    Y})$ is indeed a metric space.
\end{exo}

\begin{exo}{12.1.5}{Let $n \geq 1$, and let $a_1, a_2, \ldots, a_n$
    and $b_1, b_2, \ldots, b_n$ be real numbers. Verify the identity
    $\left(\sum_{i=1}^n a_i b_i\right)^2 + \frac{1}{2} \sum_{i=1}^{n}
    \sum_{j=1}^{n} (a_i b_j - a_j b_i)^2 = \sum_{i=1}^{n} a_i^2
    \sum_{j=1}^{n} b_j^2$, and conclude the Cauchy-Schwarz inequality.
    Then use the Cauchy-Schwarz inequality to prove the triangle
    inequality.}

  Let's prove these three statements.

  \begin{enumerate}[label=(\roman*)]
  \item To prove the first identity, let's use induction on $n$.

    The base case $n=1$ is obvious: on the left-hand side, we just get
    $(a_1 b_1)^2$, and on the right-hand side, we get $a_1^2 b_1^2$,
    hence the statement.

    Now let's suppose inductively that this identity is true for a
    given positive integer $n \geq 1$, and let's prove that it is
    still true for $n+1$. We have to prove that
    \begin{equation}
      \label{eq:12.1.5a}
      \underbrace{\left(\sum_{i=1}^{n+1} a_i b_i\right)^2}_{:=A} +
      \underbrace{\frac{1}{2} \sum_{i=1}^{n+1} \sum_{j=1}^{n+1} (a_i
        b_j - a_j b_i)^2}_{:= B}
      = \underbrace{\left(\sum_{i=1}^{n+1} a_i^2\right)
        \left(\sum_{j=1}^{n+1} b_j^2\right)}_{:=C}
    \end{equation}
    where we gave a name to each part of the identity for an easier
    computation below. Indeed,
    \begin{itemize}
    \item for $A$, we have
      \begin{align*}
        A &:= \left(\sum_{i=1}^{n+1} a_i b_i\right)^2 \\
          &= \left(a_{n+1} b_{n+1} + \sum_{i=1}^{n} a_i b_i\right)^2
        \\
          &= (a_{n+1} b_{n+1})^2 + \left(\sum_{i=1}^{n} a_i
            b_i\right)^2 + 2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i
      \end{align*}
      
    \item for $B$, we have
      \begin{align*}
        B &:= \frac{1}{2} \sum_{i=1}^{n+1} \sum_{j=1}^{n+1} (a_i
            b_j - a_j b_i)^2 \\
          &= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n+1} (a_i b_j -
            a_j b_i)^2 + \frac{1}{2} \sum_{j=1}^{n+1} (a_{n+1} b_j -
            a_j b_{n+1})^2\\
          &= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
            b_i)^2 +
            \underbrace{\frac{1}{2} \sum_{i=1}^{n} (a_i b_{n+1} -
            a_{n+1} b_i)^2}_{:= 1/2 \times S} +
            \underbrace{\frac{1}{2} \sum_{j=1}^{n} (a_{n+1} b_{j} - a_{j}
            b_{n+1})^2}_{:= 1/2 \times S} \\
          &\quad + \underbrace{\frac{1}{2} (a_{n+1}b_{n+1} -
            b_{n+1}a_{n+1})^2}_{=0} \\
          &= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
            b_i)^2 + \sum_{k=1}^{n} (a_kb_{n+1} -
            a_{n+1}b_k)^2
      \end{align*}
      
    \item and thus, for $A + B$, we now use the induction hypothesis (IH)
      to get:
      \begin{align*}
        A + B
        &:= (a_{n+1} b_{n+1})^2 + \left(\sum_{i=1}^{n} a_i
          b_i\right)^2 +
          2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i\\
        &\quad + \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
          b_i)^2 + \sum_{k=1}^{n} (a_kb_{n+1} - a_{n+1}b_k)^2\\
        &= \underbrace{\left(\sum_{i=1}^{n} a_i b_i\right)^2 +
          \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
          b_i)^2}_{\text{apply (IH) here}} \\
        &\quad + (a_{n+1} b_{n+1})^2 +
          2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i
          + \sum_{k=1}^{n} (a_kb_{n+1} - a_{n+1}b_k)^2\\
        &= \left(\sum_{i=1}^{n} a_i^2\right) \left(\sum_{j=1}^{n}
          b_j^2\right)\\
        &\quad + (a_{n+1} b_{n+1})^2 +
          2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i
          + \sum_{k=1}^{n} (a_kb_{n+1} - a_{n+1}b_k)^2\\
        &= \left(\sum_{i=1}^{n} a_i^2\right) \left(\sum_{j=1}^{n}
          b_j^2\right) + (a_{n+1} b_{n+1})^2\\
        &\quad + 2 \sum_{i=1}^{n} a_i a_{n+1} b_i b_{n+1} +
          \sum_{i=1}^{n}(a_i^2 b_{n+1}^2 - 2a_ib_{n+1}a_{n+1}b_i + a_{n+1}^2b_i^2)\\
        &= \left(\sum_{i=1}^{n} a_i^2\right) \left(\sum_{j=1}^{n}
          b_j^2\right) + \sum_{i=1}^{n}(a_i^2 b_{n+1}^2 + a_{n+1}^2
          b_i^2) \\
        &= \left(\sum_{i=1}^{n+1} a_i^2\right) \left(\sum_{j=1}^{n+1}
          b_j^2\right) \\
        &= C
      \end{align*}
      so that the identity is indeed true for all natural number $n$.
    \end{itemize}
    
  \item We can use this identity to prove the Cauchy-Schwarz identity,
    \begin{equation}
      \label{eq:12.1.5b}
      \left| \sum_{i=1}^{n} a_i b_i\right| \leq \left( \sum_{i=1}^{n}
        a_i^2 \right)^{1/2} \left( \sum_{i=1}^{n}
        b_i^2 \right)^{1/2}.
    \end{equation}

    Indeed, since $B \geq 0$ in the identity \eqref{eq:12.1.5a}, we
    have
    \[\left(\sum_{i=1}^{n} a_i b_i\right)^2 \leq \left(\sum_{i=1}^{n} a_i^2\right)
      \left(\sum_{j=1}^{n} b_j^2\right)\]
    and thus, taking the square root on both sides, we get
    \eqref{eq:12.1.5b}, as expected.
    
  \item Finally, we can use the Cauchy-Schwarz inequality to prove the
    triangle inequality.

    We have
    \begin{align*}
      \sum_{i=1}^{n} (a_i^2 + b_i^2)
      &= \sum_{i=1}^{n} a_i^2 + \sum_{i=1}^{n} b_i^2 + 2
        \sum_{i=1}^{n} a_i b_i
      &\\
      &\leq \sum_{i=1}^{n} a_i^2 + \sum_{i=1}^{n} b_i^2 + 2
        \left(\sum_{i=1}^{n} a_i^2\right)^{1/2} \left(\sum_{i=1}^{n}
        b_i^2\right)^{1/2}
      &\text{ (by eq. \eqref{eq:12.1.5b})}\\
      &\leq \left( \left( \sum_{i=1}^{n} a_i^2 \right)^{1/2}
        + \left( \sum_{i=1}^{n} b_i^2 \right)^{1/2}\right)^2&
    \end{align*}
    and, since everything is positive, we get the triangle inequality
    by taking square roots on both sides.
  \end{enumerate}  
\end{exo}

\begin{exo}{12.1.6}{Show that $(\rr^n , d_{l^2}$) in Example 12.1.6 is
    indeed a metric space.}

  We have to show the four axioms of Definition 12.1.2.

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in \rr^n$, we have
    $d_{l^2}(x,x) = \sqrt{\sum_{i=1}^{n} (x_i - x_i)^2} = 0$, as expected.
  \item Positivity: for all $x \neq y \in \rr^n$, there exists at
    least one $1 \leq i \leq n$ such that $x_i \neq y_i$, so that
    $(x_i-y_i)^2 > 0$, and
    $d_{l^2}(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} > 0$, as
    expected.
  \item Symmetry: for all $x,y \in \rr^n$, we have
    \[d_{l^2}(y,x) = \sqrt{\sum_{i=1}^{n} (y_i - x_i)^2} =
      \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} = d_{l^2}(x,y)\] as expected.
  \item Triangle inequality: for all $x,y,z \in \rr^n$, we have
    \begin{align*}
      d_{l^2}(x,z)
      &:= \left(\sum_{i=1}^{n} (x_i - z_i)^2\right)^{1/2} &\\
      &= \left(\sum_{i=1}^{n} (a_i + b_i)^2\right)^{1/2}
      &\text{with $a_i := x_i-y_i$ and $b_i := y_i-z_i$}\\
      &\leq \left(\sum_{i=1}^{n} a_i^2\right)^{1/2} +
        \left(\sum_{i=1}^{n} b_i^2\right)^{1/2}
      &\text{(Exercise 12.1.5(iii))}\\
      &\leq \left(\sum_{i=1}^{n} (x_i - y_i)^2\right)^{1/2} +
        \left(\sum_{i=1}^{n} (y_i-z_i)^2\right)^{1/2}& \\
      &\leq d_{l^2}(x,y) + d_{l^2}(y,z)&
    \end{align*}
    as expected.
  \end{enumerate}
  Thus, $(\rr^n, d_{l^2})$ is indeed a metric space.
\end{exo}

\pagebreak
\begin{exo}{12.1.7}{Show that $(\rr^n , d_{l^1}$) in Example 12.1.7 is
    indeed a metric space.}

  Once again, let's show the four axioms of Definition 12.1.2.

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in \rr^n$, we have
    $d_{l^1}(x,x) = \sum_{i=1}^{n} |x_i - x_i| = 0$, as expected.
  \item Positivity: for all $x \neq y \in \rr^n$, there exists at
    least one $1 \leq i \leq n$ such that $x_i \neq y_i$, so that
    $|x_i-y_i| > 0$, and
    $d_{l^1}(x,y) = \sum_{i=1}^{n} |x_i - y_i| > 0$, as
    expected.
  \item Symmetry: for all $x,y \in \rr^n$, we have
    \[d_{l^1}(y,x) = \sum_{i=1}^{n} |y_i - x_i| =
      \sum_{i=1}^{n} |x_i - y_i| = d_{l^1}(x,y)\]
    as expected.
  \item Triangle inequality: we already know from Proposition 4.3.3(g)
    (generalized to real numbers) that we have the triangle inequality
    $|a-c| \leq |a-b| + |b-c|$ for all $a,b,c \in \rr$. Thus, for all
    $x,y,z \in \rr^n$, we have
    \begin{equation*}
      d_{l^1}(x,z) := \sum_{i=1}^{n} |x_i - z_i| \leq \sum_{i=1}^{n}
      (|x_i - y_i| + |y_i - z_i|) =: d_{l^1}(x,y) + d_{l^1}(y,z)
    \end{equation*}
    as expected.
  \end{enumerate}
  Thus, $(\rr^n, d_{l^1})$ is indeed a metric space.
\end{exo}

\bigskip
\begin{exo}{12.1.8}{Prove the two inequalities in equation (12.1).}

  We have to prove that for all $x,y \in \rr^n$, we have
  \begin{equation}
    \label{eq:12.1.8goal}
    d_{l^2}(x,y) \leq d_{l^1}(x,y) \leq \sqrt{n} \, d_{l^2}(x,y)
  \end{equation}

  \begin{itemize}
  \item The first inequality, since everything is non-negative, is
    equivalent to $d_{l^2}(x,y)^2 \leq d_{l^1}(x,y)^2$, and we will prove
    it in this form.

    Indeed, using a trivial product expansion, we have
    \begin{align*}
      d_{l_1}(x,y)^2
      &:= \left(\sum_{i=1}^{n} |x_i - y_i|\right)^2 \\
      &= \left(\sum_{i=1}^{n} |x_i - y_i|\right) \times
        \left(\sum_{i=1}^{n} |x_i - y_i|\right) \\
      &= \sum_{i=1}^{n} |x_i - y_i|^2 + \overbrace{\sum_{1 \leq i,j \leq n ; \,
        i\neq j} |x_i-y_i| \times |x_j - y_j|}^{\geq 0} \\
      &\geq \sum_{i=1}^{n} |x_i - y_i|^2 =: d_{l^2}(x,y)^2
    \end{align*}
    as expected.
  \item For the second inequality, we use the Cauchy-Schwarz
    inequality, which says that
    \begin{align*}
      d_{l^1}(x,y) &:= \sum_{i=1}^{n} |x_i - y_i|\\
      &= \left| \sum_{i=1}^{n} |x_i - y_i| \times 1 \right|\\
      &\leq \left(\sum_{i=1}^{n} |x_i - y_i|^2 \right)^{1/2}
        \left(\sum_{i=1}^{n} 1^2 \right)^{1/2} \\
      &\leq d_{l^2}(x,y) \times \sqrt{n}
    \end{align*}
    as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.9}{Show that the pair $(\rr^n, d_{l^\infty})$ in
    Example 12.1.9 is a metric space.}

  Once again, let's show the four axioms of Definition 12.1.2. 

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in \rr^n$, we clearly have
    $d_{l^\infty}(x,x) = \sup \{|x_i - x_i| : 1 \leq i \leq n\} = 0$,
    as expected.
  \item Positivity: for all $x \neq y \in \rr^n$, there exists at
    least one $1 \leq j \leq n$ such that $x_j \neq y_j$. Thus
    $|x_j-y_j| > 0$, and
    $d_{l^\infty}(x,y) = \sup \{|x_i - y_i| : 1 \leq i \leq n\} \geq
    |x_j - y_j| > 0$, as expected.
  \item Symmetry: for all $x,y \in \rr^n$, we have
    \[d_{l^\infty}(x, y) = \sup \{|x_i - y_i| : 1 \leq i \leq n\} =
      \sup \{|y_i - x_i| : 1 \leq i \leq n\} = d_{l^\infty}(y, x)\]
    as expected.
  \item Triangle inequality. Let be $x,y,z \in \rr^n$. We have
    $|x_i - z_i| \leq |x_i - y_i| + |y_i - z_i|$ for all
    $1 \leq i \leq n$, by Proposition 4.3.3(g). But, by definition of
    the supremum, we have $|x_i - y_i| \leq d_{l^\infty}(x,y)$
    and $|y_i - z_i| \leq d_{l^\infty}(y,z)$ for all $1 \leq i \leq
    n$. Thus, we have $|x_i - z_i| \leq d_{l^\infty}(x,y) +
    d_{l^\infty}(y,z)$ for all $1 \leq i \leq n$; i.e., $d_{l^\infty}(x,y) +
    d_{l^\infty}(y,z)$ is an upper bound of the set $\{|x_i -
    z_i| : 1 \leq i \leq n\}$. By definition of the supremum, it
    implies that
    \[d_{l^\infty}(x,z) := \sup \{|x_i - z_i| : 1 \leq i \leq n\} \leq d_{l^\infty}(x,y) +
      d_{l^\infty}(y,z)\]
    as expected.
  \end{enumerate}
  Thus, $(\rr^n, d_{l^1})$ is indeed a metric space.  
\end{exo}

\bigskip
\begin{exo}{12.1.10}{Prove the two inequalities in equation (12.2).}

  We have to prove that for all $x,y \in \rr^n$,
  \[ \frac{1}{\sqrt{n}} d_{l^2}(x,y) \leq d_{l^\infty} (x,y) \leq
    d_{l^2}(x,y).\]

  First, a preliminary remark. By definition, we have
  $d_{l^\infty}(x,y) := \sup \{|x_i - y_i| : 1 \leq i \leq n\}$ for
  all $x,y \in \rr^n$. Since this distance is defined as the supremum
  of a finite set, we know (see Chapter 8 of \textit{Analysis I}) that
  there exists a $1 \leq m \leq n$ such that
  $d_{l^\infty}(x,y) = |x_m - y_m|$ (the supremum belongs to the set).
  The index ``$m$'' will have this meaning below.

  \begin{itemize}
  \item Let's prove the first inequality.
    \begin{align*}
      \frac{1}{\sqrt{n}} d_{l^2}(x,y)
      &:= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - y_i)^2} \\
      &\leq \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_m - y_m)^2} \\
      &\leq \sqrt{\frac{n}{n} (x_m - y_m)^2} \\
      &= |x_m - y_m| =: d_{l^\infty} (x,y)
    \end{align*}
    as expected.
  \item Now we prove the second one. We have
    \begin{align*}
      d_{l^2}(x,y) &:= \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} \\
                   &= \sqrt{(x_m - y_m)^2 + \sum_{1 \leq i \leq n ; \, i
                     \neq m} (x_i - y_i)^2}\\
                   &\geq \sqrt{(x_m - y_m)^2} = |x_m - y_m| =: d_{l^\infty}(x,y)
    \end{align*}
    as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.11}{Show that the discrete metric $(X, \ddisc)$ in
    Example 12.1.11 is indeed a metric space.}

  Once again, let's show the four axioms of Definition 12.1.2. 

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in X$, we have
    $\ddisc(x,x) := 0$ by definition, so that there is nothing to
    prove here.
  \item Positivity: for all $x \neq y \in X$, we have
    $\ddisc(x,y) := 1 > 0$ by definition, so that there's still
    nothing to prove.
  \item Symmetry: for all $x,y \in X$, we have $\ddisc(x,y) =
    \ddisc(y,x) = 1$, so that $\ddisc$ obeys the symmetry property.
  \item Triangle inequality. Let be $x,y,z \in X$, and let's consider
    $\ddisc(x,z)$.
    \begin{itemize}
    \item If $x=z$, then $\ddisc(x,z) = 0$. And since $\ddisc$ is a
      non-negative application, we clearly have $0 =: \ddisc(x,z) \leq
      \ddisc(x,y) + \ddisc(y,z)$ for all $y \in X$.
    \item If $x \neq z$, then we cannot have both $x=y$ and $y=z$ (it
      would be a clear contradiction with $x \neq z$). Thus, at least
      one of the propositions ``$x \neq y$'', ``$y \neq z$'' is true.
      Another way to say that is $\ddisc(x,y) + \ddisc(y,z) \geq 1$.
      But since $\ddisc(x,z) := 1$, we have actually $\ddisc(x,y) +
      \ddisc(y,z) \geq \ddisc(x,z)$, as expected.
    \end{itemize}
  \end{enumerate}
\end{exo}

\begin{exo}{12.1.12}{Prove Proposition 12.1.18.}

  First, recall that for all $x,y \in \rr^n$, we have, from Examples
  12.1.7 and 12.1.9,
  \begin{equation*}
    \frac{1}{\sqrt{n}} \, d_{l^2} (x,y) \leq
    d_{l^\infty}(x,y) \leq d_{l^2}(x,y) \leq
    d_{l^1}(x,y) \leq \sqrt{n} \, d_{l^2}(x,y).
  \end{equation*}

  Note that $n$ is a real constant here.
  
  \begin{itemize}
  \item Let's prove that $(a) \implies (b)$. If
    $\lim_{k \to \infty} d_{l^2}(x^{(k)}, x) = 0$, then by the limit
    laws, the sequence $t_k := \sqrt{n} \, d_{l^2}(x^{(k)}, x)$ also
    converges to $0$ as $k \to \infty$, since $\sqrt{n}$ is a constant
    real number. Thus, we have
    \[d_{l^2}(x^{(k)}, x) \leq d_{l^1}(x^{(k)}, x) \leq \sqrt{n} \,
      d_{l^2}(x^{(k)}, x)\] and, by the squeeze test, this implies
    that $\lim_{k \to \infty} d_{l^1}(x^{(k)}, x)$ as expected.
  \item Let's prove that $(b) \implies (c)$. If
    $\lim_{k \to \infty} d_{l^1}(x^{(k)}, x) = 0$, then we have
    \[0 \leq d_{l^\infty}(x^{(k)}, x) \leq d_{l^1}(x^{(k)}, x)\]
    and, by the squeeze test, this implies
    that $\lim_{k \to \infty} d_{l^\infty}(x^{(k)}, x)$ as expected.
  \item Let's prove that $(c) \implies (d)$. Suppose that
    $\lim_{k \to \infty} d_{l^\infty}(x^{(k)}, x) = 0$. Then, for all
    $1 \leq j \leq n$, we have
    $0 \leq |x_j^{k} - x_j| \leq d_{l^\infty}(x^{(k)}, x)$. Still by
    the squeeze test, this implies that
    $\lim_{k \to \infty} |x_j^{k} - x_j| = 0$, i.e. that
    $(x_j^{k})_{k=m}^\infty$ converges to $x_j$ as $k \to \infty$ (by
    Lemma 12.1.1), as expected.
  \item Finally, let's prove that $(d) \implies (a)$. Using the
    definition of convergence is more appropriate here. Let be
    $\epsilon > 0$ a positive real number, and let be
    $1 \leq j \leq n$. By definition, there exists a natural number
    $N \geq m$ such that $|x_j^{(k)} - x_j| \leq \epsilon / \sqrt{n}$
    whenever $k \geq N$. Thus, if $k \geq N$, we have
    \[d_{l^2}(x^{(k)}, x) := \sqrt{\sum_{j=1}^{n} (x^{(k)}_j - x_j)^2}
      \leq \sqrt{\sum_{j=1}^{n} \frac{\epsilon^2}{n}} \leq \epsilon\]
    so that $\lim_{k \to \infty} d_{l^2}(x^{(k)}, x) = 0$, i.e.,
    $(x^{k})_{k=m}^\infty$ converges to $x$ as $k \to \infty$ in the
    $l^2$ metric (by Lemma 12.1.1), as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.13}{Prove Proposition 12.1.19.}

  Let be $\seq{x^{(n)}}{m}$ a sequence of elements of a set $X$.

  \begin{itemize}
  \item First suppose that $\seq{x^{(n)}}{m}$ is eventually constant.
    Thus, by definition, there exists an $N \geq m$ and an element
    $x \in X$ such that $\seq{x^{(n)}}{m} = x$ for all $n \geq N$.
    This implies that we have $\ddisc(x^{(n)}, x) = 0$ for all $n \geq
    N$. In particular, for all $\epsilon > 0$, we have
    $\ddisc(x^{(n)}, x) \leq \epsilon$ whenever $n \geq N$, so that
    $\seq{x^{(n)}}{m}$ indeed converges to $x$ with respect to
    $\ddisc$.
  \item Conversely, suppose that $\seq{x^{(n)}}{m}$ converges to $x$
    with respect to $\ddisc$. Let be $\epsilon = 1/2$. By definition,
    there exists an $N \geq m$ such that $\ddisc(x^{(n)}, x) \leq 1/2$
    whenever $n \geq N$. Since $\ddisc(x^{(n)}, x)$ cannot be $1$, it
    is necessarily equal to $0$, so that $x^{(n)} = x$ whenever
    $n \geq N$. Thus, the sequence $x^{(n)}$ is indeed eventually
    constant.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.14}{Prove Proposition 12.1.20.}

  Suppose that we have $\lim_{n \to \infty} d(x^{(n)}, x) = 0$ and
  $\lim_{n \to \infty} d(x^{(n)}, x') = 0$. Suppose, for the sake
  of contradiction, that we have $x \neq x'$. Thus, the real number
  $\epsilon := \frac{d(x,x')}{3}$ is positive.

  Since $x^{(n)}$ converges to $x$, there exists a $N_1 \geq m$ such
  that $d(x^{(n)}, x) \leq \epsilon$ whenever $n \geq N_1$.

  Similarly, since $x^{(n)}$ converges to $x'$, there exists a
  $N_2 \geq m$ such that $d(x^{(n)}, x') \leq \epsilon$ whenever
  $n \geq N_2$.

  By the triangle inequality, we thus have, for all $n \geq \max(N_1,
  N_2)$,
  \[d(x, x') \leq d(x, x^{(n)}) + d(x^{(n)}, x') \leq \epsilon +
    \epsilon = \frac{2}{3}d(x,x')\]
  which is a contradiction (since $d(x,x') > 0$ by hypothesis).

  Thus, the limit is unique, and we must have $x=x'$.
\end{exo}

\bigskip
\begin{exo}{12.1.15}{Let be
    $X := \{\seq{a_n}{0} : \sum_{n=0}^{\infty} |a_n| < \infty\}$. We
    define on this space the metrics
    $d_{l^1}(\seq{a_n}{0}, \seq{b_n}{0}) := \sum_{n=0}^{\infty} |a_n -
    b_n|$, and $d_{l^\infty}(\seq{a_n}{0}, \seq{b_n}{0}) := \sup_{n \in
      \nn} |a_n - b_n|$. Then...}

  We have to prove the following statements.

  \begin{enumerate}
  \item $d_{l^1}$ is a metric on $X$.

    We have to prove the four axioms of Definition 12.1.2.

    \begin{enumerate}[label=(\alph*)]
    \item Let be $\seq{a_n}{0} \in X$. We have $d_{l^1}(\seq{a_n}{0},
      \seq{a_n}{0}) = \sum_{n=0}^{\infty} |a_n - a_n| = 0$, as
      expected.
    \item Let be $\seq{a_n}{0}, \seq{b_n}{0}$ two distinct elements of
      $X$. Since they are distinct, there exists at least one $m \in
      \nn$ such as $|a_m - b_m| > 0$. Thus, $d_{l^1}(\seq{a_n}{0},
      \seq{b_n}{0}) = \sum_{n=0}^{\infty} |a_n - b_n| \geq |a_m - b_m|
      > 0$, as expected.
    \item Symmetry: we clearly have
      \[d_{l^1}(\seq{b_n}{0}, \seq{a_n}{0}) = \sum_{n=0}^{\infty}
        |b_n - a_n| = \sum_{n=0}^{\infty} |a_n - b_n| =
        d_{l^1}(\seq{a_n}{0}, \seq{b_n}{0}).\]
    \item Finally, let's prove the triangle inequality. Let be
      $\seq{a_n}{0}, \seq{b_n}{0}, \seq{c_n}{0} \in X$. Since we have
      the triangle inequality for the usual distance $d$ on $\rr$
      (i.e., we have $|a_n - c_n| \leq |a_n - b_n| + |b_n - c_n|$ for
      all $n \in \nn$), we have immediately
      \begin{align*}
        d_{l^1}(\seq{a_n}{0}, \seq{c_n}{0})
        &:= \sum_{n=0}^{\infty} |a_n - c_n|\\
        &\leq \sum_{n=0}^{\infty} (|a_n - b_n| + |b_n - c_n|)
          \; \text{ (consequence of Prop. 7.1.11(h))}\\
        &\leq \sum_{n=0}^{\infty} |a_n - b_n| + \sum_{n=0}^{\infty}
          |b_n - c_n|
          \; \text{ (by Proposition 7.2.14(a))}\\
        &\leq d_{l^1}(\seq{a_n}{0}, \seq{b_n}{0}) +
          d_{l^1}(\seq{b_n}{0}, \seq{c_n}{0}).
      \end{align*}
    \end{enumerate}

    Thus, $d_{l^1}$ is indeed a metric on $X$.
  \item $d_{l^\infty}$ is a metric on $X$.

    Once again, we have to prove the four axioms of Definition 12.1.2.

    \begin{enumerate}[label=(\alph*)]
    \item Let be $\seq{a_n}{0} \in X$. We have
      $d_{l^\infty}(\seq{a_n}{0}, \seq{a_n}{0}) = \sup_{n \in \nn}
      |a_n - a_n| = 0$, as expected.
    \item Let be $\seq{a_n}{0}, \seq{b_n}{0}$ two distinct elements of
      $X$. Since they are distinct, there exists at least one $m \in
      \nn$ such as $|a_m - b_m| > 0$. Thus, $d_{l^\infty}(\seq{a_n}{0},
      \seq{b_n}{0}) = \sup_{n \in \nn} |a_n - b_n| \geq |a_m - b_m|
      > 0$, as expected.
    \item Symmetry: we clearly have
      \[d_{l^\infty}(\seq{b_n}{0}, \seq{a_n}{0}) = \sup_{n \in \nn}
        |b_n - a_n| = \sup_{n \in \nn} |a_n - b_n| =
        d_{l^\infty}(\seq{a_n}{0}, \seq{b_n}{0}).\]
    \item Finally, let's prove the triangle inequality. Let be
      $\seq{a_n}{0}, \seq{b_n}{0}, \seq{c_n}{0} \in X$. Since we have
      the triangle inequality for the usual distance $d$ on $\rr$
      (i.e., we have $|a_n - c_n| \leq |a_n - b_n| + |b_n - c_n|$ for
      all $n \in \nn$), we have immediately
      $|a_m - c_m| \leq \sup_{n \in \nn} |a_n - b_n| + \sup_{n \in
        \nn} |b_n - c_n|$ for all $m \in \nn$, by definition of the
      supremum. In other words,
      $(\sup_{n \in \nn} |a_n - b_n| + \sup_{n \in \nn} |b_n - c_n|)$
      is an upper bound for the set $\{|a_m - c_m| : m \in \nn\}$.
      Thus we have, still by definition of the supremum,
      $\sup_{n \in \nn} |a_n - c_n| \leq \sup_{n \in \nn} |a_n - b_n| +
      \sup_{n \in \nn} |b_n - c_n|$, as expected.
    \end{enumerate}
    Thus, $d_{l^\infty}$ is indeed a metric on $X$.
  \item There exist sequences $x^{(1)}$, $x^{(2)}$, ..., of elements
    of $X$ (i.e., sequences of sequences) which are convergent with
    respect to $d_{l^\infty}$, but are not convergent with respect to
    $d_{l^1}$.

    Here we are dealing with sequences of sequences: we have a
    sequence $(x^{(k)})_{k=1}^\infty$ where each $x^{(k)}$ is
    itself a sequence of real numbers. Thus, let's define
    $(x^{(k)})_{k=1}^\infty$ as follows:
    \[x^{(k)}_n := \left\{
        \begin{array}{ll}
          1/(k+1) & \text{ if } 0 \leq n \leq k \\
          0 & \text{ if } n > k.
        \end{array}
      \right.\]
    Just to make things clearer, we have for instance
    \begin{align*}
      x^{(1)} &:= \frac{1}{2}, \; \frac{1}{2}, \; 0, \; 0, \; 0, \; \ldots \\
      x^{(2)} &:= \frac{1}{3}, \; \frac{1}{3}, \; \frac{1}{3}, \; 0, \; 0, \; \ldots \\
      x^{(3)} &:= \frac{1}{4}, \; \frac{1}{4}, \; \frac{1}{4}, \; \frac{1}{4}, \; 0, \; \ldots
    \end{align*}
    Also, let be the null sequence $\seq{a_n}{0}$ defined by $a_n := 0$ for
    all $n \in \nn$. Thus:
    \begin{itemize}
    \item $(x^{(k)})_{k=1}^\infty$ converges to $\seq{a_n}{0}$ w.r.t.
      the metric $d_{l^\infty}$. Indeed, if we consider a given
      positive integer $k$ (fixed), we have
      \begin{equation*}
        |x^{(k)} - a_n| = |x^{(k)}| = \left\{
          \begin{array}{ll}
            1/(k+1) & \text{ if } 0 \leq n \leq k \\
            0 & \text{ if } n > k.
          \end{array}
        \right.
      \end{equation*}
      so that $d_{l^\infty}\left(\seq{x^{(k)}_n}{0},
        \seq{a_n}{0}\right) := \sup_{n \in \nn} |x^{(k)} - a_n| =
      \frac{1}{k+1}$.

      Thus,
      $\lim_{k \to \infty} d_{l^\infty}\left(\seq{x^{(k)}_n}{0},
        \seq{a_n}{0}\right) = 0$, or in other words,
      $(x^{(k)})_{k=1}^\infty$ converges to $\seq{a_n}{0}$ w.r.t. the
      metric $d_{l^\infty}$ in $X$.
      
    \item But $(x^{(k)})_{k=1}^\infty$ does not converges to
      $\seq{a_n}{0}$ w.r.t. the metric $d_{l^1}$. Indeed, we have, for
      each given (fixed) $k$,
      \begin{align*}
        d_{l^1} \left(\seq{x^{(k)}_n}{0}, \seq{a_n}{0}\right)
        = \sum_{n=0}^{k} \frac{1}{k+1} = 1
      \end{align*}
      Thus, we clearly do not have
      $\lim_{k \to \infty} d_{l^1}\left(\seq{x^{(k)}_n}{0},
        \seq{a_n}{0}\right) = 0$, i.e., $(x^{(k)})_{k=1}^\infty$ does
      not converge to $\seq{a_n}{0}$ w.r.t. the  metric $d_{l^1}$.
    \end{itemize}
  \item Conversely, any sequence which converges with respect to
    $d_{l^1}$ also converges with respect to $d_{l^\infty}$.

    Suppose, for the sake of contradiction, that
    $(x^{(k)})_{k=1}^\infty$ does not converge to $\seq{a_n}{0}$
    w.r.t. the  metric $d_{l^\infty}$, but does converge to $\seq{a_n}{0}$
    w.r.t. the  metric $d_{l^1}$.

    In this case, there exists a $\epsilon > 0$ such that, for all
    $k \geq 1$, we have
    $(\sup_{n \geq 0} |x^{(k)}_n - a_n|) > \epsilon$. In particulier,
    for all $k \geq 1$ and all $n \geq 0$, we have
    $|x^{(k)}_n - a_n| > \epsilon$. Thus,
    $\sum_{n=0}^{\infty} |x^{(k)}_n - a_n|$ is not even a convergent
    series, and we cannot have
    $\lim_{k \to \infty} \left(\sum_{n=0}^{\infty} |x^{(k)}_n - a_n|
    \right) = 0$.
  \end{enumerate}

  Note that this exercise actually shows that in this space $X$, the
  metrics are not equivalent; instead, the convergence in the taxi cab
  metric is stronger than the convergence in the sup norm metric.
  Thus, Proposition 12.1.18 is not true for \emph{any} metric space.
\end{exo}

\bigskip
\begin{exo}{12.1.16}{Let $\seq{x_n}{1}$ and $\seq{y_n}{1}$ be two
    sequences in a metric space $(X, d)$. Suppose that $\seq{x_n}{1}$
    converges to a point $x \in X$, and $\seq{y_n}{1}$ converges to a
    point $y \in X$. Show that
    $\lim_{n \to \infty} d(x_n, y_n) = d(x, y)$.}

  On the one hand, the triangle inequality applied two times to $d$
  gives us
  \[d(x_n, y_n) \leq d(x_n, x) + d(x,y) + d(y, y_n)\]
  but this is only half of what we need to prove the result.
  
  Similarly, we have
  \[d(x, y) \leq d(x, x_n) + d(x_n,y_n) + d(y_n, y)\]
  so that we can combine the previous two inequalities to get
  \[-d(x_n, x) - d(y_n, y) \leq d(x_n, y_n) - d(x, y) \leq d(x_n, x)
    + d(y_n, y)\]
  i.e.,
  \[|d(x_n, y_n) - d(x, y)| \leq d(x_n, x) + d(y_n, y).\]
  
  Let be $\epsilon > 0$. By hypothesis, there exists a $N_1 \geq 1$
  such that $d(x_n, x) \leq \epsilon/2$ whenever $n \geq N_1$.
  Similarly, there exists a $N_2 \geq 1$ such that $d(y_n, y) \leq
  \epsilon/2$ whenever $n \geq N_2$. Thus, if we set $N :=
  \max(N_1,N_2)$, then for all $n \geq N$ we have
  \[|d(x_n, y_n) - d(x, y)| \leq d(x_n, x) + d(y_n, y) \leq 2
    \epsilon/2 \ leq \epsilon\]
  which shows that $\lim_{n \to \infty} d(x_n, y_n) = d(x, y)$, as
  expected.    
\end{exo}

\pagebreak
\begin{exo}{12.2.1}{Verify the claims in Example 12.2.8}

  Let be $(X, \ddisc)$ a metric space, and $E$ a subset of $X$.

  \begin{itemize}
  \item Let be $x \in E$. Then $x$ is an interior point of $E$.
    Indeed, we have $B(x, 1/2) = \{x\} \subseteq E$.
  \item Let be $y \notin E$. Then $y$ is an exterior point of $E$.
    Indeed, we have $B(y, 1/2) \cap E = \{y\} \cap E = \emptyset$.
  \item Finally, there are no boundary points of $E$ in $(X, \ddisc)$.
    Indeed, let be $r > 0$ and any $x \in X$. We will always have
    $B(x, r) = \{x\}$ by definition of the discrete metric $\ddisc$.
    Thus, we have either $x \in E$ and then $x \in \inter (E)$, or $x
    \notin E$ and then $x \in \ext(E)$. Thus, $E$ has no boundary
    points.
  \end{itemize}
\end{exo}

\begin{exo}{12.2.2}{Prove Proposition 12.2.10.}

  We have to prove the following implications:

  \begin{itemize}
  \item Let's show that $(a) \implies (b)$. We will use the
    contrapositive, assuming that $x_0$ is neither an interior point
    of $E$, nor a boundary point of $E$. By definition, it means that
    $x_0$ is an exterior point of $E$, i.e. that there exists $r > 0$
    such that $B(x_0, r) \cap E = \emptyset$. This is precisely the
    negation of $x_0$ being an adherent point of $E$. Thus, we have
    showed that if $x_0$ is an adherent of of $E$, it is either an
    interior point of a boundary point.
  \item Let's show that $(b) \implies (c)$. Let be a positive integer
    $n > 0$, and suppose that $x_0$ is either an interior point of
    $E$, or a boundary point of $E$. In either case, the set
    $A_n := B(x_0, 1/n) \cap E$ is non empty, i.e., there exists
    $a_n \in X$ such that $d(a_n, x_0) < 1/n$. By the (countable)
    axiom of choice, we can define a sequence $\seq{a_n}{1}$ such that
    $a_n \in A_n$ for all $n \geq 1$.

    Let be $\epsilon > 0$. There exists $N > 0$ such that $1/N <
    \epsilon$ (Exercise 5.4.4). Thus, for all $n \geq N$, we have
    \[d(a_n, x_0) < \frac{1}{n} \leq \frac{1}{N} < \epsilon\]
    i.e., the sequence $\seq{a_n}{1}$ converges to $x_0$ with respect
    to the metric $d$, as expected.
  \item Finally, let's show that $(c) \implies (a)$. Let be $r > 0$.
    If $\seq{a_n}{1}$ in $E$ converges to $x_0$ with respect to $d$,
    then there exists a $n$ such that $d(x_0, a_n) < r$. But since
    $a_n \in E$, it means that $B(x_0, r) \cap E$ is non empty, i.e.
    that $x_0$ is an adherent point of $E$.
  \end{itemize}
\end{exo}

\begin{exo}{12.2.3}{Prove Proposition 12.2.5.}

  Let be $(X,d)$ a metric space.

  \begin{enumerate}[label=(\alph*)]
  \item Let be $E \subseteq X$. First suppose that $E$ is open; this
    means that $E \cap \partial E = \emptyset$. Let be $x \in E$, then we
    have $x \notin \partial E$. But since $x \in E$, we have $x \in
    \adh{E}$, and thus $x \in \inter(E)$ by Proposition 12.2.10(b). We
    have shown that $x \in E \implies x \in \inter(E)$, and since the
    converse implication is trivial (Remark 12.2.6), we have $E =
    \inter(E)$ as expected.

    Now suppose that $E = \inter(E)$. Let be $x \in E$. We thus have
    $x \in \inter(E)$. By definition, $x$ is thus not a boundary point
    of $E$, i.e. x $\notin \partial E$. This means that $E \cap
    \partial E = \emptyset$, i.e. that $E$ is open, as expected.
    
  \item Let be $E \subseteq X$. First suppose that $E$ is closed; i.e.
    that $\partial E \subseteq E$. Let be $x \in \adh{E}$. By
    Proposition 12.2.10, we have $\adh{E} = \inter(E) \cup \partial
    E$; such that $\adh{E}$ is the union of two subsets of $E$, and
    thus is itself a subset of $E$, as expected.

    Conversely, suppose that $\adh{E} \subseteq E$. It means that
    $\inter(E) \cup \partial E \subseteq E$, and in particular that
    $\partial E \subseteq E$, i.e. that $E$ is closed, as expected.
    
  \item Let be $x_0 \in X$, $r > 0$ and $E := B(x_0, r)$. To show that
    $E$ is open, we must show that $E = \inter(E)$ (by Proposition
    1.2.15(a)), and in particular that $E \subseteq \inter(E)$ (the
    converse inclusion being trivial). Let be $x \in E$, and let's
    show that $x \in \inter(E)$. By definition, we have
    $d(x, x_0) < r$, so that $\epsilon := r - d(x, x_0)$ is a positive
    real number. Thus, let be $y \in B(x, \epsilon)$. By the triangle
    inequality, we have
    \begin{align*}
      d(x_0, y) &< d(x, x_0) + d(x,y) \\
                &< d(x, x_0) + \epsilon \\
                &< d(x, x_0) + r - d(x, x_0) = r
    \end{align*}
    so that $y \in E$. Thus, there exists $\epsilon > 0$ such that
    $B(x, \epsilon) \subseteq E$, i.e., $x$ is an interior point of
    $E$. This shows that $E \subseteq \inter(E)$, as expected.

    Now let be $F := \{x \in X : d(x, x_0) \leq r\}$, and let be
    $\seq{a_n}{1}$ a convergent sequence in $F$. To show that $F$ is
    closed, we have to show that $\ell := \lim_{n \to \infty} a_n$
    lies in $F$ (Proposition 12.2.15(b)). Suppose, for the sake of
    contradiction, that $\ell \notin F$. We thus have $d(\ell, x_0) >
    r$, so that $\epsilon := d(\ell, x_0) - r$  is a positive real
    number. Since $\seq{a_n}{1}$ converges to $\ell$, there exists a
    $N > 0$ such that $d(a_n, \ell) < \epsilon$ whenever $n \geq N$.
    By the triangle inequality, for $n \geq N$, we have
    \begin{align*}
      d(x_0, \ell) &\leq d(x_0, a_n) + d(a_n, \ell) \\
      d(x_0, a_n) &\geq d(x_0, \ell) - d(a_n, \ell) \\
                   &\geq d(x_0, \ell) - \epsilon \\
                   &\geq d(x_0, \ell) + r - d(\ell, x_0) \\
                   &\geq r
    \end{align*}
    and thus, $a_n \notin B(x_0, r)$, a contradiction. Thus, we must
    have $\ell \in F$, so that $F$ is indeed a closed set.
    
  \item Let be $\{x_0\}$ a singleton with $x_0 \in X$. To show that
    $E$ is closed, we may use Proposition 12.2.15(b), and show that
    $\{x_0\}$ contains all its adherent points. Let be $\seq{a_n}{1}$
    a convergent sequence in $\{x_0\}$; it can only be the constant
    sequence $x_0, x_0, \ldots$. Since it is a constant sequence, its
    limit can only be $x_0$ itself, and this limit belongs to
    $\{x_0\}$. Thus, $\{x_0\}$ is a closed set.
    
  \item First we can form a lemma: for any subset $E \subseteq X$, we
    have $\inter(E) = \ext(X \backslash E)$. This is a direct
    consequence of Definition 12.2.5. Indeed, $x \in \inter(E)$ iff
    there exists a $r > 0$ such that $B(x,r) \subseteq E$, which is
    equivalent to
    ``$\exists r>0 : B(x,r) \cap (X \backslash E) = \emptyset$'',
    which is equivalent to $x \in \ext(X \backslash E)$.

    This implies that the interior points of $E$ are the exterior points
    of $X \backslash E$, and conversely, that the exterior points of
    $E$ are the interior points of $X \backslash E$. Thus, in
    particular, we have this useful fact:
    \begin{equation}
      \label{eq:12.2.3}
      \partial E = \partial(X \backslash E).
    \end{equation}

    Now we go back to the main proof. First suppose that $E$ is open.
    Thus, by Definition 12.2.12, we have
    $E \cap \partial E = \emptyset$, so that
    $\partial E \subseteq X \backslash E$, which means that
    $X \backslash E$ is a closed set. The converse also applies: if we
    suppose that $X \backslash E$ is closed, then
    $\partial (X \backslash E) \subseteq X \backslash E$. By equation
    \eqref{eq:12.2.3} above, this is equivalent to
    $\partial E \subseteq X \backslash E$, and thus we have
    $\partial E \cap E = \emptyset$. This means that $E$ is open, as
    expected.\footnote{This important result will be used in future
      proofs to turn any statement on closed sets into a statement on
      open sets.}
    
  \item Let $E_1, \ldots, E_n$ be open sets. Thus, for all
    $1 \leq i \leq n$, if $x \in E_i$, there exists a $r_i > 0$ such
    that $B(x, r_i) \subseteq E_i$. Let's define $r := \min(r_1,
    \ldots, r_n)$. We have $B(x, r) \subseteq B(x, r_i) \subseteq E_i$
    for all $1 \leq i \leq n$, i.e. $B(x,r) \subseteq E_1 \cap \ldots
    \cap E_n$. Thus, $E_1 \cap \ldots \cap E_n$ is an open set.

    Also, let $F_1, \ldots, F_n$ be closed sets. By the previous
    result (e), the complementary sets
    $X\backslash F_1, \ldots X\backslash F_n$ are open sets. Thus, we
    have just proved that
    $(X \backslash F_1) \cap \ldots \cap (X \backslash F_n)$ is an
    open set. But we have
    $(X \backslash F_1) \cap \ldots \cap (X \backslash F_n) = X
    \backslash (F_1 \cup \ldots \cup F_n)$, and this set is open.
    Thus, by (e), its complementary set, $F_1 \cup \ldots \cup F_n$,
    is closed, as expected.
    
  \item Let $(E_\alpha)_{\alpha \in I}$ be open sets. Suppose that we
    have $x \in \bigcup_{\alpha \in I} E_\alpha$. By definition, there
    exists a $i \in I$ such that $x \in E_i$. Since $E_i$ is an open
    set, there exists $r_i > 0$ such that
    $B(x, r_i) \subseteq E_i \subseteq \bigcup_{\alpha \in I}
    E_\alpha$. Thus, by (a), $\bigcup_{\alpha \in I} E_\alpha$ is an
    open set, as expected.

    Now let be $(F_\alpha)_{\alpha \in I}$ be closed sets. Suppose
    that we have a convergent sequence $\seq{x_n}{1}$ such that
    $x_n \in \bigcap_{\alpha \in I} F_\alpha$ for all $n \geq 1$.
    Thus, for all $\alpha \in I$, the sequence $\seq{x_n}{1}$ entirely
    belongs to the closed set $F_\alpha$, so that its limit $\ell$
    also lies in $F_\alpha$ according to (b). Thus,
    $\ell \in \bigcup_{\alpha \in I} F_\alpha$, so that
    $\bigcap_{\alpha \in I} F_\alpha$ is a closed set, as expected.
    
  \item Let be $E \subseteq X$.
    \begin{itemize}
    \item Let's show that $\inter(E)$ is the largest open set included
      in $E$. It has not clearly be proved in the main text that
      $\inter(E)$ is an open set, so we begin by proving it. Let be
      $x \in \inter(E)$. By definition, there exists $r > 0$ so that
      $B(x, r) \subseteq E$. But by (c), we know that $B(x, r)$ is an
      open set, so that any point $y$ of $B(x, r)$ is an interior
      point of this open ball, and thus an interior point of $E$.
      Thus, $\inter(E)$ is open.

      Now consider another open set $V \subseteq E$, and let's show
      that $V \subseteq \inter(E)$. If $x \in \inter(V)$, then there
      exists $r > 0$ such that $B(x,r) \subseteq V \subseteq E$, so
      that $x \in \inter(E)$. This shows that $V \subseteq \inter(E)$,
      as expected.

    \item Similarly, let's show that $\adh{E}$ is the smallest closed
      set that contains $E$. First we show that $\adh{E}$ is closed,
      i.e. that $\adh{\adh{E}} \subseteq \adh{E}$. (Hint: see Exercise
      9.1.6 for an intuition.) Let be $x \in \adh{\adh{E}}$. By
      definition, for all $r > 0$,
      $B(x,r) \cap \adh{E} \neq \emptyset$. Thus, there exists
      $y \in B(x,r)$ such that $y \in \adh{E}$. Thus, because $B(x,r)$
      is an open set and $y$ is adherent to $E$, there exists
      $\epsilon > 0$ such that $B(y, \epsilon) \subseteq B(x,r)$ and
      $B(y, \epsilon) \cap E \neq \emptyset$; i.e., there exists
      $z \in B(y, \epsilon) \subseteq B(x,r)$ such that $z \in E$. We
      have showed that whenever $x \in \adh{\adh{E}}$, we have $B(x,r)
      \cap E \neq \emptyset$ for all $r > 0$, i.e. that $x$ is an
      adherent point of $E$, as expected. Thus, $\adh{E}$ is closed.
      
      Now we consider a closed set $K$ such that $E \subseteq K$, and
      we have to show that $\adh{E} \subseteq K$. Let be
      $x \in \adh{E}$. By definition, for all $r > 0$, we have
      $B(x,r) \cap E \neq \emptyset$. But since $E \subseteq K$, we
      also have $B(x,r) \cap K \neq \emptyset$ for all $r > 0$. Thus,
      $x$ is an adherent point of $K$, i.e., $x \in \adh{K}$. But
      since $K$ is closed, we have $K = \adh{K}$, and thus $x \in K$.
      This shows that $\adh{E} \subseteq K$, as expected.
    \end{itemize}
  \end{enumerate}
\end{exo}

\begin{exo}{12.2.4}{Let $(X, d)$ be a metric space, $x_0$ be a point
    in $X$, and $r > 0$. Let $B$ be the open ball
    $B := B(x_0 , r) = \{x \in X : d(x, x_0 ) < r\}$, and let $C$ be
    the closed ball $C := \{x \in X : d(x, x_0 ) \leq r\}$.}

  Let's prove the following claims:

  \begin{enumerate}[label=(\alph*)]
  \item Show that $\adh{B} \subseteq C$.

    Let be $x \in \adh{B}$. By definition, since $x$ is an adherent
    point of $B$, for all $\epsilon > 0$ we have
    $B(x, \epsilon) \cap B \neq \emptyset$. In other words, there
    exists $y$ such that we have both $d(x, y) < \epsilon$ and
    $d(x_0, y) < r$. Thus, by the triangle inequality, we have
    \begin{align*}
      d(x, x_0) &\leq d(x,y) + d(y, x_0)\\
                &\leq \epsilon + r \; \; \text{ for all } \epsilon > 0
    \end{align*}
    which is equivalent (as a quick proof by contradiction would show)
    to $d(x,x_0) \leq r$. Thus, $x \in C$.

    We have indeed proved that $\adh{B} \subseteq C$.
    
  \item Give an example of a metric space $(X,d)$, a point $x_0$, and
    a radius $r > 0$ such that $\adh{B}$ is \emph{not} equal to $C$.

    Let's take $X = \rr$, $d = \ddisc$, $x = 0$ and $r = 1$. One the
    one hand, we have $B := \{0\}$ and $C := \rr$. Now let's work out
    $\adh{B}$. By Proposition 12.2.15(bd), $B$ is closed, so that we
    have $\adh{B} = B$. Thus, we clearly do not have $\adh{B} \neq C$
    here. (Note however that any $x_0 \in \rr$ would be convenient
    here; there is nothing special about $0$.)
  \end{enumerate}
\end{exo}

\begin{exo}{12.3.1}{Prove Proposition 12.3.4(b).}

  Let's show each direction of the equivalence.

  \begin{itemize}
  \item First suppose that $E$ is relatively closed w.r.t. $Y$, and
    let's show that there exists a closed subset $K \subseteq X$ such
    that $E = K \cap Y$.

    Since $E$ is closed w.r.t. $Y$, the set $Y \backslash E$ is open
    w.r.t. $Y$ (by Proposition 12.2.15(e)). Thus, by (a), there exists
    an open subset $V \subseteq X$ such that
    $Y \backslash E = V \cap Y$.

    Let be $K := X \backslash V$; this subset $K \subseteq X$ is
    closed w.r.t. $X$ by Proposition 12.2.15(e) since it is the
    complementary set of an open set. We have to show that
    $E = K \cap Y$.
    \begin{itemize}
    \item Let be $x \in E$. Thus, we have $x \in Y$, since $E
      \subseteq Y$. And since $x \in E$, by definition, we have $x
      \notin Y \backslash E$. Thus, $x \notin V \cap Y$, which implies
      that $x \notin V$ (since $x \in Y$). Thus, by definition, $x \in
      K$, and thus, $x \in K \cap Y$.
    \item Conversely, let be $x \in K \cap Y$. By definition, $x \in
      Y$ and $x \notin V$. Thus, $x \notin V \cap Y$, or, in other
      words, $x \notin Y \backslash E$. We finally get $x \in E$, as
      expected.
    \end{itemize}
    Thus, we have indeed $E = K \cap Y$, for some closed subset
    $K \subseteq X$, as expected.
    
  \item Now let's prove the converse implication: suppose that $E = K
    \cap Y$ for some closed subset $K \subseteq X$, and let's prove
    that $E$ is relatively closed w.r.t. $Y$.

    Still by Proposition 12.2.15(e), we know that the subset $V := X
    \backslash K$ is open w.r.t. $X$. Thus, by the previous result
    from this exercise, $V \cap Y$ is relatively open w.r.t. $Y$.
    Thus, its complementary set $Y \backslash (V \cap Y) = Y
    \backslash V$ is relatively closed w.r.t. $Y$. Now we want to show
    that $E = Y \backslash V$ to close the proof.

    \begin{itemize}
    \item First suppose that $x \in E$. Since $E = K \cap Y$, we thus
      have $x \in Y$ and $x \in K$, i.e. $x \notin V$. Thus, $x \in Y
      \backslash V$.
    \item Now suppose that $x \in Y \backslash V$. We thus have
      $x \in X$ (since $Y \subseteq X$) and $x \notin V$, so that we
      necessarily have $x \in K$. Thus $x \in Y \cap K$, i.e.
      $x \in E$.
    \end{itemize}
    Thus $E = Y \backslash V$ is relatively closed w.r.t. $Y$, as
    expected.
  \end{itemize}  
\end{exo}

\begin{exo}{12.4.1}{Prove Lemma 12.4.3.}

  We have to prove that any subsequence $(x^{(n_j)})_{j=1}^\infty$ of
  a convergent sequence $\seq{x^{(n)}}{m}$ converges to the same limit
  as the whole sequence itself.

  Suppose that the whole sequence $\seq{x^{(n)}}{m}$ converges to
  $x_0$. Let be $\epsilon > 0$. By definition, we have a positive
  integer $N \geq m$ such that $n \geq N \implies d(x^{(n)}, x_0) \leq
  \epsilon$. Our aim here is to show that there exists a positive
  integer $J \geq 1$ such that $j \geq J \implies d(x^{(n_j)}, x_0)
  \leq \epsilon$.

  By Definition 12.4.1, we know that we have $m \leq n_1 < n_2 < n_3 <
  \ldots$. Thus, as a quick induction would show, we have $n_j \geq m
  + j - 1$ for all $j \geq 1$. Let's take $J := N$. In this case, if
  $j \geq J$, i.e. if $j \geq N$, we have $n_j \geq m + N - 1 \geq
  N$. Thus:
  
  \[ j\geq J \implies n_j \geq N \implies d(x^{(n_j)}, x_0)
    \leq \epsilon.\]

  Since this is true for all $\epsilon > 0$, it means that
  $(x^{(n_j)})_{j=1}^\infty$ converges to $x_0$, as expected. 
\end{exo}

\bigskip
\begin{exo}{12.4.2}{Prove Proposition 12.4.5.}

  Let $\seq{x^{(n)}}{m}$ be a sequence of points in a metric space. We
  have to prove that the following two statements are equivalent:
  \begin{enumerate}[label=(\alph*)]
  \item $L$ is a limit point of $\seq{x^{(n)}}{m}$.
  \item There exists a subsequence $(x^{(n_j)})_{j=1}^\infty$ of the
    original sequence which converges to $L$.
  \end{enumerate}

  We will prove the two implications, but first, note that (with the
  notations from Definition 12.4.1) if we have $1 \leq m \leq n_1 <
  n_2 < n_3 < \ldots$, then a quick induction shows that we have $n_j
  \geq j$ for all $j \geq 1$.
  
  \begin{itemize}
  \item First we prove that (b) implies (a). If some subsequence
    $(x^{(n_j)})_{j=1}^\infty$ converges to $L$, then we have by
    definition:
    \begin{equation}
      \label{eq:12.4.2a}
      \forall \epsilon > 0, \, \exists J \geq 1 : \; j \geq J \implies
      d(x^{(n_j)}, L) \leq \epsilon
    \end{equation}
    Now, consider any $\epsilon > 0$ and any $N \geq m$. For this
    particular choice of $\epsilon$, consider the corresponding real
    number $J$ given by equation \eqref{eq:12.4.2a}, and let's define
    $p := \max(N, J)$. Thus, we have $n_p \geq p \geq J$, and by
    equation \eqref{eq:12.4.2a}, we thus have
    $d(x^{(n_p)}, L) \leq \epsilon$. If we set $n := n_p$, we have
    indeed found an $n \geq N$ such that
    $d(x^{(n)}, L) \leq \epsilon$. Thus, $L$ is a limit point of
    $\seq{x^{(n)}}{m}$, as required.
  \item Now we prove that (a) implies (b). Suppose that $L$ is a limit
    point of $\seq{x^{(n)}}{m}$. By definition, there exists a natural
    number $n_1 \geq m$ such that $d(x^{(n_1)}, L) \leq 1$. Now, for
    $j > 1$, let's define inductively
    $n_j := \min \{n > n_{j-1} : d(x^{(n)}, L) \leq 1/j\}$. This set
    is non-empty (by definition of a limit point), so that the
    well-ordering principle (Proposition 8.1.4) ensures that it has a
    (unique) minimal element, i.e. that $n_j$ indeed exists. Let's
    define the subsequence $(x^{(n_j)})_{j=1}^\infty$ obtained
    following this process. We thus have $d(x^{(n_j)}, L) \leq 1/j$
    for all $j \geq 1$, by construction.

    Thus, let be $\epsilon > 0$. There exists a $j \geq 1$ such that
    $0 < 1/j < \epsilon$ (Exercise 5.4.4). Thus, for this positive
    integer $j$, we have $d(x^{(n_j)}, L) \leq 1/j < \epsilon$. By
    construction, for all other natural numbers $k \geq j+1$, we have
    $d(x^{(n_k)}, L) \leq 1/k \leq 1/j \leq \epsilon$.

    In summary, for our arbitrary choice of $\epsilon$, we have showed
    that there exists $j \geq 1$ such that, for all $k \geq j$, we
    have $d(x^{(n_k)}, L) \leq \epsilon$. Thus, the subsequence
    $(x^{(n_j)})_{j=1}^\infty$ constructed in this way converges to
    $L$, as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.4.3}{Prove Lemma 12.4.7.}

  Suppose that $\seq{x^{(n)}}{m}$ is a convergent sequence of points
  in a metric space $(X,d)$, and that its limit is $x_0$. Let's show
  that it is a Cauchy sequence.

  By the triangle inequality, we know that for all $j,k \geq m$, we
  have:
  \[d(x^{(j)}, x^{(k)}) \leq d(x^{(j)}, x_0) + d(x^{(k)}, x_0).\]

  Let be $\epsilon > 0$. Since $\seq{x^{(n)}}{m}$ converges to $x_0$,
  there exists an $N \geq m$ such that we have $d(x^{(n)}, x_0) \leq
  \epsilon/3$ for all $n \geq N$. Thus, if we take $j,k \geq N$, we
  have:
  \begin{align*}
    d(x^{(j)}, x^{(k)}) &\leq d(x^{(j)}, x_0) + d(x^{(k)}, x_0) \\
                        &\leq \epsilon/3 + \epsilon/3\\
                        &< \epsilon
  \end{align*}
  which means that $\seq{x^{(n)}}{m}$ is a Cauchy sequence, as
  expected.  
\end{exo}

\bigskip
\begin{exo}{12.4.4}{Prove Lemma 12.4.9.}

  Let be an arbitrary $\epsilon > 0$. Since the subsequence
  $(x^{(n_j)})_{j=1}^\infty$ converges to $x_0$, there exists a
  $J \geq 1$ such that $d(x^{(n_j)}, x_0) \leq \epsilon/3$ whenever
  $j \geq J$.

  But the whole sequence $\seq{x^{(n)}}{m}$ is supposed to be a Cauchy
  sequence. Thus, there also exists a $N \geq m$ such that $d(x^{(j)},
  x^{(k)}) < \epsilon/3$ whenever $j,k \geq N$.

  Now, let be $K := \max(J,N)$. If $k \geq K$, we have
  \begin{align*}
    d(x^{(k)}, x_0) &\leq d(x^{(k)}, x^{(n_k)}) + d(x^{(n_k)}, x_0) \\
                    &< \epsilon/3 + \epsilon/3\\
                    &< \epsilon
  \end{align*}
  which means that $\seq{x^{(n)}}{m}$ converges to $x_0$, as expected.
\end{exo}

\bigskip
\begin{exo}{12.4.5}{Let $\seq{x^{(n)}}{m}$ be a sequence of points in
    a metric space $(X,d)$ and let $L \in X$. Show that if $L$ is a
    limit point of the sequence $\seq{x^{(n)}}{m}$, then $L$ is an
    adherent point of the set $\{x^{(n)} : n \geq m\}$. Is the
    converse true?}

  First suppose that $L$ is a limit point of $\seq{x^{(n)}}{m}$. By
  definition, it means that
  \begin{equation}
    \label{eq:12.4.5a}
    \forall \epsilon > 0, \, \forall N \geq m, \, \exists n \geq N \;
    : \; d(x^{(n)}, L) \leq \epsilon
  \end{equation}

  Let be an arbitrary $\epsilon > 0$, and let's take $N = m$. By
  formula \eqref{eq:12.4.5a} above, there exists an $n \geq N$ such
  that $d(x^{(n)}, L) \leq \epsilon$. Thus, this $x^{(n)}$ belongs to
  both sets $\{x^{(n)} : n \geq m\}$ and $B(L, \epsilon)$. We have
  just proved that for all $\epsilon > 0$, the intersection
  $B(L, \epsilon) \cap \{x^{(n)} : n \geq m\}$ is always non-empty. In
  other words, $L$ is thus an adherent point of
  $\{x^{(n)} : n \geq m\}$.

  However, the converse is not true. Indeed, consider the sequence
  $\seq{x^{(n)}}{1}$ defined in $(\rr, d)$ by $x^{(1)} = 1$ and
  $x^{(n)} = 0$ for all $n \geq 2$, i.e. the sequence
  $1, 0, 0, 0, \ldots$. It is clear that $L := 1$ is an adherent point
  of $\{x^{(n)} : n \geq 1\}$ (which is just the set $\{0, 1\}$). But
  $1$ is not a limit point of $\seq{x^{(n)}}{1}$, since we have
  $d(x^{(n)}, 1) > 1/2$ for all $n \geq 2$.
\end{exo}

\bigskip
\begin{exo}{12.4.6}{Show that every Cauchy sequence can have at most
    one limit point.}

  Suppose that $\seq{x^{(n)}}{m}$ is a Cauchy sequence in a metric
  space $(X,d)$, such that $L, L'$ are limit points. Then we have
  $L=L'$. We will give two different proofs for this fact.

  \begin{itemize}
  \item \textbf{Proof 1} (short proof using previous results). By
    Proposition 12.4.5, since $L$ is a limit point of
    $\seq{x^{(n)}}{m}$, there exists a subsequence that converges to
    $L$. But by Lemma 12.4.9, it means that the whole original
    sequence $\seq{x^{(n)}}{m}$ also converges to $L$. The same
    argument can be used to show that the whole sequence
    $\seq{x^{(n)}}{m}$ converges to $L'$. But by uniqueness of limits
    (Proposition 12.1.20), we must have $L=L'$, as expected.
  \item \textbf{Proof 2} (a more ``manual'' proof). Let be
    $\epsilon > 0$. Since $\seq{x^{(n)}}{m}$ is a Cauchy sequence,
    there exists $N \geq m$ such that
    $d(x^{(p)}, x^{(q)}) \leq \epsilon/3$ for all $p,q \geq N$.

    If $L$ is a limit point of $\seq{x^{(n)}}{m}$, then for this $N
    \geq m$, there exists $p \geq N$ such that $d(x^{(p)}, L) \leq
    \epsilon /3$. Similarly, there exists $q \geq N$ such that
    $d(x^{(q)}, L') \leq \epsilon/3$.

    We thus have, by triangle inequality:
    \begin{align*}
      d(L, L') &\leq d(L, x^{(p)}) + d(x^{(p)}, x^{(q)}) + d(x^{(q)},
                 L') \\
               &\leq \epsilon/3 + \epsilon/3 + \epsilon/3 \\
               &\leq \epsilon
    \end{align*}
    Thus, $d(L,L') \leq \epsilon$ for all $\epsilon > 0$, which
    implies $L=L'$.
  \end{itemize}  
\end{exo}

\begin{exo}{12.4.7}{Prove Proposition 12.4.12.}

  For statement (a), consider a convergent sequence $\seq{y^{(n)}}{m}$
  of elements of $Y \subseteq X$. Since it is convergent, it is a
  Cauchy sequence (Lemma 12.4.7). Saying that $(Y, d_{Y \times Y})$ is
  complete means that $\seq{y^{(n)}}{m}$ converges in
  $(Y, d_{Y \times Y})$. Thus, every convergent sequence in $Y$ has
  its limit in $Y$: this is exactly the characterization of closed
  sets given by Proposition 12.2.15(b).

  For statement (b), consider a Cauchy sequence $\seq{y^{(n)}}{m}$ of
  elements of a given closed subset $Y \subseteq X$. Since $(X,d)$ is
  complete, $\seq{y^{(n)}}{m}$ must converge to some value $L \in X$.
  But since $Y$ is closed, we have $L \in Y$ by Proposition
  12.2.15(b). Thus, every Cauchy sequence in $Y$ converges in $Y$.
  This means that $(Y, d_{Y \times Y})$ is complete, as expected.
\end{exo}

\bigskip
\begin{exo}{12.4.8}{The following construction generalizes the
    construction of the reals from the rationals in Chapter 5. In what
    follows, we let $(X,d)$ be a metric space.}
  
  We have to prove the following statements. Note that this is a
  generalization of the process of construction of the real numbers,
  so that we can use all results relative to the real numbers below.
  
  \begin{enumerate}[label=(\alph*)]
  \item Given any Cauchy sequence $\seq{x^{(n)}}{1}$ in $X$, we denote
    $\formallimit{x_n}$ its formal limit. We say that two formal
    limits $\formallimit{x_n}, \formallimit{y_n}$ are equal iff
    $\lim_{n \to \infty} d(x_n, y_n) = 0$. Then, this equality
    relation obeys the reflexive, symmetry and transitive axioms.
    \begin{itemize}
    \item This relation is reflexive: for every Cauchy sequence
      $\seq{x^{(n)}}{1}$, we have ${d(x_n, x_n) = 0}$ for all
      $n \geq 1$, by definition of a metric. Thus, $d(x_n, x_n)$ is
      constant and equal to zero, so that
      $\lim_{n \to \infty} d(x_n, x_n) = 0$.
    \item By the property of symmetry of the metric $d$, we have
      $d(x_n, y_n) = d(y_n, x_n)$ for all $n \geq 1$ and all Cauchy
      sequence $\seq{x^{(n)}}{1}, \seq{y^{(n)}}{1}$. Thus,
      $\formallimit{x_n} = \formallimit{y_n}$ iff $\lim_{n \to \infty}
      d(x_n, y_n) = 0$, iff $\lim_{n \to \infty} d(y_n, x_n) = 0$,
      which is equivalent to $\formallimit{y_n} = \formallimit{x_n}$.
    \item For transitivity, suppose that $\seq{x^{(n)}}{1}$,
      $\seq{y^{(n)}}{1}$ and $\seq{z^{(n)}}{1}$ are Cauchy sequences
      in $X$. If $\formallimit{x_n} = \formallimit{y_n}$ and
      $\formallimit{y_n} = \formallimit{z_n}$, then by definition we
      have $\lim_{n \to \infty} d(x_n, y_n) = 0$ and $\lim_{n \to
        \infty} d(y_n, z_n) = 0$. Let be $\epsilon > 0$. By
      definition, there exists $N_1 \geq 1$ such that $d(x_n, y_n)
      \leq \epsilon/2$ whenever $n \geq N_1$. Similarly, there exists
      $N_2 \geq 1$ such that $d(y_n, z_n) \leq \epsilon/2$ whenever $n
      \geq N_2$. Thus, if $n \geq N := \max(N_1, N_2)$, we have by the
      triangle inequality $d(x_n, z_n) \leq d(x_n, y_n) + d(y_n, z_n)
      \leq \epsilon$. It means that $\lim_{n \to \infty} d(x_n, z_n)$,
      i.e. that $\formallimit{x_n} = \formallimit{z_n}$, as expected.
    \end{itemize}
    
  \item Let $\adh{X}$ be the space of all formal limits of Cauchy
    sequences in $X$, with the above equality relation. Define a
    metric $d_{\adh{X}} : \adh{X} \times \adh{X} \to \rr^+$ by setting
    \[ d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n}) := \lim_{n
        \to \infty} d(x_n, y_n).\]
    Then this function is well-defined and gives $\adh{X}$ the
    structure of a metric space.
    \begin{itemize}
    \item First we have to show that the limit
      $\lim_{n \to \infty} d(x_n, y_n)$ exists (in $\rr^+$) for all Cauchy
      sequences $\seq{x^{(n)}}{1}, \seq{y^{(n)}}{1}$. We already know
      that $\rr$ is complete, thus $\rr^{+}$ is complete as a closed
      subset of the complete space $\rr$ (Proposition 12.4.12(b)).

      Let be the sequence defined by $u_{n} := d(x_{n}, y_{n})$ for
      all $n \geq 1$. Obviously, this sequence is in $\rr^{+}$, which is
      a complete space. Thus, to show that it converges, we just have
      to show that it is a Cauchy sequence.

      Consider the usual metric on $\rr^{+}$. We have, for all
      $p,q \geq 1$,
      \begin{align*}
        |u_{p} - u_q| &= |d(x_p, y_p) - d(x_q, y_q)| \\
                      &\leq |d(x_p, x_q) + d(x_q, y_q) + d(y_q, y_p) -
                        d(x_q, y_q)| \\
                      &\leq |d(x_p, x_q)| + |d(y_p, y_q)|.
      \end{align*}
      Now let be $\epsilon > 0$. Since $\seq{x^{(n)}}{1}$ and
      $\seq{y^{(n)}}{1}$ are Cauchy sequences, there exists
      $N_1, N_2 \geq 1$ such that $d(x_p, x_q) \leq \epsilon/2$ whenever $p,q \geq
      N_1$, and $d(y_p, y_q) \leq \epsilon/2$ whenever $p,q \geq N_2$. Thus, if
      $p,q \geq N:= \max(N_1, N_2)$, we have
      \[|u_p - u_q| \leq |d(x_p, x_q)| + |d(y_p, y_q)| \leq \epsilon.\]
      This shows that $\seq{u_n}{1}$ is a Cauchy sequence, and thus,
      that $\lim_{n \to \infty} d(x_n, y_n)$ exists.
    \item Now we must show that the axiom of substitution is obeyed.
      In other words, consider a Cauchy sequence $\seq{z^{(n)}}{1}$ in
      $(X,d)$ such that $\formallimit z_n = \formallimit x_n$. We must
      show that
      $d_{\adh{X}}(\formallimit z_n, \formallimit y_n) =
      d_{\adh{X}}(\formallimit x_n, \formallimit y_n)$, i.e. that
      \begin{equation}
        \label{eq:12.4.8b}
        \lim_{n \to \infty} d(z_n, y_n) = \lim_{n \to \infty} d(x_n, y_n)
      \end{equation}
      By the previous bullet point, we know that both limits in
      \eqref{eq:12.4.8b} do exist. Thus, the limit laws apply. We
      have:
      \[d(z_n, y_n) \leq d(z_n, x_n) + d(x_n, y_n)\]
      but since $\lim_{n \to \infty} d(z_n, x_n) = 0$ by definition, we
      obtain
      \[\lim_{n \to \infty} d(z_n, y_n) \leq \lim_{n \to \infty} d(x_n, y_n)\]
      if we take the limits of both sides in the previous inequality.

      But similarly, we have $d(x_n, y_n) \leq d(x_n, z_n) + d(z_n,
      y_n)$, so that a similar argument gives
      \[\lim_{n \to \infty} d(x_n, y_n) \leq \lim_{n \to \infty} d(z_n, y_n).\]

      Thus, we have indeed $\lim_{n \to \infty} d(z_n, y_n) = \lim_{n \to \infty}
      d(x_n, y_n)$, as expected.
    \item Finally, we must show that $d_{\adh{X}}$ is a metric on
      $\adh{X}$. To prove this statement, we must show that
      $d_{\adh{X}}$ obeys all four axioms that define a metric.
      \begin{itemize}
      \item First, it is clear that
        $d_{\adh{X}}(\formallimit{x_n}, \formallimit{x_n}) = \lim_{n \to
          \infty} d(x_n, x_n) = 0$ for all Cauchy sequence
        $\seq{x^{(n)}}{1}$ in $(X,d)$.
      \item Now let be two Cauchy sequences $\seq{x^{(n)}}{1}$,
        $\seq{y^{(n)}}{1}$ in $X$, such that $\formallimit x_n \neq
        \formallimit y_n$. This latest property implies that $\lim_{n
          \to \infty} d(x_n, y_n) > 0$, by definition. Thus,
        $d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n}) > 0$.
      \item Symmetry: we have
        \begin{align*}
          d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n})
          &= \lim_{n \to \infty} d(x_n, y_n) \\
          &= \lim_{n \to \infty} d(y_n, x_n) \text{ (symmetry of $d$ on
            $\rr^+$)}\\
          &= d_{\adh{X}}(\formallimit{y_n}, \formallimit{x_n})
        \end{align*}
        for all Cauchy sequences $\seq{x^{(n)}}{1}$,
        $\seq{y^{(n)}}{1}$.
      \item Triangle inequality: by the limit laws, we have
        \begin{align*}
          &d_{\adh{X}}(\formallimit{x_n}, \formallimit{z_n})
            = \lim_{n \to \infty} d(x_n, z_n) \\
          &\leq \lim_{n \to \infty} (d(x_n, y_n) + d(y_n, z_n)) \\
          &\leq \lim_{n \to \infty} d(x_n, y_n) + \lim_{n \to \infty} d(y_n, z_n) \\
          &\leq d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n}) +
            d_{\adh{X}}(\formallimit{y_n}, \formallimit{z_n})
        \end{align*}
        for all Cauchy sequences $\seq{x^{(n)}}{1}$,
        $\seq{y^{(n)}}{1}$ and $\seq{z^{(n)}}{1}$.
      \end{itemize}
    \end{itemize}
    Thus, $d_{\adh{X}}$ is indeed a metric on $\adh{X}$.

  \item The metric space $(\adh{X}, d_{\adh{X}})$ is complete.

    To prove this statement, consider a Cauchy sequence $\seq{u_n}{1}$
    in $\adh{X}$: we have to prove that this sequence converges in
    $(\adh{X}, d_{\adh{X}})$.
    
    By definition, $\seq{u_n}{1}$ is a Cauchy sequence of formal
    limits of Cauchy sequences that take their values in $X$; i.e.,
    for all $k \geq 1$, there exists a Cauchy sequence
    $\seq{x_n^{(k)}}{1}$ of elements of $X$ such that
    $u_k := \formallimit x_n^{(k)}$.

    Since all $\seq{x_n^{(k)}}{1}$ are Cauchy sequences, then for all
    $k \geq 1$, there exists a threshold $N_k$ such that
    $d(x_n^{(k)}, x_{N_k}^{(k)}) < 1/k$ whenever $n \geq N_k$. Thus,
    (using the countable axiom of choice) we can build a sequence
    $(z_k)_{k=1}^\infty$ defined by
    \[z_k := \left(x_{N_k}^{(k)}\right)\]
    for all $k \geq 1$. Now:
    \begin{itemize}
    \item We claim that $(z_k)_{k=1}^\infty$ is itself a Cauchy sequence.
      Indeed, consider an arbitrary positive real number $\epsilon > 0$. We
      must prove that $d(z_p, z_q) := d(x_{N_p}^{(p)}, x_{N_q}^{(q)})$
      is eventually lesser than $\epsilon$.

      Since $\seq{u_n}{1}$ is a Cauchy sequence in $\adh{X}$, there
      exists a $N \geq 1$ such that, if $p,q \geq N$, we have
      $d_{\adh{X}}(u_p, u_q) < \epsilon/3$, i.e.:
      \begin{align*}
        \epsilon/3 &> d_{\adh{X}}(u_p, u_q) \\
            &\geq d_{\adh{X}} (\formallimit x_n^{(p)}, \formallimit
              x_n^{(q)}) \\
            &\geq \lim_{n \to \infty} d(x_n^{(p)}, x_n^{(q)})
      \end{align*}
      Thus, there exists a $N' \geq 1$ such that, if $n \geq N'$, we have
      $d(x_n^{(p)}, x_n^{(q)}) \leq \epsilon/3$\footnote{Indeed, for any
        sequence $\seq{v_n}{1}$ that converges to $\ell$, if we have
        $0 \leq \ell < \epsilon$, then there exists an $N \geq 1$ such that
        $v_n \leq \epsilon$ whenever $n \geq N$ (why? use a proof by
        contradiction.).\label{ft.limsuites}}. Also, by Exercise
      5.4.4, there exists a $k > 0$ such that $1/k \leq \epsilon/3$. Thus, if
      $n,p,q \geq \max(k, N', N_p, N_q)$, we have
      \begin{align*}
        d(z_p, z_q)
        &= d(x_{N_p}^{(p)}, x_{N_q}^{(q)}) \\
        &\leq \underbrace{d(x_{N_p}^{(p)}, x_n^{(p)})}_{\leq 1/p \, \leq \, \epsilon/3}
          + \underbrace{d(x_n^{(p)}, x_n^{(q)})}_{\leq \epsilon/3}
          + \underbrace{d(x_n^{(q)}, x_{N_q}^{(q)})}_{\leq 1/q \, \leq \, \epsilon
          /3} \\
        &\leq \epsilon/3 + \epsilon/3 + \epsilon/3\\
        &\leq \epsilon
      \end{align*}
      Thus, $(z_k)_{k=1}^\infty$ is indeed a Cauchy sequence in $X$.
    \item Consequently, we can take the formal limit $L :=
      \formallimit z_n$, and this formal limit $L$ lies in $\adh{X}$
      by definition. We claim that $\lim_{n \to \infty} u_n = L \in \adh{X}$;
      proving this claim will close the proof of (c).

      Let be $\epsilon > 0$. Since $\seq{z_n}{1}$ is a Cauchy sequence in
      $X$, there exists a $N_1 \geq 1$ such that
      $d(z_p, z_q) \leq \epsilon/2$ whenever $p,q \geq N_1$.

      Once again, by Exercise 5.4.4, there exists a $K' \geq 1$ such that
      $1/K' < \epsilon/2$. Thus, if $k \geq K$ and $n > N_k$, we have
      \[d(x_n^{(k)}, z_k) := d(x_n^{(k)}, x_{N_k}^{(k)}) < \frac{1}{k}
        \leq \frac{1}{K} < \frac{\epsilon}{2}.\]

      Thus, by the triangle inequality, we have, for all $n >
      \max(N_k, N_1)$,
      \[d(x_n^{(k)}, z_n) \leq d(x_n^{(k)}, z_k) + d(z_k, z_n) \leq \epsilon/2 +
        \epsilon/2 \leq \epsilon.\]

      Consequently, we have, for all $k > K'$,
      \[d_{\adh{X}}(u_k, L) := \lim_{n \to \infty} d(x_n^{(k)}, b_n) < \epsilon.\]
      This shows that $\seq{u_n}{1} \to L$ in
      $(\adh{X}, d_{\adh{X}})$, which closes the proof.
    \end{itemize}
    
  \item We identify an element $x \in X$ with the corresponding formal
    limit $\formallimit x$ in $\adh{X}$.
    \begin{itemize}
    \item This is legitimate since we have
      $x = y \iff \formallimit x = \formallimit y$.
        
        Indeed, it is clear that if $x = y$, then we have
        $\formallimit x = \formallimit y$ by definition. Conversely,
        if $\formallimit x = \formallimit y$, then we have
        $\lim_{n \to \infty} d(x,y) = 0$, i.e. $d(x,y) = 0$, i.e.
        $x = y$. Thus, this identification is legitimate.
      \item With this identification, we have $d(x,y) =
        d_{\adh{X}}(x,y)$. Indeed:
        \begin{align*}
          d_{\adh{X}}(x,y)
          &= d_{\adh{X}}(\formallimit x, \formallimit y) \\
          &= \lim_{n \to \infty} d(x,y) \\
          &= d(x,y).
        \end{align*}
      \end{itemize}
      Thus, $(X,d)$ can be thought of as a subspace of
      $(\adh{X}, d_{\adh{X}})$.
      
    \item The closure of $X$ in $\adh{X}$ is $\adh{X}$.
      
      Indeed, let be $C$ the closure of $X$ in $\adh{X}$. We clearly
      have $C \subseteq \adh{X}$, by definition. Thus we just have to show
      that $\adh{X} \subseteq C$.

      Let be $x \in \adh{X}$, and let's show that $x \in C$. By
      definition, $x \in C$ means that $x$ is an adherent point of $X$
      in $\adh{X}$, i.e. that for all $\epsilon > 0$,
      $B_{(\adh{X}, d_{\adh{X}})}(x, \epsilon) \cap X \neq \emptyset$. In other words, for
      all $\epsilon > 0$, we must show that there exists a
      $y \in X$ such that $d_{\adh{X}}(x,y) < \epsilon$.

      Thus, let be $\epsilon > 0$. By definition, $x$ is the formal limit of
      a Cauchy sequence $\seq{x_n}{1}$ of elements of $X$, so that
      $x := \formallimit x_n$. Since $\seq{x_n}{1}$ is a Cauchy
      sequence, there exists an $N \geq 1$ such that
      $d(x_n, x_N) < \epsilon/2$ whenever $n \geq N$. Thus:
      \begin{align*}
        d_{\adh{X}}(x, x_N)
        &:= d_{\adh{X}}(\formallimit x_n, \formallimit x_N) \\
        &= \lim_{n \to \infty} d(x_n, x_N) \\
        &\leq \epsilon/2 < \epsilon
      \end{align*}
      so that $y := x_N$ is a convenient choice. This shows that $x$
      is an adherent point of $X$ in $\adh{X}$, as expected.
      
    \item Finally, the formal limit agrees with the actual limit,
      i.e., $\lim_{n \to \infty} x_n = \formallimit x_n \in \adh{X}$ for all
      Cauchy sequence $\seq{x_n}{1}$ in $X$.

      Indeed, let be $\seq{x_n}{1}$ a Cauchy sequence of elements of
      $X$. We know that $(X,d)$ can be thought of as a subspace of
      $(\adh{X}, d_{\adh{X}})$, so that $\seq{x_n}{1}$ can be thought
      of as a sequence of elements of $\adh{X}$. But we have showed
      that $(\adh{X}, d_{\adh{X}})$ is complete. Thus, the sequence
      $\seq{x_n}{1}$ converges in $\adh{X}$ to a certain limit $L \in
      \adh{X}$; i.e., we have $\lim_{n \to \infty} x_n = L$ for some $L \in
      \adh{X}$.

      Consider this limit $L$. By definition of $\adh{X}$, there
      exists a Cauchy sequence $\seq{a_n}{1}$ of elements of $X$ such
      that $L := \formallimit a_n$. What we need to prove is that we
      have
      \begin{equation}
        \label{eq:12.4.8f}
        L = \lim_{n \to \infty} x_n = \formallimit a_n = \formallimit x_n
      \end{equation}
      and thus, it is sufficient to show that
      $\formallimit a_n = \formallimit x_n$, since we already have the
      other equalities. And, by definition of the equality relation
      established in (a), in order to prove that
      $\formallimit a_n = \formallimit x_n$, we just have to show that
      $\lim_{n \to \infty} d(x_n, a_n) = 0$. Or, in yet another equivalent
      way, we have to show that for all $\epsilon > 0$, there exists an $N \geq
      1$ such that $d(x_n, a_n) \leq \epsilon$ whenever $n \geq N$.

      Thus, let be an arbitrary $\epsilon > 0$. Let's unfold our hypotheses.
      \begin{itemize}
      \item We know that the sequence $\seq{x_n}{1}$ converges to $L$
        in $\adh{X}$. Thus, by definition, there exists a $N_1 \geq 1$
        such that $d_{\adh{X}}(x_k, L) \leq \epsilon/2$ whenever
        $k \geq N_1$. In other words,
        $\lim_{n \to \infty} d(x_k, a_n) \leq \epsilon/3 < \epsilon/2$ whenever $k \geq N_1$.

        Thus, there exists a $N_2$ such that
        $d(x_k, a_n) \leq \epsilon/2$ whenever $k \geq N_1$ and
        $n \geq N_2$ (see footnote \ref{ft.limsuites} p.
        \pageref{ft.limsuites} from the present document).
      \item We also know that $\seq{x_n}{1}$ is a Cauchy sequence. It
        means that there exists a $N_3 \geq 1$ such that
        $d(x_p, x_q) \leq \epsilon/2$ for all $p,q \geq N_3$.
      \end{itemize}
      Let be $N := \max(N_1, N_2, N_3)$. Using the triangle
      inequality, we finally get, for all $n \geq N$,
      \begin{align*}
        d(x_n, a_n) &\leq d(x_n, x_N) + d(x_N, a_n) \\
                    &\leq \epsilon/2 + \epsilon/2 \\
                    &\leq \epsilon
      \end{align*}
      This closes the proof.
    \end{enumerate}
\end{exo}

\begin{exo}{12.5.1}{Show that Definitions 9.1.22 and 12.5.3 match when
    talking about subsets of the real line with the standard metric.}

  Consider $Y \subseteq \rr$ and the standard metric $d(x,y) = |x-y|$ for all
  $x,y \in \rr$. We have to show that both definitions of boundedness
  are equivalent in this case.

  \begin{itemize}
  \item First, suppose that $Y$ is bounded in the sense of Definition
    12.5.3. Thus, there exists a real number $x$ and a positive real
    number $r > 0$ such that $Y \subseteq B(x,r)$. In other words, we have
    $Y \subseteq \, ]x-r, x+r[ \, \subseteq [x-r, x+r]$. Let be
    $M := |x| + |r|$. We clearly have $x+r \leq M$, and $-M \leq x-r$. Thus,
    we have $Y \subseteq [-M, M]$, and $Y$ is bounded in the sense of
    Definition 9.1.22.
  \item Conversely, suppose that $Y$ is bounded in the sense of
    Definition 9.1.22. Thus, there exists a positive real $M > 0$ such
    that $Y \subseteq [-M, M] \subset ]-2M, 2M[$. But this later interval is simply
    $B(0, 2M)$, so that $Y$ is bounded in the sense of Definition
    12.5.1, taking $x := 0$ and $r := 2M$.
  \end{itemize}
\end{exo}

\begin{exo}{12.5.2}{Prove Proposition 12.5.5.}

  We must prove that any compact space $(X,d)$ is both complete and
  bounded. In both cases, we will use a proof by contradiction.

  \begin{itemize}
  \item First, let's prove completeness. Suppose, for the sake of
    contradiction, that the compact space $(X,d)$ is not complete.
    Since it is not complete, there exists a Cauchy sequence
    $\seq{x^{(n)}}{1}$ of elements of $X$ which does not converge in
    $(X,d)$. But since it is compact, there exists a subsequence
    $(x^{(n_k)})_{k=1}^\infty$ of this Cauchy sequence, which converges in
    $(X,d)$. But, by Lemma 12.4.9, if a Cauchy sequence has a
    convergent subsequence, then it is convergent itself; thus
    $\seq{x^{(n)}}{1}$ converges. It is a clear contradiction. Thus,
    $(X,d)$ must be complete.
  \item Now we show boundedness. Similarly, suppose for the sake of
    contradiction that $(X,d)$ is not bounded. It means that, for all
    positive real $r>0$ and all $x \in X$, we have
    $X \not \subseteq B(x,r)$. In particular, for any positive natural number
    $n \geq 1$ and an arbitrary $x \in X$, the set
    $X \backslash B(x,n)$ is not empty. Thus, using the (countable) axiom of
    choice, we can build a sequence $\seq{x^{(n)}}{1}$ such that
    $x^{(n)} \in X \backslash B(x,n)$ for all positive integer
    $n \geq 1$. Or, in other words, we have $d(x, x^{(n)}) \geq n$ for all
    $n \geq 1$.

    But recall that $(X,d)$ is compact. Thus, there must exist a
    convergent subsequence $(x^{(n_k)})_{k=1}^\infty$ of the original
    sequence. Say that this subsequence converges to some value $L$.
    Thus, by definition,
    \[\forall \epsilon > 0, \exists K \geq 1 \, : \, k \geq K \implies d(x^{(n_k)}, L) \leq \epsilon.\]
    Let's take $\epsilon := 1$ (there is nothing special about this value;
    this is just any arbitrary $\epsilon$ to obtain a contradiction). There
    must exist a $K_{1} \geq 1$ such that $d(x^{(n_k)}, L) \leq 1$ whenever
    $k \geq K_{1}$. But, at the same time, we have by the triangle
    inequality
    \begin{align*}
      d(x^{(n_k)}, x) &\leq d(x^{(n_k)}, L) + d(L, x)\\
      \implies d(x^{(n_k)}, L) &\geq d(x^{(n_k)}, x) - d(L,x)
    \end{align*}

    For instance by the Archimedean principle, there exists an $N \in
    \nn$ such that $N \geq d(L,x) + 3$. Let be $K_2 := \min\{ k \in \nn :
    n_k \geq N\}$ (this natural number exists simply because $n_N \geq N$,
    so that the set is not empty). We thus have
    \[d(x, x^{(n_k)}) \geq n_k \geq N \geq d(L,x) + 3\]
    for all $k \geq K_2$.

    Thus, for all $k \geq \max(K_1, K_2)$, we have both $d(x^{(n_k)}, x)
    \leq 1$ (because $k \geq K_1$), and $d(x^{(n_k)}, L) \geq d(x^{(n_k)}, x) -
    d(L,x) \geq d(L,x) + 3 - d(L,x) \geq 3$ (because $k \geq K_2$). This is a
    contradiction. Thus, $(X,d)$ is bounded.
  \end{itemize}
\end{exo}

\begin{exo}{12.5.3}{Prove Theorem 12.5.7.}

  Let be $(\rr^{n}, d)$ an Euclidean space, where $d$ is either the
  Euclidean, taxicab or sup norm metric. Also, let be
  $E \subseteq \rr^{n}$. We have to prove that $E$ is compact iff $E$ is
  closed and bounded. By Corollary 12.5.6, we already know that if $E$
  is compact, then it is closed and bounded. We thus have to prove the
  converse implication.

  Suppose that $E$ is both closed and bounded. Since $E$ is a subset
  of $\rr^{n}$, we can write $E := E_{1} \times \ldots \times E_{n}$, where
  $E_{j} \subseteq \rr$ for all $1 \leq j \leq n$.

  We have to prove that any sequence $(x^{(k)})_{k=1}^{\infty}$ in $E$ has
  a convergent subsequence in $(E, d)$. This sequence can be written
  as a sequence of vectors of length $n$, i.e., we have
  $x^{(k)} = (x_{1}^{(k)}, \ldots, x_{n}^{(k)})$, where
  $x_{j}^{(k)} \in E_j$ for all $k \geq 1$ and all $1 \leq j \leq n$.

 We will first need a lemma:

  \smallskip
  \textbf{Lemma}. If $E$ is bounded, then each $E_j \subseteq \rr$ is also
  bounded.

  \begin{proof}[Sketch of proof]
    Suppose that $d$ is the sup norm metric. If $E$ is bounded, we
    have $E \subseteq B(x, r)$ for some $x \in \rr^{n}$ and some $r > 0$
    (Definition 12.5.3). In other words, we have $d(x, y) < r$ for all
    $y \in E$. Since $d$ is the sup norm metric, this implies that
    \[\forall j \in \llbracket 1, n \rrbracket, \,
      |x_{j} - y_{j}| \leq \max_{j = 1, \ldots, n} |x_{j} - y_{j}| := d(x,y) <
      r.\] Thus, $E_j \subseteq B(x_{j}, r)$, i.e. $E_{j}$ is bounded for all
    $1 \leq j \leq n$.

    The proof is similar if $d$ is the Euclidean metric, or the
    taxicab metric.
  \end{proof}

  Now we go back to the main proof. Since each sequence
  $(x_{j}^{(k)})_{k=1}^{\infty}$ is a sequence of real numbers in the
  bounded subset $E_{j} \subseteq \rr$, then by Theorem 9.1.24 this sequence
  has a convergent subsequence $(x_{j}^{(k_{l})})_{l=1}^{\infty}$, which
  converges to $L_{j} \in \rr_{j}$. But by Proposition 12.1.18, this
  implies that the whole subsequence $(x^{(k_{l})})_{l=1}^{\infty}$ converges
  to $(L_{1}, \ldots, L_{n})$ (since it converges component-wise).

  Thus, $(x_{j}^{(k)})_{k=1}^{\infty}$ indeed has a convergent subsequence,
  as expected; and $E$ is compact.
\end{exo}

\bigskip
\begin{exo}{12.5.4}{Let $(\rr, d)$ be the real line with the standard
    metric. Give an example of a continuous function $f : \rr \to
    \rr$, and an open set $V \subseteq \rr$, such that the image $f(V) :=
    \{f(x) : x \in V\}$ of $V$ is not open.}

  As a simple example, consider the constant function $f(x) = 0$
  defined on $V := ]-1, 1[$. The interval $V$ is clearly open, but we
  have $f(V) = \{0\}$. This singleton (or more generally, any
  singleton) is not open in $(\rr, d)$, since for all $r>0$, there
  always exists a real number $x$ such that $x \in B(0, r) \backslash \{0\}$.
\end{exo}

\bigskip
\begin{exo}{12.5.5}{Let $(\rr, d)$ be the real line with the standard
    metric. Give an example of a continuous function $f: \rr \to \rr$, and
    closed set $F \subseteq \rr$, such that $f(F)$ is not closed.}

  One can give the example of the function $\tan^{-1}(x)$ defined on
  the closed set $F := \rr$, but this function has not really been
  defined so far in the book. So, let's use a simpler example.
  
  Consider the closed set $F := [1, +\infty[$ and the function $f(x) =
  1/x$. We have $f(F) = ]0, 1]$, which is not a closed set.  
\end{exo}

\pagebreak
\begin{exo}{12.5.6}{Prove Corollary 12.5.9.}

  Consider a sequence $K_1 \supset K_2 \supset K_3 \supset \ldots$ of non-empty compact sets
  in a metric space $(X,d)$. We have to show that $\bigcap_{n=1}^\infty K_n \neq \emptyset$.

  Let's work in the space $(K_1, d_{K_1 \times K_1})$. We define the sets
  $V_n := K_1 \setminus K_n$ for all $n \geq 1$, i.e.,
  \begin{align*}
    V_1 &:= K_1 \setminus K_1 = \emptyset \\
    V_2 &:= K_1 \setminus K_2 \\
    V_3 &:= K_1 \setminus K_3 \\
        & \ldots
  \end{align*}
  so that the $V_n$ clearly constitute an increasing sequence:
  \[V_1 \subseteq V_2 \subseteq V_3 \subseteq \ldots,\]
  so that $\bigcup_{k = 1}^n V_k = V_n$ for all $n \geq 1$.
  
  Furthermore, each set $V_n$ is open in $(K_1, d_{K_1 \times K_1})$, since
  it is the complementary set of a compact (and then closed) set
  (Proposition 12.2.15 (e)).

  Suppose, for the sake of contradiction, that we have $\bigcap_{n=1}^\infty K_n
  = \emptyset$. We would thus have:
  \begin{align*}
    \bigcup_{n=1}^\infty V_n &= \bigcup_{n=1}^\infty (K_1 \setminus K_n) \\
                  &= K_1 \setminus \left(\bigcap_{n=1}^\infty K_n\right) \text{ (Exercise 3.4.11)}\\
                  &= K_1 \setminus \emptyset \text{ (by hypothesis)} \\
                  &= K_1.
  \end{align*}

  But since $K_1$ is compact, then by Theorem 12.5.8, there exists a
  finite open cover of $K_1$, i.e., there exists a finite number $k$
  of indices $n_1 < \ldots < n_k\}$ such that
  \[\bigcup_{n \in \{n_1, \ldots, n_k\}} V_n = K_1.\]

  But since the $V_n$ form an increasing sequence, this implies
  $V_{n_k} = K_1$, i.e., $K_1 \setminus K_{n_k} = K_1$, so that we finally get
  $K_{n_k} = \emptyset$.

  But all the sets $K_n$ were supposed to be non empty: this is thus a
  contradiction, and we must have $\bigcap_{n=1}^\infty K_n \neq \emptyset$.
\end{exo}

\bigskip
\begin{exo}{12.5.7}{Prove Theorem 12.5.10.}

  Let be $(X,d)$ a metric space.

  \begin{enumerate}[label=(\alph*)]
  \item Let be $Z \subseteq Y \subseteq X$, with $Y$ compact. We have to show that $Z$
    is closed iff it is compact. We already know that if $Z$ is
    compact, then it is closed (Corollary 12.5.6); so that we just
    have to show the converse implication.

    Suppose that $Z$ is closed, and let be $(z^{(n)})_{n=1}^{\infty}$ a
    sequence of elements of $Z$. Since $Z \subseteq Y$, $(z^{(n)})_{n=1}^{\infty}$
    is also a sequence of elements of $Y$; and since $Y$ is compact,
    there exists a subsequence $(z^{(n_{k})})_{k=1}^{\infty}$ that
    converges to some $z \in Y$. But since $Z$ is closed, we must have
    $z \in Z$ (by Proposition 12.2.15(b)). Thus, any sequence of
    elements of $Z$ has a subsequence that converges in $Z$, i.e., $Z$
    is indeed compact.
    
  \item Let be $Y_1, \ldots, Y_n$ be $n$ compact subsets of $X$; we have to
    show that the finite union $Y_1 \cup \ldots \cup Y_n$ is compact. Let's
    use the topological characterization of compact sets: suppose that
    we have an open cover $\bigcup_{\alpha \in I} V_\alpha$ (possibly uncountable), i.e.
    that
    \[Y_1 \cup \ldots \cup Y_n \subseteq \bigcup_{\alpha \in I} V_\alpha.\]

    Clearly, we have $Y_1 \subseteq  \bigcup_{\alpha \in I} V_\alpha$, and since $V_1$ is
    compact, there exists a finite open cover, i.e. $Y_1 \subseteq  \bigcup_{i =
      1}^{s_1} V_{a_i}$. Similarly, there exist finite open covers for
    each other subset $Y_i$, i.e.,
    \begin{align*}
      Y_2 &\subseteq  \bigcup_{i = 1}^{s_2} V_{b_i} \\
          & \ldots \\
      Y_n &\subseteq \bigcup_{i = 1}^{s_n} V_{n_i}.
    \end{align*}
    Thus, there exists a finite open cover
    \[Y_1 \cup \ldots \cup Y_n \subseteq \bigcup_{\alpha \in \{a_1, \ldots, a_{s_1}, b_1, \ldots, b_{s_2}, \ldots,
        n_1, \ldots, n_{s_n}\}} V_\alpha\]
    so that $Y_1 \cup \ldots \cup Y_n$ is indeed compact.
    
  \item Let be $Y$ a finite subset of $X$; we have to show that $Y$ is
    compact.

    First, suppose that $Y$ is a singleton $\{a\}$. By definition, any
    sequence of elements of $Y$ can only be the constant sequence $a,
    a, a, \ldots$. Thus, any subsequence of this sequence is still the
    constant sequence $a, a, \ldots$, and still converges to $a$. Thus, any
    sequence of elements of $Y$ has a subsequence that converges in
    $Y$, i.e., $Y$ is compact.

    Now suppose that $Y$ is a finite subset of cardinality $n$. Let's
    write $Y := \{y_1, \ldots, y_n\}$. This can also be written
    $Y := \{y_1\} \cup \ldots \cup \{y_n\}$, so that we are back in the previous
    case (b): $Y$ is the finite union of compact subsets of $X$. Thus,
    $Y$ is itself compact.

    Note that for the limit case $Y = \emptyset$, we can say that the empty
    set is just a closed\footnote{See Remark 12.2.14.} subset of the
    compact set $\{a\}$, so that by the previous case (a), $Y = \emptyset$ is
    compact.
  \end{enumerate}
\end{exo}

\begin{exo}{12.5.8}{Let $(X, d_{l^1})$ be the metric space from
    Exercise 12.1.15. For each natural number $n$, let
    $e^{(n)} = (e^{(n)}_j)_{j=0}^\infty$ be the sequence in $X$ such that
    $e^{(n)}_j := 1$ when $n = j$ and $e^{(n)}_j := 0$ when
    $n \neq j$. Show that the set $\{e^{(n)} : n \in \nn\}$ is a closed and
    bounded subset of $X$, but is not compact.}

  Recall that $(X,d_{l^1})$ is the metric space of absolutely
  convergent sequences, with the metric defined by
  $d_{l^1}((a^{(n)}), (b^{(n)})) := \sum_{n=0}^\infty |a_n - b_n|$. Hereafter,
  we denote $E := \{e^{(n)} : n \in \nn\}$, with
  \begin{align*}
    e^{(0)} &:= 1, 0, 0, 0, \ldots \\
    e^{(1)} &:= 0, 1, 0, 0, \ldots \\
    e^{(2)} &:= 0, 0, 1, 0, \ldots \\
    &\ldots
  \end{align*}

  \begin{itemize}
  \item First, we show that $E$ is not compact. To prove this
    statement, we just have to find one sequence of elements of $E$
    that has no convergent subsequence in $E$.

    Consider the ``canonical'' sequence of elements of $E$ defined by
    $e^{(0)}, e^{(1)}, e^{(2)}, \ldots$. The distance between any two
    distinct elements of this sequence is
    \[d_{l^{(1)}}(e^{(j)}, e^{(k)}) := \sum_{i=0}^\infty |e_i^{(j)} -
      e_i^{(k)}| = 2 > 1.\]

    Thus, this sequence is not a Cauchy sequence itself, and it is
    clear that no subsequence can be a Cauchy sequence either. Thus,
    no subsequence of this sequence can converge in $E$, i.e., $E$ is
    not compact.
    
  \item However, $E$ is a closed subset of $X$. To prove this
    property, consider a convergent sequence of elements of $E$; we
    have to prove that its limit lies in $E$. We've just shown that
    the distance between any two distinct terms $e^{(j)}, e^{(k)}$ for
    $j \neq k$ is equal to $2$. Thus, if a sequence of elements of $E$
    converges, it must be eventually $0.5$-stable, and the only
    possibility for that is to be eventually constant. In other words,
    it must be eventually equal to $e^{(n_0)}$ for $n_0 \in \nn$, so
    that it necessarily converges to $e^{(n_0)}$, which is an element
    of $E$. This shows that $E$ is closed.
    
  \item Furthermore, $E$ is bounded. To show the boundedness of $E$,
    we have to show that $E \subseteq B_{(X, d_{l^1})}((x_j)_{j=0}^\infty, r)$
    for some $r > 0$ and some sequence $(x_j)_{j=0}^\infty \in X$.
    Consider the zero sequence $(z_j)_{j=0}^\infty := 0, 0, 0, \ldots$. This
    is clearly a sequence in $X$ (since it converges to $0$), and we
    have
    \[d_{l^1}\left((z_j)_{j=0}^\infty, (e^{(n)}_j)_{j=0}^\infty\right)
      = \sum_{j=0}^\infty |z_j - e^{(n)}_j| = 1 < 2\]
    for all $n \in \nn$. Thus, we have $E \subseteq B_{(X,
      d_{l^1})}((z_j)_{j=0}^\infty, 2)$, which shows that $E$ is bounded.
  \end{itemize}

  Thus, the case of the subset $E$ of the metric space $(X, d_{l^1})$
  shows that the Heine-Borel theorem (stated for the metric space
  $(\rr^n, d)$) is not valid in more general metric spaces.
\end{exo}

\bigskip
\begin{exo}{12.5.9}{Show that a metric space $(X, d)$ is compact if
    and only if every sequence in $X$ has at least one limit point.}

  A metric space $(X,d)$ is compact iff any sequence of elements of
  $X$ has a subsequence that converges in $(X,d)$. Thus, the statement
  is a direct consequence of Proposition 12.4.5, which says basically
  that ``having a convergent subsequence'' and ``having a limit
  point'' are synonymous.
\end{exo}

\bigskip
\begin{exo}{12.5.13}{Let $E$ and $F$ be two compact subsets of $\rr$
    (with the standard metric $d(x, y) = |x - y|$). Show that the
    Cartesian product $E \times F := \{(x, y) : x \in E, y \in F \}$ is a
    compact subset of $\rr^2$ (with the Euclidean metric $d_{l^2}$).}

  To prove that $E \times F$ is compact, we will show that it is both
  closed and bounded (by Heine-Borel theorem).

  \begin{itemize}
  \item First we show that $E \times F$ is bounded.

    Since $E$ and $F$ are compact, they are themselves bounded (by
    Heine-Borel theorem). Thus, there exist $a \in E$, $b \in F$ and $r_1,
    r_2 > 0$ such that $E \subseteq B_d(a, r_1)$ and $F \subseteq B_d(b, r_2)$, by
    Definition 12.5.3. In other words, we have:
    \begin{align*}
      \forall x \in E, & |x-a| < r_1 \\
      \forall y \in F, & |y-b| < r_2.
    \end{align*}
    Thus, let be $(x, y) \in E \times F$. We have:
    \begin{align*}
      d_{l^2}\left( (x,y), (a,b) \right)
      &= \sqrt{(x-a)^2 + (y-b)^2} \\
      &< \sqrt{r_1^2 + r_2^2}.
    \end{align*}
    This means that each $(x,y) \in E \times F$ lies in
    $B_{d_{l^2}}\left((a,b), \sqrt{r_1^2 + r_2^2}\right)$. Thus,
    $E \times F$ is indeed bounded.
    
  \item Now let's show that $E \times F$ is closed.

    Since $E$ and $F$ are compact, they are themselves closed (by
    Heine-Borel theorem). Consider a sequence
    $\seq{(x^{(n)}, y^{(n)})}{1}$ of elements of $E \times F$ which
    converges to $(x_0, y_0)$ with respect to $d_{l^2}$. By
    Proposition 12.1.18, this means that this sequence converges
    component-wise, i.e. that $\seq{x^{(n)}}{1}$ converges to $x_0$,
    and $\seq{y^{(n)}}{1}$ converges to $y_0$. By definition, we have
    $x_0 \in E$ and $y_0 \in F$, since $E$ and $F$ are closed. Thus,
    $(x_0, y_0) \in E \times F$. This shows that $E \times F$ is indeed bounded.
  \end{itemize}

  Thus, $E \times F$ is compact, as expected.
\end{exo}

\pagebreak
\section{Continuous functions on metric spaces}
\label{cha:cont-funct-metr}

\begin{exo}{13.1.1}{Prove Theorem 13.1.4.}

  Since the implication $(b) \implies (c)$ may be slightly more
  difficult to write, we will prove the implications $(a) \implies
  (c)$, $(c) \implies (b)$ and $(b) \implies (a)$ in this order.

  Let be $f : (X, d_X) \to (Y, d_Y)$, and $x_0 \in X$.

  \begin{itemize}
  \item First let's prove $(a) \implies (c)$. Suppose that $f$ is
    continuous at $x_0$, and let be $V \subseteq Y$ an open set that contains
    $f(x_0)$. By Proposition 12.2.15(a), there exists a $\epsilon > 0$ such
    that $B_Y(f(x_0), \epsilon) \subseteq V$. But since $f$ is continuous at
    $x_0$, we know that there exists a $\delta > 0$ such that
    $d_X(x, x_0) < \delta \implies d_Y(f(x), f(x_0)) < \epsilon$. Thus, if we set
    $U := B_X(x_0, \delta)$, we have found an open set $U \subseteq X$ such that
    $f(U) \subseteq B_Y(f(x_0), \epsilon) \subseteq V$, as required.
  \item Now we prove $(c) \implies (b)$. Consider a sequence
    $\seq{x^{(n)}}{1}$ in $X$ which converges to $x_0$ with respect to
    $d_X$. Let be an arbitrary $\epsilon > 0$; we set
    $V_\epsilon := B_Y(f(x_0), \epsilon)$. By $(c)$, we know that there exists an open
    set $U \subseteq X$ containing $x_0$ and such that $f(U) \subseteq V_\epsilon$. But since
    $U$ is open set, by Proposition 12.2.15(a), there exists a $\delta > 0$
    such that $B_X(x_0, \delta) \subseteq U$.

    Since $\seq{x^{(n)}}{1}$ converges to $x_0$, there exists a
    natural number $N \geq 1$ such that $d_X(x^{(n)}, x_0) < \delta$ whenever
    $n \geq N$. Or, in other words, we have $x^{(n)} \in B_X(x_0, \delta) \subseteq U$
    whenever $n \geq N$.

    But since $f(U) \subseteq V$ by hypothesis, we thus have $f(x^{(n)}) \in V_\epsilon$
    whenever $n \geq N$. Since this is true for any arbitrary $\epsilon > 0$,
    this shows that the sequence $\seq{f(x^{(n)})}{1}$ converges to
    $f(x_0)$ with respect to $d_Y$, as expected.
    
  \item Finally, we prove $(b) \implies (a)$. Suppose that
    $\seq{f(x^{(n)})}{1}$ converges to $f(x_0)$ whenever
    $\seq{x^{(n)}}{1}$ converges to $x_0$, and let's show that $f$ is
    continuous at $x_0$.

    Suppose, for the sake of contradiction, that $f$ is \emph{not}
    continuous at $x_0$. Thus, there exists an $\epsilon > 0$ such that for
    all $\delta > 0$, there exists an $x \in X$ such that
    $d_Y(f(x), f(x_0)) \geq \epsilon$ although $d_X(x, x_0) < \delta$.

    Thus, using the (countable) axiom of choice, we build a sequence
    $\seq{x^{(n)}}{1}$ such that, for all $n \geq 1$, we have
    $d_Y(f(x^{(n)}), f(x_0)) \geq \epsilon$ although
    $d_X(x^{(n)}, x_0) < \frac{1}{n}$. It is thus clear that
    $\seq{x^{(n)}}{1}$ converges to $x_0$, but that
    $\seq{f(x^{(n)})}{1}$ does not converge to $f(x_0)$, since
    $f(x^{(n)})$ and $f(x_0)$ are never $\epsilon/2$-close. This is a
    contradiction with $(c)$. Thus, $f$ must be continuous at $x_0$,
    as expected.
  \end{itemize}  
\end{exo}

\begin{exo}{13.1.2}{Prove Theorem 13.1.5.}

  We already know from Theorem 13.1.4 that $(a)$ and $(b)$ are
  equivalent. Let's prove the other implications.

  \begin{itemize}
  \item First we prove that $(a) \implies (c)$. Let be $V$ an open set
    in $Y$. We must show that $f^{-1}(V)$ is an open set in $X$. Thus,
    if we take an arbitrary $x_0 \in f^{-1}(V)$, we must show that there
    exists an $r_0 > 0$ such that $B_X(x_0, r_0) \subseteq f^{-1}(V)$ (cf.
    Theorem 12.2.15(a)).

    Consider this arbitrary $x_0 \in f^{-1}(V)$. By definition, we have
    $f(x_0) \in V$. But since $V$ is an open set, there exists an $\epsilon >
    0$ such that $B_Y(f(x_0), \epsilon) \subseteq V$.

    But $f$ is continuous: for this $\epsilon > 0$, there exists a
    $\delta > 0$ such that, for $x \in X$, we have
    $d_X(x_0, x) < \delta \implies d_Y(f(x_0), f(x)) < \epsilon$. In other words,
    we have $x \in B_X(x_0, \delta) \implies f(x) \in B_Y(f(x_0), \epsilon) \subseteq V$.

    Thus, if we set $r_0 := \delta$, we are done: for all $x \in B_X(x_0,
    r_0)$, we have $f(x) \in V$, i.e. $x \in f^{-1}(V)$. This shows that
    $B_X(x_0, \delta) \subseteq f^{-1}(V)$, and thus that $f^{-1}(V)$ is an open
    set, as expected.
  \item Now we show that $(c) \implies (d)$. By Theorem 12.2.15(e), we
    know that $F \subseteq X$ is closed iff $X \setminus F$ is open. Thus, consider
    $F \subseteq Y$ a closed set in $Y$. Let be $V := Y \setminus F$ its complementary
    set, which is thus an open set. By $(c)$, the set $f^{-1}(V)$ is
    an open set in $X$. But we have :
    \begin{align*}
      f^{-1}(F) &= \{x \in X : f(x) \in F\} \\
                &= \{x \in X : f(x) \in Y \setminus V\} \\
                &= \{x \in X : f(x) \notin V\}
    \end{align*}
    so that $f^{-1}(F) = X \setminus f^{-1}(V)$. Since $f^{-1}(F)$ is the
    complementary set of the open set $f^{-1}(V)$, it is closed in $X$,
    as expected.
  \item The implication $(d) \implies (c)$ can be shown in exactly the
    same way as above.
  \item Finally, let's show that $(c) \implies (a)$. Let be $\epsilon > 0$,
    let be $x_0 \in X$. Consider $V := B_Y(f(x_0), \epsilon)$, which is an open
    set in $Y$. By $(c)$, the set $f^{-1}(V)$ is open in $X$. Thus, by
    Theorem 12.2.15(a), there exists a $\delta > 0$ such that $B_X(x_0, \delta)
    \subseteq f^{-1}(V)$. Thus, if $x \in B_X(x_0, \delta)$, we have $f(x) \in V$.

    In other words, for any $\epsilon > 0$, there exists a $\delta > 0$ such that
    $d_X(x, x_0) < \delta \implies d_Y(f(x), f(x0)) < \epsilon$. This shows that
    $f$ is continuous at $x_0$, for any arbitrary $x_0 \in X$, as
    expected.
  \end{itemize}
\end{exo}

\begin{exo}{13.1.3}{Use Theorem 13.1.4 and Theorem 13.1.5 to prove
    Corollary 13.1.7.}
  
  To show (a), consider $\seq{x^{(n)}}{1}$ a sequence of elements of
  $X$ that converges to $x_0 \in X$. Since $f$ is continuous at
  $x_0$, then by Theorem 13.1.4(b), we know that
  $\seq{f(x^{(n)})}{1}$ converges to $f(x_0) \in Y$. But
  $\seq{f(x^{(n)})}{1}$ is a sequence of elements of $Y$. Since $g$
  is continuous at $f(x_0)$, then still by Theorem 13.1.4(b), we
  know that $\seq{g(f(x^{(n)}))}{1}$ converges to $g(f(x_0)) \in Z$.
  
  Thus, we have proved that for any sequence $\seq{x^{(n)}}{1}$ of
  elements of $X$ that converges to $x_0 \in X$, the sequence $\seq{g
    \circ f(x^{(n)})}{1}$ converges to $g \circ f(x_0)$. This shows that $g \circ
  f$ is continuous at $x_0$, as expected.
  
  Once (a) is proved, the result (b) is clear, since it is just (a)
  at any arbitrary $x_0 \in X$.
\end{exo}

\bigskip
\begin{exo}{13.1.4}{Give an example of functions $f : \rr \to \rr$ and
    $g : \rr \to \rr$ such that (a) $f$ is not continuous, but $g$ and
    $g \circ f$ are continuous; (b) $g$ is not continuous, but $f$ and
    $g \circ f$ are continuous; (c) $f$ and $g$ are not continuous, but
    $g \circ f$ is continuous. Explain brieﬂy why these examples do not
    contradict Corollary 13.1.7.}

  Here, the simplest way is to use piecewise constant functions, at
  least for one of the functions $f,g$.

  \begin{enumerate}[label=(\alph*)]
  \item Let be, for instance,
    \[f(x) :=
      \begin{cases}
        0  &\text{ if } x < 0 \\
        1 & \text{ if } x \geq 0
      \end{cases}
    \]
    and the constant function $g(x) := 3$. We thus have $g \circ f(x) = 3$
    for all $x \in \rr$, so that $g \circ f$ is continuous.
  \item Let be, for instance,
    \[g(x) :=
      \begin{cases}
        -1  &\text{ if } x < 0 \\
        1 & \text{ if } x \geq 0
      \end{cases}
    \]
    and $f(x) := x^2 + 1$. We thus have $g \circ f(x) = 1$ for all
    $x \in \rr$, so that $g \circ f$ is continuous.
  \item Let be, for instance,
    \[f(x) :=
      \begin{cases}
        0  &\text{ if } x < 0 \\
        3 & \text{ if } x \geq 0
      \end{cases}
    \]
    and
    \[g(x) :=
      \begin{cases}
        -1  &\text{ if } x < 0 \\
        1 & \text{ if } x \geq 0.
      \end{cases}
    \]
    We thus have $g \circ f(x) = 1$ for all $x \in \rr$, so that
    $g \circ f$ is continuous.
  \end{enumerate}

  This does not contradict Corollary 13.1.7, since the initial
  hypothesis of this corollary is that both functions $f,g$ are
  continuous, and it says nothing about non discontinuous functions.
\end{exo}

\bigskip
\begin{exo}{13.1.5}{Let $(X, d)$ be a metric space, and let
    $(E, d|_{E \times E})$ be a subspace of $(X, d)$. Let
    $\iota_{E \to X} : E \to X$ be the inclusion map, defined by setting
    $\iota_{E \to X} (x) := x$ for all $x \in E$. Show that
    $\iota_{E \to X}$ is continuous.}

  Let be $x_0 \in E$ an arbitrary point in $E$, and let be $\epsilon > 0$ a
  positive real number. Note that we have, for all $x \in E$, 
  \[d(\iota_{E \to X}(x_0), \iota_{E \to X}(x)) = d_{E \times E}(x_0, x).\]

  Thus, if we take $\delta := \epsilon$ in Definition 13.1.1 of continuity, we are
  done: if $d_{E \times E}(x_0, x) < \epsilon$, we automatically have $d(\iota_{E \to
    X}(x_0), \iota_{E \to X}(x)) < \epsilon$, so that $\iota_{E \to X}$ is continuous at
  any arbitrary $x_0 \in E$, as expected.
\end{exo}

\bigskip
\begin{exo}{13.1.6}{Let $f : X \to Y$ be a function from one metric
    space $(X, d_X)$ to another $(Y, d_Y)$. Let $E$ be a subset of $X$
    (which we give the induced metric $d_X|_{E \times E}$), and let
    $f|_E : E \to Y$ be the restriction of $f$ to $E$, thus
    $f|_E (x) := f (x)$ when $x \in E$. If $x_0 \in E$ and $f$ is
    continuous at $x_0$, show that $f|_E$ is also continuous at $x_0$.
    (Is the converse of this statement true? Explain.) Conclude that
    if $f$ is continuous, then $f|_E$ is continuous.}

  Let's use Exercise 13.1.5. First we note that we have
  $f|_E = f \circ \iota_{E \to X}$. Indeed, $f \circ \iota_{E \to X}$ is a function from
  $E$ to $Y$ just like $f$, and for all $x \in E$, we clearly have
  $f \circ \iota{E \to X} (x) = f(x) = f|_E(x)$.

  We have shown in Exercise 13.1.5 that $\iota_{E \to X}$ is continuous at
  any $x_0 \in E$, and $f$ is supposed to be continuous at $x_0 \in E$.
  Thus, by Corollary 13.1.7, $f|_E$ is continuous at $x_0$ since it is
  the composition of two continuous functions. Since this is true for
  any arbitrary $x_0 \in E$, the function $f|_E$ is continuous on $E$.

  The converse statement is not true: consider the piecewise constant
  function $f : \rr \to \rr$ defined by $f(x) = -1$ if $x < 0$ and
  $f(x) = 1$ if $x \geq 0$ for all $x \in \rr$; and let be
  $E := [0, +\infty[$. The restriction $f|_E$ is clearly continuous (as a
  constant function) at $0$, but the function $f$ itself is clearly
  not continuous at $0$.
\end{exo}

\bigskip
\begin{exo}{13.2.1}{Prove Lemma 13.2.1.}

  Here we just have to prove the statement (a), since the statement
  (b) is essentially (a) applied to any arbitrary $x_0 \in X$.

  \begin{itemize}
  \item First suppose that $f$ and $g$ are both continuous at
    $x_0 \in X$, and let be $\seq{x{^{(n)}}}{1}$ a sequence of elements
      of $X$. Then, by Theorem 13.1.4(b), the sequence
      $\seq{f(x^{(n)})}{1}$ converges to $f(x_0)$ in $\rr$, and the
      sequence $\seq{g(x^{(n)})}{1}$ converges to $g(x_0)$ in $\rr$.

      Thus, by Theorem 12.1.18, the sequence
      $\seq{(f(x^{(n)}), g(x^{(n)}))}{1}$ of elements of $\rr^2$
      converges to $(f(x_0), g(x_0))$ in $\rr^2$ with respect to the
      metric $d_{l^2}$ (since it converges component-wise). In other
      words, for any arbitrary sequence $\seq{x^{(n)}}{1}$ that
      converges to $x_0$, the sequence $\seq{f \oplus g(x^{(n)})}{1}$
      converges to $(f(x_0), g(x_0)) = f \oplus g (x_0)$. Thus,
      $f \oplus g$ is continuous at $x_0$, by Theorem 13.1.4.
      
    \item Conversely, if $f \oplus g$ is continuous at $x_0$, then for any
      sequence $\seq{x^{(n)}}{1}$ of elements of $X$, the sequence
      $\seq{f \oplus g(x^{(n)})}{1}$ converges to $(f(x_0), g(x_0))$, by
      Theorem 13.1.4. Thus, by Theorem 12.1.18, $\seq{f(x^{(n)})}{1}$
      converges to $f(x_0)$ in $\rr$, and $\seq{g(x^{(n)})}{1}$
      converges to $g(x_0)$ in $\rr$. Thus, $f$ and $g$ are both
      continuous at $x_0$.
  \end{itemize}
\end{exo}

\begin{exo}{13.2.2}{Prove Lemma 13.2.2.}

  First we prove that the addition function, $f : (x,y) \mapsto x+y$,
  defined from $\rr^2$ to $\rr$, is continuous. Consider a sequence
  $\seq{x_n, y_n}{1}$ of elements of $\rr^2$, that converges to
  $(x_0, y_0)$ with respect to the metric $d_{l^2}$. In particular,
  $\seq{x_n}{1}$ and $\seq{y_n}{1}$ are both sequences of real
  numbers, and we know from Proposition 12.1.8 that $\seq{x_n}{1}$
  converges to $x_0$, and $\seq{y_n}{1}$ converges to $y_0$. But we
  have $f(x_n, y_n) = x_n + y_n$, and we know by the limit laws
  (Proposition 6.1.19(a)) that
  $\lim_{n \to \infty} (x_n + y_n) = x_0 + y_0 =: f(x_0, y_0)$. Thus, for any
  sequence $\seq{(x_n, y_n)}{1}$ of elements of $\rr^2$ which
  converges to $(x_0, y_0)$, we have proved that
  $\seq{f(x_n, y_n)}{1}$ converges to $f(x_0, y_0)$. Thus, the
  addition function $f$ is continuous at any $(x_0, y_0) \in \rr^2$, and
  thus is continuous on $\rr^2$.

  A similar proof applies for the substraction function, the
  multiplication function, and the maximum and minimum functions.

  An additional precaution is required for the division function,
  $f : (x,y) \mapsto x/y$ defined from $\rr \times \rr \setminus\{0\}$ to
  $\rr$. Let be $(x_0, y_0) \in \rr \times \rr \setminus\{0\}$, and let be
  $\seq{x_n, y_n}{1}$ a sequence of elements in
  $\rr \times \rr \setminus\{0\}$ which converges to $(x_0, y_0)$. In the proof
  above, we can indeed apply Proposition 6.1.19(f) since $y_n \neq 0$ for
  all $n \geq 1$.  
\end{exo}

\bigskip
\begin{exo}{13.2.3}{Show that if $f : X \to \rr$ is a continuous
    function, so is the function $|f| : X \to \rr$ deﬁned by
    $|f|(x) := |f(x)|$.}

  The function $|f|$ is the composition of the functions $f$ and
  $x \mapsto |x|$. We already know (from Proposition 9.4.12) that the
  function $x \mapsto |x|$ is continuous on $\rr$ (basically because
  $|x| := \max(x, -x)$, and thus is the composition of two continuous
  functions). Thus, by Corollary 13.1.7, $|f|$ is continuous on $X$.  
\end{exo}

\bigskip
\begin{exo}{13.2.4}{Let $\pi_1 : \rr^2 \to \rr$ and
    $\pi_2 : \rr^2 \to \rr$ be the functions $\pi_1(x, y) := x$ and
    $\pi_2(x, y) := y$. Show that $\pi_1$ and $\pi_2$ are continuous.
    Conclude that if $f : \rr \to X$ is any continuous function into a
    metric space $(X, d)$, then the functions $g_1 : \rr^2 \to X$ and
    $g_2 : \rr^2 \to X$ deﬁned by $g_1(x, y) := f(x)$ and
    $g_2(x, y) := f(y)$ are also continuous.}

  Let be a sequence of elements of $\rr^2$, that we will note as
  $\seq{(x_n, y_n)}{1}$, and suppose that this sequence converges to
  $(x_0, y_0) \in \rr^2$. By Proposition 12.1.18, we know that this
  sequence converges component-wise, i.e. that the sequence of real
  numbers $\seq{x_n}{1}$ converges to $x_0$, and the sequence of real
  numbers $\seq{y_n}{1}$ converges to $y_0$. But we have
  $x_n = \pi_1(x_n, y_n)$, $x_0 = \pi_1(x_0, y_0)$,
  $y_n = \pi_2(x_n, y_n)$, and $y_0 = \pi_2(x_, y_0)$. To summarise, for
  any sequence $\seq{(x_n, y_n)}{1}$ of elements of $\rr^2$ that
  converges to $(x_0, y_0)$, the sequence $\pi_1(x_n, y_n)$ converges to
  $\pi_1(x_0, y_0)$, so that $\pi_1$ is continuous. A similar argument
  shows that $\pi_2$ is continuous.

  Furthermore, the function $g_1$ defined above can be expressed as
  $g_1 := f \circ \pi_1$, and is thus continuous as the composition of two
  continuous functions (Corollary 13.1.7). Similarly,
  $g_2 = f \circ \pi_2$ is continuous.
\end{exo}

\bigskip
\begin{exo}{13.2.5}{Let $n, m \geq 0$ be integers. Suppose that for
    every $0 \leq i \leq n$ and $0 \leq j \leq m$ we have a real number
    $c_{ij}$. Form the function $P : \rr^2 \to \rr$ defined by
    \[ P(x,y) := \sum_{i=0}^n \sum_{j=0}^m c_{ij} x^i y^j.\] Show that
    $P$ is continuous. Conclude that if $f : X \to \rr$ and
    $g : X \to \rr$ are continuous functions, then the function
    $P(f, g) : X \to \rr$ defined by $P(f, g)(x) := P (f(x), g(x))$ is
    also continuous.}

  By Exercise 13.2.4, we know that if $f : \rr \to \rr$ is continuous,
  then the function $f \circ \pi_1 : \rr^2 \to \rr$ is also continuous. It is
  clear that the function $f : x \mapsto x^i$ is continuous for all integer
  $i \geq 0$ (as a product of $i$ continuous functions: apply Corollary
  13.2.3 $i$ times to the identity function $x \mapsto x$). Thus, the
  function $f_1 = f \circ \pi_1$ defined by $f_1(x,y) = x^i$ is continuous
  on $\rr^2$. Similarly, the function $f_2(x,y) = y^j$ is continuous
  on $\rr^2$ for all $j \geq 0$.

  By Corollary 13.2.3 another time, the function
  $(x,y) \mapsto x^i y^j$ is thus continuous (since it is the product of two
  continuous functions), and still by Corollary 13.2.3, the function
  $(x, y) \mapsto c_{ij} x^i y^j$ is also continuous, for all real constant
  $c_{ij}$. And thus, $P(x,y)$ is also continuous as a sum of
  continuous functions.

  Now consider the function $H := P(f,g)$, such that
  $H(x) := P(f(x), g(x))$. We have $H = P \circ (f \oplus g)$. But we have just
  showed that $P$ is continuous, and we already know that $f \oplus g$ is
  continuous whenever $f$ and $g$ are continuous (Lemma 2.2.1). Thus,
  $H$ continuous (as the composition of two continuous functions), as
  expected.
\end{exo}

\bigskip
\begin{exo}{13.2.6}{Let $\rr^m$ and $\rr^n$ be Euclidean spaces. If
    $f : X \to \rr^m$ and $g : X \to \rr^n$ are continuous functions, show
    that $f \oplus g : X \to \rr^{m+n}$ is also continuous, where we have
    identified $\rr^m \times \rr^n$ with $\rr^{m+n}$ in the obvious manner.
    Is the converse statement true?}

  This exercise generalizes the result of Lemma 13.2.1. The proof will
  thus be very close to the approach adopted in Exercise 13.2.1.

  \begin{itemize}
  \item Let be an arbitrary $x_0 \in X$, and let's show that
    $f \oplus g$ is continuous at $x_0$.

    Let be $\seq{x^{(n)}}{1}$ a sequence of elements of $X$ that
    converges to $x_0$. We will use below the following notations:
    \begin{align*}
      f(x_0) &:= (x_1, \ldots, x_m) \\
      g(x_0) &:= (y_1, \ldots, y_m) \\
      f(x^{(k)}) &:= (x^{(k)}_1, \ldots, x^{(k)}_m) \text{ for all } k \geq 1 \\
      g(x^{(k)}) &:= (y^{(k)}_1, \ldots, y^{(k)}_n) \text{ for all } k \geq 1  
    \end{align*}

    First, note that by Proposition 12.1.18, the sequence
    $\seq{x^{(n)}}{1}$ converges component-wise, i.e., we have
    $\lim_{k \to \infty} x^{(k)}_p = x_p$ for all $1 \leq p \leq m$.

    Since $f$ is continuous, the sequence $\seq{f(x^{(n)})}{1}$
    converges to $f(x_0) \in \rr^m$. Similarly for $g$, the sequence
    $\seq{g(x^{(n)})}{1}$ converges to $g(x_0) \in \rr^n$.

    By definition, we have
    $f \oplus g (x_0) = (x_1, \ldots, x_m, y_1, \ldots, y_n) \in \rr^{m+n}$.

    But we also have, for all $k \geq 1$,
    \begin{align*}
      f \oplus g (x^{(k)})
      &:= \left(f(x^{(k)}), g(x^{(k)})\right) \\
      &= \left(x^{(k)}_1, \ldots, x^{(k)}_m, y^{(k)}_1, \ldots, y^{(k)}_n\right) 
    \end{align*}

    And since we already know that this sequence converges
    component-wise, we have by Proposition 12.1.18
    \[\lim_{k \to \infty} f \oplus g (x^{(k)}) = (x_1, \ldots, x_m, y_1, \ldots, y_m) =: f \oplus
      g (x_0).\]

    Thus, $f \oplus g$ is continuous at $x_0$. And since this is true for
    any arbitrary $x_0 \in X$, $f \oplus g$ is continuous on $X$.
    
  \item The converse statement is also true; this can also be proved
    in a very similar fashion as Exercise 13.2.1.
  \end{itemize}
\end{exo}

\begin{exo}{13.2.7}{Let $k \geq 1$, let $I$ be a ﬁnite subset of $\nn^k$,
    and let $c : I \to \rr$ be a function. Form the function $P : \rr^k
    \to \rr$ defined by
    \[P (x_1 , \ldots , x_k) := \sum_{(i_1, \ldots, i_k) \in I} c(i_1 , \ldots , i_k)
      x_1^{i_1} \ldots x_k^{i_k}.\] Show that $P$ is continuous.}

  Let's use induction on $k$.

  \begin{itemize}
  \item First, if $k=1$, we have $I \subset \nn$, and $P$ is simply of the
    form $P(x) := \sum_{i \in I} c_i x^i$. Such a function is clearly
    continuous by Lemma 13.2.2, as the product and sum of continuous
    functions.
  \item We can also note that, for $k=2$, the result corresponds
    exactly to Exercise 13.2.5.
  \item Now suppose that the property is true for a given positive
    integer $k$, and let's show that it is still true for $k+1$.

    Let be $I$ a finite subset of $\nn^{k+1}$, and a function $P(x_1,
    \ldots, x_k, x_{k+1})$ as defined above. Note that (for example by
    Corollary 3.6.14(e)), since $I$ is supposed to be finite, we have
    $I = I_1 \times \ldots \times I_k \times I_{k+1}$ where each $I_j$ is also a finite
    subset of $\nn$. In particular, $I_{k+1}$ is finite.

    We thus have:
    \begin{align*}
      P(x_1, \ldots, x_k, x_{k+1})
      &:= \sum_{(i_1, \ldots, i_k, i_{k+1}) \in I} c(i_1, \ldots, i_k, i_{k+1}) \,
        x_1^{i_1} \ldots x_k^{i_k} x_{k+1}^{i_{k+1}} \\
      &= \sum_{i_{k+1} \in I_{k+1}} \left(\sum_{(i_1, \ldots, i_k) \in I_1 \times \ldots \times I_k}  c(i_1, \ldots, i_k,
        i_{k+1}) x_1^{i_1} \ldots x_k^{i_k}\right) x_{k+1}^{i_{k+1}}
    \end{align*}
    By the induction hypothesis, the expression enclosed in the
    parentheses is a continuous function. Thus,
    $P(x_1, \ldots, x_{k+1})$ is also continuous, as a (finite) sum and
    product of continuous functions.
  \end{itemize}
\end{exo}

\begin{exo}{13.2.8}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces.
    Define the metric
    $d_{X \times Y} : (X \times Y ) \times (X \times Y) \to [0, \infty[$ by the formula
    \[d_{X \times Y} \left((x, y), (x', y')\right) := d_X (x, x') + d_Y (y,
      y').\] Show that $(X \times Y, d_{X \times Y})$ is a metric space, and
    deduce an analogue of Proposition 12.1.18 and Lemma 13.2.1.}

  \begin{enumerate}
  \item First we prove that $d_{X \times Y}$ is indeed a metric. In all the
    proofs below, we simply use the fact that $d_X$ and $d_Y$ are
    themselves metrics.
    \begin{itemize}
    \item For all $(x, y) \in X \times Y$, we have:
      \[
        d_{X \times Y} ((x,y), (x,y))
        = \underbrace{d_X(x,x)}_{=0} + \underbrace{d_Y(y,y)}_{=0}
        =0.
      \]
    \item For all distinct points $(x,y), (x',y') \in X \times Y$, we have
      \[d_{X \times Y} \left((x,y), (x', y')\right)
        = \underbrace{d_X(x, x')}_{>0} + \underbrace{d_Y(y, y')}_{>0}
        > 0.
      \]
    \item Symmetry: for all $(x,y), (x',y') \in X \times Y$, we have
      \begin{align*}
        d_{X \times Y} \left((x,y), (x', y')\right)
        &= d_X(x,x') + d_Y(y,y')\\
        &= d_X(x',x) + d_Y(y',y)\\
        &= d_{X \times Y}((x', y'), (x, y)).
      \end{align*}
    \item Triangle inequality: for all $(x,y), (x',y'), (x'', y'') \in X
      \times Y$, we have
      \begin{align*}
        &d_{X \times Y} \left((x,y), (x', y')\right) + d_{X \times Y}
        \left((x',y'), (x'', y'')\right)\\
        = \; &d_X(x, x') + d_X(x', x'') + d_Y(y, y') + d_Y(y', y'') \\
        \leq \; &d_X(x, x'') + d_Y(y, y'') \\
        \leq \; &d_{X \times Y}\left((x, x''), (y, y')\right).
      \end{align*}
    \end{itemize}
    Thus, $d_{X \times Y}$ is indeed a metric on $X \times Y$.
    
  \item Now we give an analogue of Proposition 12.1.18. Note that $X$
    and $Y$ are ``abstract'' metric spaces here, and $d_X, d_Y$ also
    are ``abstract'' distances, so that we cannot really give an
    analogue to the notion of the equivalence of metrics
    $d_{l^1}, d_{l^2}, d_{l^\infty}$. However, we can give an analogue of
    the notion of ``component-wise'' convergence. Indeed, let be
    $\seq{a^{(n)}}{1}$ a sequence of elements of $X \times Y$ (so that we
    can write $a^{(n)} := (x^{(n)}, y^{(n)})$ for all $n \geq 1$ with
    obvious notations), and let be $(x_0, y_0)$ a point in
    $X \times Y$. The following two statements are equivalent:
    \begin{enumerate}[label=(\roman*)]
    \item $\seq{a^{(n)}}{1} := \seq{x^{(n)}, y^{(n)}}{1}$ converges to
      $(x_0, y_0)$ with respect to the metric $d_{X \times Y}$
    \item $\seq{x^{(n)}}{1}$ converges to $x_0$ with respect to $d_X$
      and $\seq{y^{(n)}}{1}$ converges to $y_0$ with respect to $d_Y$.
    \end{enumerate}

    \begin{proof}
      First we show that (i) $\implies$ (ii). If
      $\seq{x^{(n)},y^{(n)}}{1}$ converges to $(x_0, y_0)$ with
      respect to $d_{X \times Y}$, then by definition we have
      $\lim_{n \to \infty} d_{X \times Y}((x^{(n)}, y^{(n)}), (x_0, y_0)) = 0$.
      Thus, for any arbitrary $\epsilon > 0$, there exists $N \geq 1$ such that
      for all $n \geq N$ we have
      \begin{align*}
        d_{X \times Y}((x^{(n)}, y^{(n)}), (x_0, y_0)) &< \epsilon \\
        d_X(x^{(n)}, x_0) + d_Y(y^{(n)}, y_0) &< \epsilon.
      \end{align*}
      In particular, we have both $d_X(x^{(n)}, x_0) < \epsilon$ and
      $d_Y(y^{(n)}, y_0) < \epsilon$ whenever $n \geq N$. Thus, we have
      $\lim_{n \to \infty} d_X(x^{(n)}, x_0) = 0$ and
      $\lim_{n \to \infty} d_Y(y^{(n)}, y_0) = 0$, which is precisely the
      statement (ii).

      Now we show that (ii) $\implies$ (i). Let be $\epsilon > 0$. Since we
      have both $\lim_{n \to \infty} d_X(x^{(n)}, x_0) = 0$ and
      $\lim_{n \to \infty} d_Y(y^{(n)}, y_0) = 0$, there exists $N_1, N_2 \geq
      1$ such that $d_X(x^{(n)}, x_0) < \epsilon/2$ whenever $n \geq N_1$, and
      $d_Y(y^{(n)}, y_0) < \epsilon/2$ whenever $n \ge N_2$. Thus, for all $n \geq
      \max(N_1, N_2)$, we have
      \begin{align*}
        d_{X \times Y}((x^{(n)}, y^{(n)}), (x_0, y_0))
        &= d_X(x^{(n)}, x_0) + d_Y(y^{(n)}, y_0) \\
        &< \epsilon/2 + \epsilon/2\\
        &< \epsilon
      \end{align*}
      which means that  $\lim_{n \to \infty} d_{X \times Y}((x^{(n)}, y^{(n)}),
      (x_0, y_0)) = 0$. This is the statement (i).     
    \end{proof}
    
  \item Similarly, we can give an analogue for Lemma 13.2.1:
    
    Let be $S$ an arbitrary domain for the functions $f : S \to X$ and
    $g : S \to Y$. Consider the metric spaces $(X, d_X)$, $(Y, d_Y)$ and
    $(X \times Y, d_{X \times Y})$. Then the function
    $f \oplus g : S \to X \times Y$ is continuous at $s_0 \in S$ iff both
    $f$ and $g$ are continuous at $s_0$.

    \begin{proof}
      Suppose that both $f$ and $g$ are continuous at $s_0 \in S$. Let be
      $\epsilon > 0$, and let be $\seq{s^{(n)}}{1}$ a sequence of elements of $S$
      that converges to $s_0$. By definition, there exists $N_1 \geq 1$
      such that $d_X(f(s^{(n)}), f(s_0)) < \epsilon/2$ whenever $n \geq N_1$;
      and there exists $N_2 \geq 1$ such that $d_Y(g(s^{(n)}), g(s_0)) <
      \epsilon/2$ whenever $n \geq N_2$. Thus, for $n \geq N := \max(N_1, N_2)$,
      \[
        d_{X \times Y}(f \oplus g(s^{(n)}), f \oplus g(s_0))
        := \underbrace{d_X(f(s^{(n)}), f(s_0))}_{< \epsilon/2}
          + \underbrace{d_Y(g(s^{(n)}), g(s_0))}_{< \epsilon/2}
        < \epsilon
      \]
      which means that $f \oplus g$ is continuous at $s_0$, as expected.

      The converse statement can be proved similarly (see also the
      proof of 2. above).
    \end{proof}
  \end{enumerate}
\end{exo}

\begin{exo}{13.2.10}{Let $f : \rr^2 \to \rr$ be a continuous function.
    Show that for each $x \in \rr$, the function $y \mapsto f(x, y)$ is
    continuous on $\rr$, and for each $y \in \rr$, the function
    $x \mapsto f (x, y)$ is continuous on $\rr$. Thus a function
    $f(x, y)$ which is jointly continuous in $(x, y)$ is also
    continuous in each variable $x, y$ separately.}

  Let be $x_0 \in \rr$ a real number. We will just prove the result for
  the function $g : y \mapsto f(x_0, y)$, since the result for the other
  function can be shown in the same way. We will work below with the
  metric space $(\rr^2, d_{l^2})$ but the proof can easily be adapted
  for the metrics $d_{l^1}$ or $d_{l^\infty}$.

  Let be $y_0 \in \rr$ an arbitrary real number: we have to prove that
  $g$ is continuous at $y_0$. Let be $\epsilon > 0$ an arbitrary positive
  real number. Since $f$ is continuous on $\rr^2$, it is continuous in
  particular at $(x_0, y_0) \in \rr^2$. Thus, there exists a
  $\delta > 0$ such that $|f(x, y) - f(x_0, y_0)| < \epsilon$ whenever
  $d((x,y), (x_0, y_0)) < \delta$.

  If we set $x := x_0$ and choose any $y \in \rr$ such that
  $|y-y_0| < \delta$, we will have
  $d((x_0, y), (x_0, y_0)) = |y-y_0| < \delta$. Thus, we will have
  $\epsilon > |f(x_0, y) - f(x_0, y_0)| = |g(y) - g(y_0)|$. This means that
  $g$ is continuous at $y_0$, as expected.
\end{exo}

\bigskip
\begin{exo}{13.2.11}{Let $f : \rr^2 \to \rr$ be the function deﬁned by
    $f(x, y) := \frac{xy}{x^2 + y^2}$ when $(x, y) \neq (0, 0)$, and
    $f(x, y) = 0$ otherwise. Show that for each fixed $x \in \rr$, the
    function $y \mapsto f(x, y)$ is continuous on $\rr$, and that for each
    fixed $y \in \rr$, the function $x \mapsto f(x, y)$ is continuous on
    $\rr$, but that the function $f : \rr \mapsto \rr$ is not continuous on
    $\rr$. This shows that the converse to Exercise 13.2.10 fails; it
    is possible to be continuous in each variable separately without
    being jointly continuous.}

  First, let be $x_0 \in \rr$ an arbitrary real number, and let's
  consider the function $g : \rr \to \rr$ such that $g(y) = f(x_0 ,y)$
  for all $y \in \rr$. We have two cases:
  \begin{itemize}
  \item If $x_0 = 0$, then $g$ is the constant null function, i.e.
    $g(y) = 0$ for all $y \in \rr$. Thus (for example by Exercise
    9.4.2), $g$ is continuous on $\rr$.
  \item If $x_0 \neq 0$, then $x_0^2 + y^2 \neq 0$ for all
    $y \in \rr$. Thus, $g(y) = \frac{xy}{x^2+y^2}$ is the ratio between
    the continuous function $y \mapsto xy$ and the continuous, positive
    function $y \mapsto x_0^2 + y^2$ (these functions can be shown to be
    continuous by applying Lemma 13.2.2 several times). Thus, by
    Corollary 13.2.3, $g$ is continuous on $\rr$.
  \end{itemize}

  Of course, the same demonstration shows that the function $h : x \mapsto
  f(x,y)$ is also continuous on $\rr$.

  Now let's show that $f$ is not jointly continuous on $\rr$, and more
  precisely that it is not continuous at the point $(0,0)$. By Theorem
  13.1.5, to show that a function $f$ is not continuous, it is enough to
  find a sequence $\seq{x^{(n)}}{1}$ that converges to $x_0$, but such
  that $\seq{f(x^{(n)})}{1}$ does not converge to $f(x_0)$. Consider
  the sequence $(1/n, 1/n)_{n=1}^\infty$ of elements of $\rr^2$. It is
  clear that this sequence converges to $(0,0)$. However, we have
  $f(1/n, 1/n) = \frac{1/n^2}{2/n^2} = \frac{1}{2}$. Thus,
  $\seq{f(1/n, 1/n)}{1}$ does not converge to $f(0,0) = 0$, which
  means that $f$ is not continuous at $(0,0)$.  
\end{exo}

\bigskip
\begin{exo}{13.2.12}{Let $f : \rr^2 \to \rr$ be the function defined by
    $f(x,y) := x^2/y$ when $y \neq 0$, and $f(x,y) := 0$ when $y = 0$.
    Show that $\lim_{t \to 0} f(tx, ty) = f(0,0)$ for every
    $(x,y) \in \rr^2$, but that $f$ is not continuous at the origin.
    Thus being continuous on every line through the origin is not
    enough to guarantee continuity at the origin!}

  By definition, we first note that we have $f(0,0) = 0$. Furthermore,
  for all $t \in \rr$, we have
  $f(tx, ty) = \frac{t^2x^2}{ty} = \frac{tx^2}{y}$. Thus, considering
  the function $t \mapsto \frac{tx^2}{y}$ with $x,y$ fixed real numbers, we
  have immediately
  $\lim_{t \to 0} f(tx, ty) = \lim_{t \to 0} \frac{tx^2}{y} = 0 = f(0,0)$,
  as expected.

  However, $f$ is not continuous at $(0,0)$. Indeed, consider the
  sequence $\seq{1/n, 1/n^2}{1}$ of elements of $\rr^2$. This sequence
  converges (component-wise, and thus globally) to $(0,0)$ when $n \to
  \infty$. However, we have $f(1/n, 1/n^2) = \frac{1/n^2}{1/n^2} = 1$ for
  all $n \in \nn$, so that it is clear that $\seq{f(1/n, 1/n^2)}{1}$
  does not converge to $f(0,0) = 0$. Thus, $f$ is not continuous at
  the origin.
\end{exo}

\bigskip
\begin{exo}{13.3.1}{Prove Theorem 13.3.1.}

  We have to prove that continuous maps preserve compactness. Thus,
  let's consider a function $f : X \to Y$ from one metric space $(X,
  d_X)$ to another $(Y, d_Y)$, and let be $K \subseteq X$ a compact subset of
  $X$. We have to prove that the image $f(K)$ is also compact.

  To prove that $f(K)$ is compact, let's use Definition 12.5.1.
  Consider a sequence $\seq{y^{(n)}}{1}$ of elements of $f(K)$, and
  let's show that there exists a convergent subsequence.

  By definition, for all positive integer $n \geq 1$, there exists an
  element $x^{(n)} \in K$ such that $y^{(n)} = f(x^{(n)})$. For example
  by using the countable axiom of choice, we thus get a sequence
  $\seq{x^{(n)}}{1}$ of elements of $K$. Since $K$ is compact by
  hypothesis, there exists a subsequence $(x^{(n_k)})_{k=1}^\infty$ that
  converges in $K$ to some $x_0 \in K$ (relative to $d_X$). Since $f$ is
  continuous on $X$, it is in particular continuous at $x_0$; and
  using Theorem 13.1.4 (i.e., sequential characterization of
  continuity), we immediately can say that the subsequence
  $(y^{(n_k)})_{k=1}^\infty = (f(x^{(n_k)}))_{k=1}^\infty$ converges to
  $f(x_0)$ in $f(K)$.

  This shows that $f(K)$ is compact, as expected.
\end{exo}

\bigskip
\begin{exo}{13.3.2}{Prove Proposition 13.3.2.}

  We have to prove the maximum principle in the general setting of
  metric spaces. Let be $f : X \to \rr$ a continuous function on a
  compact metric space $(X, d)$.

  \begin{itemize}
  \item First we prove that $f$ is bounded. We can simply modify the
    proof of Lemma 9.6.3. Suppose, for the sake of contradiction, that
    $f$ is not bounded. Thus, for all $n \geq 1$, the set
    $X_n := \{x \in X : |f(x)| > n\}$ is non-empty, and contains at
    least one element $x^{(n)}$. Thus, using the countable axiom of
    choice, we can form a sequence $\seq{x^{(n)}}{1}$ of elements of
    $X$ such that $x^{(n)} \in X_n$ for all $n \geq 1$. But since $X$ is
    compact, the sequence $\seq{x^{(n)}}{1}$ has a convergent
    subsequence, say $(x^{(n_k)})_{k=1}^\infty$, which converges to
    $L$ in $X$. Since $f$ is continuous, the sequence
    $(f(x^{(n_k)}))_{k=1}^\infty$ converges to $f(L)$. Since this
    sequence is convergent, it is necessarily bounded. But this is
    a contradiction with the fact that we have $|f(x^{(n_k)})| > n_k \geq
    k$ for all $k \geq 1$. Thus, $f$ must be bounded.
  \item Now let's prove that $f$ attains its maximum on $X$, i.e. that
    there exists a point $x_{max} \in X$ such that $f(x) \leq f(x_{max})$ for
    all $x \in X$. We have shown that $f$ is bounded on $X$, thus there
    exists an $M \in \rr$ such that $M := \sup_{x \in X} f(x)$. By
    definition of a supremum, for all $n \geq 1$, the set
    $G_n := \{x \in X : M - 1/n < f(x) \leq M\}$ is non-empty and contains
    at least one element $x^{(n)}$. Thus, using the countable axiom of
    choice, we can form a sequence $\seq{x^{(n)}}{1}$ such that
    $x^{(n)} \in G_n$ for all $n \geq 1$. Because $X$ is compact, there
    exists a subsequence $(x^{(n_k)})_{k=1}^\infty$ that converges to some point
    $x_{max} \in X$. Since $f$ is continuous, the sequence
    $(f(x^{(n_k)}))_{k=1}^\infty$ also converges (to $f(x_{max})$).

    By construction, we have $f(x) \leq M$ for all $x \in X$, and thus we
    have in particular $f(x_{max}) \leq M$.

    On the other hand, we have $M - 1/k \leq M - 1/n_k < f(x^{(n_k)})$
    for all $k \geq 1$, so that taking the limit as $k \to \infty$ on both sides
    leads to $f(x_{max}) \geq M$.

    Thus, there indeed exists a $x_{max} \in X$ such that $f(x_{max}) =
    M = \sup_{x \in X} f(x)$, as required.
    
  \item A similar proof would show that $f$ also attains its minimum
    on $X$.
  \end{itemize}  
\end{exo}

\begin{exo}{13.3.3}{Show that every uniformly continuous function is
    continuous, but give an example that shows that not every
    continuous function is uniformly continuous.}

  Below, we consider two metric spaces $(X, d_X)$ and $(Y, d_Y)$.

  \begin{itemize}
  \item Let be $f : X \to Y$ a function that is uniformly continuous on
    $X$. Let be $\epsilon > 0$. By definition, there exists a $\delta > 0$ such
    that for all $x,x' \in X$, we have $d_X(x,x') < \delta \implies d_Y(f(x),
    f(x')) < \epsilon$.

    Consider an arbitrary $x' \in X$ and let's show that $f$ is
    continuous at $X$. For this same $\epsilon > 0$, we already have a $\delta >
    0$ such that $d_Y(f(x), f(x')) < \epsilon$ whenever $d_X(x,x') < \delta$.
    Thus, $f$ is continuous at $x'$.
  \item Now we give an example of function $f : X \to Y$ that is
    continuous but not uniformly continuous. A simple example would be
    to consider $X = [0, +\infty[$ and $Y = [0, +\infty[$, with
    $f(x) = x^2$. Consider $\epsilon := 1$. We have to show that, for all
    $\delta > 0$, there exist $x,y \in \rr$ such that
    $|x^2 - y^2| \geq 1$ although $|x-y| < \delta$. If we consider an
    arbitrary $\delta > 0$ and set $y := x + \delta/2$, we clearly have
    $|x-y| < \delta$. However, the inequality
    $|x^2 - (x+\delta/2)^2| \geq 1$ always has a solution, in
    $[0, \infty[$; more precisely, any $x \geq 1/\delta - \delta/4$ will do the trick.
    Thus, $f$ is not uniformly continuous.
  \end{itemize}
\end{exo}

\begin{exo}{13.3.4}{Let $(X, d_X), (Y, d_Y), (Z, d_Z)$ be metric
    spaces, and let $f : X \to Y$ and $g : Y \to Z$ be two uniformly
    continuous functions. Show that $g \circ f : X \to Z$ is also uniformly
    continuous.}

  Let be $\epsilon > 0$ an arbitrary real number.
  \begin{itemize}
  \item Since $g$ is uniformly continuous, there exists a $\delta > 0$ such
    that, for all $y,y' \in Y$, we have $d_Y(y, y') < \delta \implies
    d_Z(g(y), g(y')) < \epsilon$.
  \item For this $\delta > 0$, since $f$ is uniformly continuous, there
    exists a $\delta' > 0$ such that, for all $x,x' \in X$, we have
    $d_X(x,x') < \delta' \implies d_Y(f(x), f(x')) < \delta$.
  \item Combining the two previous results, there exists a $\delta' > 0$
    such that, for all $x,x' \in X$,
    \[d_X(x,x') < \delta' \implies d_Y(f(x), f(x')) < \delta \implies d_Z(g \circ f
      (x), g \circ f (x')) < \epsilon.\]
    This shows that $g \circ f$ is uniformly continuous, as expected.
  \end{itemize}
\end{exo}

\begin{exo}{13.3.5}{Let $(X, d_X)$ be a metric space, and let
    $f : X \to \rr$ and $g : X \to \rr$ be uniformly continuous functions.
    Show that the direct sum $f \oplus g : X \to \rr^2$ defined by
    $f \oplus g(x) := (f(x), g(x))$ is uniformly continuous.}

  First note that the exercise does not specify which metric
  ($d_{l^1}$, $d_{l^2}$ or $d_{l^\infty}$) should be considered on $\rr^2$.
  However, we will see that one choice will come naturally, and that
  the two others cases can be deduced immediately from that choice.

  Let be $\epsilon > 0$. Since $f$ is uniformly continuous, there exists a
  $\delta_1 > 0$ such that, for all $x,x' \in X$, we have $d_X(x,x') < \delta_1
  \implies |f(x) - f(x')| < \epsilon/2$.

  Similarly, since $g$ is uniformly continuous, there exists a
  $\delta_2 > 0$ such that, for all $x,x' \in X$, we have $d_X(x,x') < \delta_2
  \implies |g(x) - g(x')| < \epsilon/2$.

  Thus, if we set $\delta := \min(\delta_1, \delta_2)$, then for all $x,x' \in X$ we
  have
  \[d_X(x,x') < \delta \implies |f(x) - f(x')| + |g(x) - g(x')| < \epsilon/2 +
    \epsilon/2\]
  i.e.,
  \[d_{l^1}\left((f(x), g(x)), (f(x'), g(x'))\right) < \epsilon\]
  or finally
  \begin{equation}
    \label{eq:13.3.5}
    d_{l^1}\left(f \oplus g(x), f \oplus g (x')\right) < \epsilon.
  \end{equation}
  This means that $f \oplus g$ is indeed uniformly continuous if we
  consider the metric space $(\rr^2, d_{l^1})$. However, remember from
  Exercise 12.1.8 and 12.1.10 that we have $d_{l^\infty}(x,y) \leq
  d_{l^2}(x,y) \leq d_{l^1}(x,y)$ for all $x,y \in \rr^2$. Thus, the
  inequality \eqref{eq:13.3.5} remains true if we consider the metric
  spaces $(\rr^2, d_{l^2})$ or $(\rr^2, d_{l^\infty})$ instead. This shows
  that $f \oplus g$ is indeed uniformly continuous in all cases.
\end{exo}

\bigskip
\begin{exo}{13.3.6}{Show that the addition function
    $(x, y) \mapsto x + y$ and the subtraction function
    $(x, y) \mapsto x - y$ are uniformly continuous from $\rr^2$ to
    $\rr$, but the multiplication function $(x, y) \mapsto xy$ is not.
    Conclude that if $f : X \to \rr$ and $g : X \to \rr$ are uniformly
    continuous functions on a metric space $(X, d)$, then
    $f + g : X \to \rr$ and $f - g : X \to \rr$ are also uniformly
    continuous. Give an example to show that $fg : X \to \rr$ need not
    be uniformly continuous. What is the situation for $\max(f, g)$,
    $\min(f, g)$, $f/g$, and $cf$ for a real number $c$?}

  Because it is by far the most common case, we will consider below
  that we work on the metric space $(\rr, d_{l^2})$. Similar proofs
  would show the results for other choices of metrics on $\rr^2$.

  \begin{itemize}
  \item First let's show that $f : (x,y) \mapsto x+y$ is uniformly
    continuous. Let be $\epsilon > 0$. We have to show that there exists a
    $\delta > 0$ such that, for all $(x,y), (x',y') \in \rr^2$, we have
    $d_{l^2}((x,y), (x',y')) < \delta \implies |f(x,y) - f(x',y')| < \epsilon$,
    i.e.
    \[\sqrt{(x-x')^2 + (y-y')^2} < \delta \implies |(x+y) - (x'+y')| < \epsilon.\]
    First note that we always have
    $|x-x'| \leq \sqrt{(x-x')^2 + (y-y')^2}$, and
    $|y-y'| \leq \sqrt{(y-y')^2 + (y-y')^2}$. Consequently, if we set
    $\delta := \epsilon/3$, and if we suppose that
    $\sqrt{(x-x')^2 + (y-y')^2} < \delta$, we will have
    $|x-x'| \leq \epsilon/3$ and $|y-y'| \leq \epsilon/3$. Thus, we will also have, by
    triangle inequality,
    $|(x+y) - (x'+y')| \leq |x+y| + |x'+y'| \leq \epsilon/3 + \epsilon/3 < \epsilon$, as
    expected. Thus, the addition function is indeed uniformly
    continuous.
  \item Only a very small modification in the previous proof is needed
    to handle the case of the substraction function, so that we will
    not give the detailed proof here.
  \item However, the multiplication function $f : (x,y) \mapsto xy$ is not
    uniformly continuous. Indeed, let's take $\epsilon := 1$. We have to show
    that for all $\delta > 0$, we will always be able to find
    $(x,y, (x',y') \in \rr^2$ such that $|xy - x'y'| \geq 1$ although
    $d_{l^2}((x,y), (x',y')) < \delta$.

    Consider an arbitrary $\delta > 0$, and an arbitrary $(x,y) \in \rr^2$. If we
    set $(x', y') := (x+\delta/2, y)$, we clearly have $d_{l^2}((x,y),
    (x',y')) < \delta$. On the other hand, we have
    \begin{align*}
      |xy - x'y'| &= |xy - (x+\delta/2)y| \\
                  &= |xy - xy - y\delta/2| \\
                  &= \frac{\delta}{2} |y|
    \end{align*}
    Thus, we just have to choose $y \geq \frac{2}{\delta}$ to get
    $|xy - x'y'| \geq 1$, although $d_{l^2}((x,y), (x',y')) < \delta$. This
    shows that $f$ is not uniformly continuous.
  \item Now consider $f,g : X \to \rr$ two uniformly continuous
    functions. We can note that $f + g : X \to \rr$ is simply the
    composition of the function $f \oplus g$ with the addition function.
    But we have just shown that the addition function is uniformly
    continuous, and we have shown in Exercise 13.3.5 that $f \oplus g$ is
    also uniformly continuous. Thus, $f+g$ is uniformly continuous, by
    Exercise 13.3.4. The same argument applies to $f-g$.
  \item On the other hand, if $f,g : X \to \rr$ are two uniformly continuous
    functions, the function $fg$ is not necessarily continuous.
    Indeed, we have a ``free'' example provided by Exercise 13.3.3: if
    we take $f(x) = g(x) = x$, we have $(fg)(x) = x^2$ and we have
    already shown that this function is not uniformly continuous.
  \item If $f,g : X \to \rr$ are two uniformly continuous functions, the
    function $\max(f,g)$ is uniformly continuous, since it is simply
    the composition of $f \oplus g$ and of the function
    $(x,y) \mapsto \max(x,y)$. The first one is continuous by Exercise
    13.3.5, the second one is continuous by Proposition 13.3.2. The
    same argument applies to $\min(f,g)$.
  \item Finally, the function $f/g$ is not necessarily continuous: we
    can simply consider the case of $f(x) = 1$ and $g(x) = x$, which
    are clearly uniformly continuous, whereas $(f/g)(x) = 1/x$ is not
    (see for instance Example 9.9.1).
  \end{itemize}
\end{exo}

\begin{exo}{13.4.1}{Let $(X, \ddisc)$ be a metric space with the
    discrete metric. Let $E$ be a subset of $X$ which contains at
    least two elements. Show that $E$ is disconnected.}

  By definition, there exists at least two elements $x,y \in E$. Thus,
  we have $\{x\} \subseteq E$, and the set $X \setminus \{x\}$ contains at least the
  element $y$, and is thus non empty. In other words, $\{x\}$ is a
  proper subset of $X$, i.e. $\{x\} \subsetneq X$.

  By Remark 12.2.14, we know that for the discrete metric $\ddisc$,
  every set is both open and closed. Thus, the set $\{x\}$ is a proper
  subset of $X$ that is non-empty, open, and closed. This shows that
  $(X, \ddisc)$ is connected (Definition 13.4.1).
\end{exo}

\pagebreak
\begin{exo}{13.4.2}{Let $f : X \to Y$ be a function from a connected
    metric space $(X, d)$ to a metric space $(Y, \ddisc)$ with the
    discrete metric. Show that $f$ is continuous if and only if it is
    constant. (Hint: use Exercise 13.4.1.)}

  (Remark: this exercise is much easier to solve if we assume that we
  already know Theorem 13.4.6 and/or Corollary 13.4.7. We will proceed
  accordingly.)

  \begin{itemize}
  \item First, it is clear that if $f$ is constant, then it is
    continuous (since $\ddisc(f(x_0), f(x)) = 0$, and is thus inferior to
    any $\epsilon > 0$, no matter how close are $x, x_0 \in X$).
  \item Furthermore, if $f$ is continuous, then it is constant.
    Indeed, $X$ is connected, thus $f(X)$ must be connected (by
    Theorem 13.4.6). By Exercise 13.4.1, $f(X)$ can only have one
    element, i.e., $f$ is constant.
  \end{itemize}
\end{exo}

\begin{exo}{13.4.3}{Prove the equivalence of statements (b) and (c) in
    Theorem 13.4.5.}

  First we prove that (b) $\implies$ (c). $X$ is supposed to be a
  non-empty set of the real line, and thus has a supremum
  $b \in \extrr$ and an infimum $a \in \extrr$ (by Definition 5.5.10).
  Consider a real number $z \in ]a,b[$. Because $z < b$, then by
  definition of the supremum of $X$, there exists a real number $y \in
  X$ such that $z < y \leq b$. Similarly, because $a < z$, there must
  exist a $x \in X$ such that $a \leq x < z$. Thus, there exist $x,y \in X$
  such that $a \leq x < z < y \leq b$. By property (b), this implies that $z
  \in X$. Since this applies to any $z \in [a,b]$, we actually have $]a,b[
  \subseteq X$. Thus, $X$ can be either $]a,b[$, $[a,b[$, $]a, b]$ or $[a,b]$,
  but in each case it is an interval in the sense of Definition 9.1.1.

  Now let's prove that (c) $\implies$ (b). Suppose that $X$ is an
  interval in the sense of Definition 9.1.1, for instance that
  $X = [a,b]$. Then it is clear that for all $a \leq x < y \leq b$ we have
  $[x,y] \subseteq X$. This is just as clear for any other type of interval
  listed in Definition 9.1.1.
\end{exo}

\bigskip
\begin{exo}{13.4.4}{Prove Theorem 13.4.6. (Hint: the formulation of
    continuity in Theorem 13.1.5(c) is the most convenient to use.)}

  Given that connectedness is a ``negative'' notion (i.e., not being
  separable into two disjoint open sets), it may seem more natural to
  give a proof by contradiction. Thus, suppose that $f(E)$ is
  \emph{not} connected. In this case, we can split $f(E)$ into two
  disjoint open (non-empty) sets $V$ and $W$, i.e. we have
  $f(E) = V \sqcup W$.

  \begin{itemize}
  \item Since $f$ is continuous on $X$, it is in particular continuous
    on $E$. Thus, by Theorem 13.1.5(c), $f^{-1}(V)$ and $f^{-1}(W)$
    are open in $X$, and the sets $A := E \cap f^{-1}(V)$ and
    $B : = E \cap f^{-1}(W)$ are open in $E$\footnote{Note that we use
      here a relative topology in the metric space
      $(E, d|_{E \times E})$.}.
  \item Of course, since $V$ and $W$ are non-empty, $A$ and $B$ are
    also non-empty.
  \item Furthermore, $A$ and $B$ are disjoint. Indeed, suppose that we
    have $x \in A \cap B$. Thus we would have $f(x) \in V \cap W$, which is a
    contradiction with the fact that $V$ and $W$ are disjoint.
  \item Finally, we have $E = A \sqcup B$. Indeed, the inclusion
    $A \sqcup B \subseteq E$ is clear (since $A, B$ are subsets of $E$ by
    definition). Conversely, the inclusion $E \subseteq A \sqcup B$ can be proved
    easily: if $x \in E$, then $f(x) \in f(E)$, i.e.
    $f(x) \in V \sqcup W$. If $f(x) \in V$, then $x \in A$; and if $x \in W$, then
    $x \in B$. In both cases, $x \in A \sqcup B$, which proves the second
    inclusion. Thus, we have $E = A \sqcup B$.
  \end{itemize}

  Thus, the metric space $(E, d|_{E \times E})$ is not connected, since $E$
  can be split into two open disjoint and non-empty subsets. This is a
  contradiction with our initial hypothesis, as expected; and $f(E)$
  must be connected.

  (Note that for this exercise, we could have re-used some statements
  proved earlier, for example in Exercises 3.4.2 or 3.4.4, to make the
  proof a bit shorter.)
\end{exo}
\bigskip

\begin{exo}{13.4.5}{Use Theorem 13.4.6 to prove Corollary 13.4.7.}

  Let be $f : X \to \rr$ a continuous map, let be $E \subseteq X$ a connected
  set, and let be $a,b \in E$. Suppose that we have $f(a) \leq f(b)$ (the
  proof can easily be adapted if $f(b) > f(a)$), and let be $f(a) \leq y
  \leq f(b)$.

  By hypothesis, $E$ is a connected subset of $X$, so that $f(E)$ must
  also be connected, by Theorem 13.4.6. And since $a,b \in E$, we
  clearly have $f(a), f(b) \in f(E)$, by definition. Recall that we
  suppose here $f(a) \leq f(b)$, and that $f(E) \subseteq \rr$ by definition.
  
  \begin{itemize}
  \item (Trivial case) If $f(a) = f(b)$, then we necessarily have
    $f(a) = y = f(b)$, so that we just have to pick $c := a$ to
    exhibit a number $c \in E$ such that $f(c) = y$.
  \item (General case) If $f(a) < f(b)$, then by Theorem 13.4.5, since
    $f(E)$ is a connected subset of $\rr$, the interval $[f(a), f(b)]$
    is contained in $f(E)$. Thus, for any $y \in [f(a), f(b)]$, we have
    $y \in f(E)$, and there exists a $c \in E$ such that $f(c) = y$, as
    expected.
  \end{itemize}
\end{exo}

\begin{exo}{13.4.6}{Let $(X,d)$ be a metric space, and let
    $(E_\alpha)_{\alpha \in I}$ be a collection of connected sets in
    $X$ (we suppose that $I$ is non-empty). Suppose also that
    $\bigcap_{\alpha \in I} E_\alpha$ is non-empty. Show that
    $\bigcup_{\alpha \in I} E_\alpha$ is connected.}

  As in Exercise 13.4.4, it may seem more natural to give a proof by
  contradiction to prove connectedness. Thus, let's suppose for the
  sake of contradiction that $\bigcup_{\alpha \in I} E_\alpha$ is disconnected. It means
  that there exists two open, disjoint and non empty sets $V$ and $W$
  in $X$ such that $\bigcup_{\alpha \in I} E_\alpha = V \sqcup W$.

  Let's take an arbitrary $E_\alpha$. We clearly have
  $E_\alpha \subseteq V \sqcup W$. But since $E_\alpha$ is connected, we must have either
  $E_\alpha \subseteq V$, or $E_\alpha \subseteq W$. Indeed, if it were not the case,
  $E_\alpha$ could be split into two open (in $E_\alpha$), disjoint and
  non-empty sets (namely $V \cap E_\alpha$ and $W \cap E_\alpha$), which is a
  contradiction. Thus, for all $\alpha \in I$, we have either $E_\alpha \subseteq V$, or
  $E_\alpha \subseteq W$.

  Furthermore, there exists at least one $\alpha_1 \in I$ such that
  $E_{\alpha_1} \subseteq V$ (otherwise $V$ would be empty, which is excluded); and
  similarly there exists at least one $\alpha_2 \in I$ such that $E_{\alpha_2} \subseteq
  W$. Thus, $E_{\alpha_1} \cap E_{\alpha_2} = \emptyset$, which implies that $\bigcap_{\alpha \in I}
  E_\alpha$ is empty. This contradicts out initial hypothesis.

  Thus, $\bigcup_{\alpha \in I} E_\alpha$ is necessarily connected.
\end{exo}

\bigskip
\begin{exo}{13.4.7}{Let $(X, d)$ be a metric space, and let $E$ be a
    subset of $X$. We say that $E$ is path-connected iff, for every
    $x, y \in E$, there exists a continuous function
    $\gamma : [0, 1] \to E$ from the unit interval $[0, 1]$ to $E$ such that
    $\gamma(0) = x$ and $\gamma(1) = y$. Show that every non-empty
    path-connected set is connected.}

  As for the previous exercises, we will use a proof by contradiction
  here. Thus, let be $E$ a path-connected set, and suppose for the
  sake of contradiction that $E$ is not connected. Thus, there exist
  two open (in $E$), disjoint and non empty sets $V, W \subseteq X$ such that
  $E = V \sqcup W$.

  Consider two points, $x \in V$ and $y \in W$. Since $E$ is path
  connected, there exists a continuous map $\gamma : [0,1] \to E$ such that
  $\gamma(0) = x$ and $\gamma(1) = y$. We know (for instance by Theorem 13.4.5)
  that the interval $[0,1]$ is a connected set. Thus, the image set $S
  := \gamma([0,1])$ is also a connected set, since $\gamma$ is continuous
  (Theorem 13.4.6).

  But since $V$ is open in $E$, then $V \cap S$ is open in $S$
  (Proposition 12.3.4). Similarly, $W \cap S$ is open in $S$. These sets
  are non-empty since they contain $x$ and $y$ respectively; and they
  are clearly disjoint since $V$ and $W$ are disjoint. Thus, we have
  $S = (S \cap V) \sqcup (S \cap W)$, so that $S$ is disconnected. This is a
  contradiction.

  Thus, $E$ must be a connected set.
\end{exo}
\bigskip

\begin{exo}{13.4.8}{Let $(X, d)$ be a metric space, and let $E$ be a
    subset of $X$. Show that if $E$ is connected, then the closure
    $\adh{E}$ of $E$ is also connected. Is the converse true?}

  Suppose, for the sake of contradiction, that $E$ is connected but
  $\adh{E}$ is not connected. In this case, there exist two open (in
  $\adh{E}$), disjoint and non-empty sets $V$ and $W$ such that
  $\adh{E} = V \sqcup W$. Intuitively, we will show that this partition of
  $\adh{E}$ will provide a partition of $E$ into two open, non-empty
  sets, which will be a contradiction with the connectedness of $E$.
  Thus, let be $V' := V \cap E$ and $W' := W \cap E$.
  \begin{itemize}
  \item These sets $V', W'$ are open in $E$, by Proposition~12.3.4.
  \item These sets are also non-empty\footnote{This is, quite
      paradoxically, the key point of this proof. Or, more precisely,
      this is the point that will not necessarily be true if we
      consider a superset of $E$ that is larger that $\adh{E}$ (can
      you find an example?). The general idea here is that $\adh{E}$
      is ``so close to $E$'' that $\adh{E}$ has ``no room to be
      disconnected'' if $E$ is connected.}. Indeed, let be $x \in V$.
    Since $V \subset \adh{E}$, $x$ is adherent to $E$. Thus, $x$ belongs
    either to $E$, or to $\partial E$. If $x \in E$, then clearly,
    $x \in V' := V \cap E$. If $x \in \partial E$, remember that $V$ is open in
    $\adh{E}$, so that there exists an open ball
    $B_{\adh{E}}(x, \epsilon) \subseteq V$. But this open ball has a non-empty
    intersection with $E$, because $x \in \adh{E}$. Thus, in both cases,
    $V'$ is non-empty. The same argument applies to $W$ and $W'$.
  \item Furthermore, since $E \subseteq \adh{E}$, we have\footnote{See for
      instance Proposition 3.1.28(f).}
    \begin{align*}
      V' \sqcup W' &= (V \cap E) \sqcup (W \cap E) \\
              &= (V \sqcup W) \cap E \\
              &= \adh{E} \cap E \\
              &= E.
    \end{align*}
  \end{itemize}
  Thus, $E$ is disconnected, which is a contradiction.

  However, the converse statement is not true. Indeed, consider the
  set $E := ]-1,0[ \sqcup ]0,1[$ in $\rr$. This set is clearly
  disconnected, since it is natively the disjoint union of two open
  intervals. But the closure $\adh{E} = [-1,1]$ is connected, since it
  is an interval.
\end{exo}

\pagebreak
\section{Uniform convergence}
\label{sec:uniform-convergence}
\begin{exo}{14.1.1}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces,
    let $E$ be a subset of $X$, let $f : E \to Y$ be a function, and let
    $x_0$ be an element of $E$. Show that the limit
    $\lim_{x\to x_0; x \in E} f(x)$ exists if and only if the limit
    $\lim_{x\to x_0; x \in E \setminus \{x_0\}} f(x)$ exists and is equal to
    $f(x_0)$. Also, show that if the limit
    $\lim_{x \to x_0; x \in E} f(x)$ exists at all, then it must equal
    $f(x_0)$.}

  We will denote (i) and (ii) the two following statements:
  \begin{enumerate}[label=(\roman*)]
  \item $\lim_{x \to x_0; x \in E}  f(x) = f(x_0)$;
  \item $\lim_{x \to x_0; x \in E \setminus \{x_0\}}  f(x) = f(x_0)$.
  \end{enumerate}

  Let's prove that (i) and (ii) are equivalent.
    
  \begin{itemize}
  \item First we show that (i) $\implies$ (ii). If
    $\lim_{x \to x_0; x \in E}{f(x)}$ exists, then there exists
    $L \in Y$ such that $\lim_{x \to x_0; x \in E}{f(x)} = L$. Let be
    $\epsilon > 0$. There exists a $\delta > 0$ such that:
    \begin{equation}
      \label{eq:14.1.1a}
      (x \in E \, \text{ and } \, d_X(x, x_0) < \delta)
      \implies
      d_Y(f(x), L) < \epsilon.
    \end{equation}

    Thus, for all $x \in E \setminus \{x_0\}$ such that
    $d_X(x, x_0) < \delta$, we will also have
    $d_Y(f(x), L) < \epsilon$, since $E \setminus \{x_0\} \subset E$. This means that we
    have $\lim_{x \to x_0 ; x \in E \setminus \{x_0\}}{f(x)} = L$.

    But we still have to prove that $L = f(x_0)$. Suppose for the sake
    of contradiction that $L \neq f(x_0)$. In particular, this implies
    that $\epsilon' := d_Y(f(x_0), L)/2 > 0$. Thus, by \eqref{eq:14.1.1a},
    there exists a $\delta'>0$ such that $d_Y(f(x), L) < \epsilon'$ for all
    $x \in E$ with $d_X(x, x_0) < \delta'$. But if we consider
    $x = x_0$, we clearly have $d_Y(f(x), L) = 2\epsilon' > \epsilon'$ although
    $x \in E$ and $d_X(x, x_0) = 0 < \delta'$, a contradiction. Thus, we must
    have $L = f(x_0)$.

    Note that this shows both that (i) $\implies$ (ii), and that if
    $\lim_{x \to x_0; x \in E}{f(x)}$ exists at all, then this limit is
    necessarily equal to $f(x_0)$.
    
  \item Now we show that (ii) $\implies$ (i). Let be $\epsilon>0$. Since we
    have (by hypothesis)
    $\lim_{x \to x_0 ; x \in E \setminus \{x_0\}}{f(x)} = f(x_0)$, there exists a
    $\delta>0$ such that
    \begin{equation}
      \label{eq:14.1.1b}
      (x \in E \setminus \{x_0\} \, \text{ and } \, d_X(x, x_0) < \delta)
      \implies
      d_Y(f(x), f(x_0)) < \epsilon.
    \end{equation}

    Now let be $x \in E$. We have two cases: either $x \neq x_0$, or
    $x=x_0$. If $x \neq x_0$, \eqref{eq:14.1.1b} is true. If $x=x_0$, we
    have both $d_X(x, x_0) = 0 < \delta$ and
    $d_Y(f(x), f(x_0)) = 0 < \epsilon$. Thus, in both cases, if
    $x \in E$ and $d_X(x, x_0) < \delta$, we have indeed
    $d_Y(f(x), f(x_0)) = 0 < \epsilon$. Thus,
    $\lim_{x \to x_0; x\in E}{f(x)} = f(x_0)$, and in particular (i) is
    true.
  \end{itemize}
\end{exo}

\bigskip
\begin{exo}{14.1.2}{Prove Proposition 14.1.5.}

  In this Proposition, the three statements (a), (b), (c) are the
  exact counterparts of Theorem 13.1.4, whereas statement (d) is sort
  of a whole new statement. We can thus prove it separately, showing
  first that (a) $\implies$ (c) $\implies$ (b) $\implies$ (a), and
  then that (a) $\iff$ (d).

  \begin{itemize}
  \item First we show that (a) $\implies (c)$. Suppose that
    $\lim_{x \to x_0; x \in E} f(x) = L$, and let be $V \subseteq Y$ an open set
    such that $L \in V$. By definition of an open set (Proposition
    12.2.15(a)), there exists $\epsilon > 0$ such that
    $B_Y(L, \epsilon) \subseteq V$. By hypothesis (a), for this $\epsilon > 0$, there exists
    a $\delta > 0$ such that:
    \[(x \in E; x \in B_X(x_0, \delta) \implies f(x) \in B_Y(L, \epsilon) \subseteq V.\]
    If we set $U := B_X(x_0, \delta)$, we have indeed found an open set $U$
    such that $f(U \cap E) \subseteq V$, as expected.
  \item Now we show that (c) $\implies$ (b). Consider a sequence
    $\seq{x^{(n)}}{1}$ of elements of $E$ that converges to $x_0$. Let
    be $\epsilon > 0$. It is clear that $V := B_Y(L, \epsilon)$ is an open set
    containing $L$. By (c), there exists an open set $U \subseteq X$
    containing $x_0$ such that $f(U \cap E) \subseteq V$. Since $U$ is open,
    there exists an open ball $B_X(x_0, \delta) \subseteq U$; and since
    $\seq{x^{(n)}}{1}$ converges to $x_0$, there exists an $N \geq 1$
    such that $n \geq N \implies x^{(n)} \in B_X(x_0, \delta)$. In particular,
    $n \geq N \implies x^{(n)} \in U \cap E$, so that by (c), we have
    $f(x^{(n)}) \in V$. Unfolding these statements, for all $\epsilon > 0$,
    there exists $N \geq 1$ such that $n \geq N \implies d_Y(f(x^{(n)}), L)
    < \epsilon$. Thus, $\seq{f(x^{(n)})}{1}$ converges to $L$, as expected.
  \item Now let's show that (b) $\implies$ (a). Suppose, for the sake
    of contradiction, that we have
    $\lim_{x \to x_0; x\in E} f(x) \neq L$. Thus, there exists an
    $\epsilon > 0$ such that, for all $\delta > 0$, we have an
    $x \in E$ with $d_Y(f(x), L) \geq \epsilon$ although
    $d_X(x, x_0) < \delta$. In particular, for all $n \geq 1$, there exists a
    $x^{(n)} \in E$ such that $d_X(x^{(n)}, x_0) < 1/n$ but
    $d_Y(f(x^{(n)}), L) \geq \epsilon$. By the (countable) axiom of choice, we
    can form the corresponding sequence $\seq{x^{(n)}}{1}$, which
    clearly converges to $x_0$. However, the sequence
    $\seq{f(x^{(n)})}{1}$ does not converge to $L$, a contradiction
    with (b). Thus, we must have $\lim_{x \to x_0; x\in E} f(x) = L$.
  \item Now let's show that (a) $\implies$ (d). Suppose that
    $\lim_{x \to x_0; x \in E} f(x) = L = g(x_0)$. We have indeed
    $\lim_{x \to x_0; x \in E \cup \{x_0\}} g(x) = L = g(x_0)$, because
    \begin{itemize}
    \item if $x \in E \setminus \{x_0\}$, we have
      $\lim_{x \to x_0; x \in E \setminus \{x_0\}} g(x) = \lim_{x \to x_0; x \in E \setminus
        \{x_0\}} f(x) = L = g(x_0)$ by definition (using the fact that
      $E \setminus \{x_0\} \subseteq E$);
    \item if $x = x_0$, we have of course $g(x) = g(x_0)$, i.e.
      $d_Y(g(x), g(x_0)) = 0$.
    \end{itemize}
    Thus, in both cases, for all $\epsilon > 0$, we can indeed find a
    $\delta > 0$ such that $d_Y(g(x), g(x_0)) < \epsilon$ whenever
    $x \in E \cup \{x_0\}$ and $d_X(x, x_0) < \delta$. In other words,
    $g$ is continuous at $x_0$.

    Furthermore, if $x_0 \in E$, we already know by Exercise 14.1.1 that
    we must have $f(x_0) = L$.
  \item Finally, let's show that (d) $\implies$ (a). By hypothesis, we
    have $\lim_{x \to x_0; x \in E \cup \{x_0\}} g(x) = g(x_0) = L$, and if
    $x_0 \in E$, we have $f(x_0) = g(x_0) = L$. We can split into two
    cases:
    \begin{itemize}
    \item if $x_0 \notin E$, then $E \setminus \{x_0\}$ is simply $E$, so that the
      result is trivial: we have
      $L = \lim_{x \to x_0; x \in E \setminus \{x_0\}} g(x) = \lim_{x \to x_0; x \in
        E} f(x)$;
    \item if $x_0 \in E$, then $E \cup \{x_0\}$ is simply $E$, and $g$ is
      simply $f$, so that (a) is trivial.
    \end{itemize}
  \end{itemize}
\end{exo}

\begin{exo}{14.1.5}{Let $(X, d_X)$, $(Y, d_Y)$, $(Z, d_Z)$ be metric
    spaces, and let $x_0 \in X$, $y_0 \in Y$, $z_0 \in Z$. Let
    $f : X \to Y$ and $g : Y \to Z$ be functions, and let $E$ be a subset
    of $X$. If we have $\lim_{x \to x_0; x \in E} f(x) = y_0$ and
    $\lim_{y \to y_0; y \in f(E)} g(y) = z_0$, conclude that
    $\lim_{x \in x_0; x \in E} g \circ f(x) = z_0$.}

  Let be $\epsilon > 0$.

  \begin{itemize}
  \item Since $g$ converges to $z_0$ at $y_0$ in $f(E)$, there exists
    a $\delta > 0$ such that $d_Z(g(y), z_0) < \epsilon$ whenever
    $y \in f(E)$ and $d_Y(y, y_0) < \delta$.
  \item Consider this same number $\delta > 0$. Since $f$ converges to
    $y_0$ at $x_0$ in $E$, there exists an $\alpha > 0$ such that
    $d_Y(f(x), y_0) < \delta$ whenever $x \in E$ and $d_X(x, x_0) < \alpha$.
  \end{itemize}

  Thus, for any $\epsilon > 0$, we can find an $\alpha > 0$ such that:
  \[
    \left(x \in E, \; d_X(x, x_0) < \alpha \right)
    \implies
    \left(f(x) \in f(E), \; d_Y(f(x), y_0) < \delta \right)
    \implies
    d_Z(g \circ f(x), z_0) < \epsilon.
  \]

  Thus, we have indeed  $\lim_{x \in x_0; x \in E} g \circ f(x) = z_0$.
\end{exo}

\bigskip
\begin{exo}{14.1.6}{State and prove an analogue of the limit laws in
    Proposition 9.3.14 when $X$ is now a metric space rather than a
    subset of $\rr$. (Hint: use Corollary 13.2.3.)}

  \textbf{Proposition.} Let $(X, d_X)$ and $(Y, d_Y)$ be metric
  spaces, and let $f,g : X \to Y$ be functions. Let be $E \subseteq X$ and
  $x_0$ an adherent point of $E$. Suppose that
  $\lim_{x \to x_0; x \in E} f(x) = L$ and
  $\lim_{x \to x_0; x \in E} g(x) = M$. Then $f+g$ has limit $L+M$ at
  $x_0$; $f-g$ has limit $L-M$ at $x_0$; $\max(f,g)$ and $\min(f,g)$
  have limit $\max(L,M)$ and $\min(L,M)$ at $x_0$ respectively; $fg$
  has limit $LM$ at $x_0$; and $cf$ has limit $cL$ at $x_0$ for any
  constant $c$. Furthermore, if $g(x) \neq 0$ for all $x \in X$ and
  $M \neq 0$, then $f/g$ has limit $L/M$ at $x_0$.

  \begin{proof}
    We just prove the first claim, i.e. that $\lim_{x \to x_0; x \in E}
    (f+g)(x) = L+M$, since all the other claims can be proved
    similarly.

    The hint suggests to use Corollary 13.2.3, but this result applies
    to continuous functions and we do not know know whether $f, g$ are
    continuous at $x_0$ (we can possibly even have $x_0 \notin E$ here, if
    $x_0$ is only a boundary point of $E$). However, we can actually
    ``build'' continuous counterparts for $f, g$, using Proposition
    14.1.5(d). Indeed, let be $F : E \cup \{x_0\} \to Y$ such that
    $F(x) = f(x)$ if $x \neq x_0$, and $F(x_0) = L$. Similarly, let be
    $G : E \cup \{x_0\} \to Y$ defined by $G(x) = g(x)$ if
    $x \neq x_0$ and $G(x_0) = M$. This time, it is clear by Proposition
    14.1.5(d) that $F, G$ are continuous at $x_0$.

    Thus, by Corollary 13.2.3, $F+G$ is continuous at $x_0$.
    Consequently, we have
    \[\lim_{x \to x_0; x \in E \cup \{x_0\}} (F+G)(x) = (F+G)(x_0) = L+M.\]

    This function $(F+G) : E \cup \{x_0\} \to Y$ is a continuous function
    such that $(F+G)(x_0) = L+M$, and $(F+G)(x) = (f+g)(x)$ for all
    $x \in E \setminus \{x_0\}$. The function $F+G$ satisfies Proposition
    14.1.5(d)\footnote{We also have the part ``if $x_0 \in E$, then
      $(f+g)(x_0) = L+M$'' because we have this part for $f,g$ taken
      separately.}, and thus we also have Proposition 14.1.5(a), i.e.
    $\lim_{x \to x_0; x \in E} (f+g)(x) = L+M$, as expected.
  \end{proof}
\end{exo}

\smallskip
\begin{exo}{14.2.1}{The purpose of this exercise is to demonstrate a
    concrete relationship between continuity and pointwise
    convergence, and between uniform continuity and uniform
    convergence. Let $f : \rr \to \rr$ be a function. For any
    $a \in \rr$, let $f_a : \rr \to \rr$ be the shifted function
    $f_a(x) := f(x - a)$.
    \begin{enumerate}[label=(\alph*)]
    \item Show that $f$ is continuous if and only if, whenever
      $\seq{a_n}{0}$ is a sequence of real numbers which converges to
      zero, the shifted functions $f_{a_n}$ converge pointwise to $f$.
    \item Show that $f$ is uniformly continuous if and only if,
      whenever $\seq{a_n}{0}$ is a sequence of real numbers which
      converges to zero, the shifted functions $f_{a_n}$ converge
      uniformly to $f$.
    \end{enumerate}}

  Let be $a \in \rr$, and $\seq{a_n}{0}$ a sequence of real numbers that
  converges to $0$.

  \begin{enumerate}[label=(\alph*)]
  \item Suppose that $f$ is continuous. Let be $\epsilon > 0$, and let be an
    arbitrary $x \in \rr$. In particular, since $f$ is continuous at
    $x$, there exists a $\delta_x > 0$ such that $|x-y| < \delta_x \implies |f(x) -
    f(y)| < \epsilon$. Furthermore, since $\seq{a_n}{0}$ converges to $0$,
    there exists a $N \geq 0$ such that $n \geq N \implies |a_n| < \delta_x$. Thus,
    for any $x \in \rr$ and any $\epsilon >0$ we have a $N \geq 0$ such that
    \[n \geq N
      \implies |x - (x-a_n)| < \delta_x
      \implies \underbrace{|f(x) - f(x-a_n)|}_{=|f(x)-f_{a_n}(x)|} <
      \epsilon\]
    which means that $\seq{f_{a_n}}{0}$ converges pointwise to $f$.

    Conversely, suppose that $\seq{f_{a_n}}{0}$ converges pointwise to
    $f$ for any sequence $\seq{a_n}{0}$, but suppose, for the sake of
    contradiction, that $f$ is not continuous. Thus, there exists some
    $\epsilon > 0$ such that, for all $n \geq 0$, we have a
    $y_n \in \rr$ with $|x - y_n| < \frac{1}{n+1}$ but
    $|f(x) - f(y_n)| \geq \epsilon$. By the (countable) axiom of choice, we can
    form the corresponding sequence $\seq{y_n}{0}$. And if we define
    another sequence $a_n := x-y_n$, then it is clear that
    $\seq{a_n}{0}$ converges to $0$ (since $|a_n| < \frac{1}{n+1}$ for
    all $n \geq 0$); but that the functions $f_{a_n}$ do not converge
    pointwise to $f$, since $|f(x) - f_{a_n}(x)| \geq \epsilon$ for all $n \geq 0$.
  \item Suppose that $f$ is uniformly continuous. Let be
    $\epsilon > 0$. By definition, there exists a $\delta > 0$ such that, for all
    $x,y \in \rr$, we have
    $|x-y| < \delta \implies |f(x) - f(y)| < \epsilon$. Furthermore, since
    $\seq{a_n}{0}$ converges to $0$, there exists a $N \geq 0$ such that
    $n \geq N \implies |a_n| < \delta$. Thus, for any $\epsilon >0$ we have a
    $N \geq 0$ such that, for all $x \in \rr$,
    \[n \geq N
      \implies |x - (x-a_n)| < \delta
      \implies \underbrace{|f(x) - f(x-a_n)|}_{=|f(x)-f_{a_n}(x)|} <
      \epsilon\]
    which means that $\seq{f_{a_n}}{0}$ converges uniformly to $f$.

    Conversely, suppose that $\seq{f_{a_n}}{0}$ converges uniformly to
    $f$ for any sequence $\seq{a_n}{0}$, but suppose, for the sake of
    contradiction, that $f$ is not uniformly continuous. Thus, there
    exists some $\epsilon > 0$ such that, for all $n \geq 0$, we have some
    $x_n, y_n \in \rr$ with $|x_n - y_n| < \frac{1}{n+1}$ but
    $|f(x_n) - f(y_n)| \geq \epsilon$. By the (countable) axiom of choice, we
    can form the corresponding sequences $\seq{x_n}{0}$ and
    $\seq{y_n}{0}$. If we define a new sequence $a_n := x_n - y_n$,
    then $\seq{a_n}{0}$ clearly converges to $0$. However,
    $\seq{f_{a_n}}{0}$ does not converge uniformly to $f$, since for
    all $n \geq 0$, we have
    \[ \epsilon \leq |f(x_n) - f(y_n)| = |f(x_n) - f_{a_n}(x_n)|.\]
    This is thus a contradiction, and $f$ must be uniformly
    continuous.
  \end{enumerate}
\end{exo}

\begin{exo}{14.2.2}{(a) Let $\seq{f^{(n)}}{1}$ be a sequence of
    functions from one metric space $(X, d_X)$ to another $(Y, d_Y)$,
    and let $f : X \to Y$ be another function from $X$ to $Y$. Show that
    if $f^{(n)}$ converges uniformly to $f$, then $f^{(n)}$ also
    converges pointwise to $f$.\\
    (b) For each integer $n \geq 1$, let $f^{(n)} : ]-1, 1[ \to \rr$ be the
    function $f^{(n)}(x) := x^n$. Prove that $f^{(n)}$ converges
    pointwise to the zero function $0$, but does not converge
    uniformly to any function $f : ]-1, 1[ \to \rr$.\\
    (c) Let $g : (-1, 1) \to \rr$ be the function $g(x) := x/(1-x)$.
    With the notation as in (b), show that the partial sums
    $\sum_{n=1}^N f(n)$ converges pointwise as $N \to \infty$ to
    $g$, but does not converge uniformly to $g$, on the open interval
    $]-1, 1[$. (Hint: use Lemma 7.3.3.) What would happen if we
    replaced the open interval $]-1, 1[$ with the closed interval
    $[-1, 1]$?}

  \vspace{-4mm}
  \begin{enumerate}[label=(\alph*)]
  \item Let be $\epsilon > 0$. Since the $f^{(n)}$ converge uniformly to
    $f$, there exists a $N \geq 0$ such that, for all $x \in X$, we have
    $d_X(f^{(n)}(x), f(x)) < \epsilon$. Thus, for any fixed
    $x \in \rr$, this same $N \geq 0$ is such that
    $d_X(f^{(n)}(x), f(x)) < \epsilon$ whenever $n \geq N$, i.e., the
    $f^{(n)}$ converge pointwise to $f$.
  \item By Lemma 6.5.2, we know that for all $x \in ]-1, 1[$, we have
    $\lim_{n \to \infty} x^n = 0$. Thus, $f^{(n)}$ converges pointwise to the
    zero function.

    However, $f^{(n)}$ does not converge uniformly to the zero
    function\footnote{If the uniform limit exists, it can only be
      equal to the pointwise limit, by (a) and the uniqueness of
      limits.}, as stated in the main text. More formally, we can give
    a proof in the same spirit as what we did in Exercises 13.3.3 and
    13.3.5. Indeed, let be $\epsilon = 0.1$, and suppose that there exists $N
    \geq 0$ such that, for all $x \in ]-1,1[$, $n \geq N \implies |x^n| < 0.1$.
    Actually, we are always able to find an $x \in ]-1,1[$ such that
    $|x^n| \geq 0.1$ although $n \geq N$. For instance, for $n = N$ and $x =
    0.1^{1/N}$, we have $|x^n| = \left|((1/10)^{1/N})^N\right| = 0.1$.
  \item Here we will denote $S_N(x) := \sum_{n=1}^{N} f^{(n)}(x)$ the
    partial sums. We already know, by Lemma 7.3.3, that we have, for
    all $x \in ]-1,1[$,
    \begin{equation*}
      \lim_{N \to \infty} \sum_{n=0}^{N} f^{(n)}(x) = \frac{1}{1-x},
    \end{equation*}
    so that we have
    \[\lim_{N \to \infty} \sum_{n=1}^{N} f^{(n)}(x)
      = \lim_{N \to \infty} \left(\sum_{n=0}^{N} f^{(n)}(x) - 1\right) =
      \frac{1}{1-x} - 1 = \frac{x}{1-x},\]
    thus meaning that $S_N$ converges pointwise to $x/(1-x)$ as
    $N \to \infty$, as expected.

    However, $S_N$ does not converge uniformly to $x/(1-x)$ as
    $N \to \infty$. The explicit expression of the partial sums is
    $S_N := \frac{1-x^{N+1}}{1-x} - 1 = \frac{x(1-x^N)}{1-x}$, for all
    $N \geq 1$, and for all $x \in ]-1, 1[$. In the same spirit as question
    (b), let be $\epsilon := 0.1$, and suppose that there exists
    $N_0 \geq 0$ such that, for all $x \in ]-1,1[$, we have
    \[
      N \geq N_0 \implies \left|\frac{x(1-x^N)}{1-x} -
        \frac{x}{1-x}\right| = \left| \frac{-x^{N+1}}{1-x} \right| =
      \frac{x^{N+1}}{1-x} < 0.1
    \]
    Actually, we are always able to find an $x \in ]-1,1[$ such that
    $\frac{x^{N+1}}{1-x} \geq 0.1$ although $N \geq N_0$. For instance, for
    $N = N_0$ and $x = (1/10)^{\frac{1}{N_0+1}}$, we have
    $\frac{x^{N+1}}{1-x} = 1/9 > 0.1$ although $N \geq N_0$. Thus, we do
    not have uniform convergence here.
    
    Finally, if we replace $]-1,1[$ by $[-1,1]$, then $S_N(x)$ does
    not even converge pointwise (see Lemma 7.3.3 when $x=1$ or
    $x=-1$).
  \end{enumerate}
\end{exo}

\begin{exo}{14.2.3}{Let be $(X, d_X)$ a metric space, and for every
    integer $n \geq 1$, let $f_n : X \to \rr$ be a real-valued
    function. Suppose that $f_n$ converges pointwise to another
    function $f : X \to \rr$ on $X$ (in this question we give $\rr$ the
    standard metric $d(x,y) := |x-y|$). Let $h : \rr \to \rr$ be a
    continuous function. Show that the functions $h \circ f_n$ converge
    pointwise to $h \circ f$ on $X$, where $h \circ f_n : X \to \rr$ is the
    function $h \circ f_n(x) := h(f_n(x))$, and similarly for $h \circ f$.}

  This exercise is a trivial application of the equivalence (a) $\iff$
  (b) in Theorem 13.1.4. Indeed, let be an arbitrary $x_0 \in X$. Since
  $f_n$ converges pointwise to $f$ on $X$, the sequence
  $\seq{f_n(x_0)}{1}$ converges to $f(x_0)$ in $(\rr, d)$, by
  definition. And since $h$ is continuous on $\rr$, then by Theorem
  13.2.4(b), we have $\lim_{n \to \infty} h(f_n(x_0)) = h(f(x_0))$, which
  means exactly that $\seq{h \circ f_n}{1}$ converges pointwise to
  $h \circ f$ on $X$.
\end{exo}

\bigskip
\begin{exo}{14.2.4}{Let $f_n : X \to Y$ be a sequence of bounded
    functions from one metric space $(X, d_X)$ to another metric space
    $(Y, d_Y)$. Suppose that $f_n$ converges uniformly to another
    function $f : X \to Y$. Suppose that $f$ is a bounded function;
    i.e., there exists a ball $B_{(Y,d_Y)}(y_0, R)$ in $Y$ such that
    $f(x) \in B_{(Y, d_Y)}(y_0, R)$ for all $x \in X$. Show that the
    sequence $f_n$ is uniformly bounded; i.e. there exists a ball
    $B_{(Y,d_Y)}(y_0, R)$ in $Y$ such that
    $f_n(x) \in B_{(Y,d_Y)}(y_0, R)$ for all $x \in X$ and all positive
    integers $n$.}

  Let be $\epsilon := 1$, for instance. Since
  $\seq{f_n}{1}$ converges to $f$, there exists $N \geq 1$ such that,
  for all $x \in X$, we have $d_Y(f_n(x), f(x)) < 1$ whenever
  $n > N$.
  
  Furthermore, $f$ is supposed to be bounded. Thus, there exists a
  $y_0 \in Y$ and a real $R > 0$ such that $d_Y(f(x), y_0) < R$ for
  all $x \in X$.
  
  Thus, if $n > N$, we have (by triangle inequality) for all $x \in
  X$,
  \[d_Y(f_n(x), y_0) \leq d_Y(f_n(x), f(x)) + d_Y(f(x), y_0) < 1 + R.\]
  In other words, the sequence $\seq{f_n}{N+1}$ is uniformly bounded.
  Now, we still have to extend this result to the whole sequence
  $\seq{f_n}{1}$.
  
  Recall that each $f_n$ is bounded. Thus, there exists in particular
  a finite sequence of centers
  $y_1, \ldots, y_{N} \in Y$ and a finite sequence of radii
  $R_1, \ldots, R_{N} > 0$ such that, for all $x \in X$,
  \[f_1(x) \in B_Y(y_1, R_1) \; ; \; \ldots
    \; ; \; f_{N}(x) \in B_Y(y_{N}, R_{N}).\]
  
  Let be
  \[
    S := \left(\max_{1 \leq i \leq N} d_Y(y_i, y_0)\right) + \left(\max_{1
        \leq i \leq N} R_i\right) + (R + 1).
  \]
  We have thus by triangle inequality, for all $1 \leq i \leq N$ and for
  all $x \in X$,
  \begin{alignat*}{3}
    d_Y(f_i(x), y_0) &\leq \quad d_Y(f_i(x), y_i) &+& \quad d_Y(y_i, y_0) \\
                     &\leq \quad R_i &+& \quad d_Y(y_i, y_0) \\
                     &\leq \left(\max_{1 \leq i \leq N} R_i\right)  &+&
                                                               \left(\max_{1 \leq i \leq N} d_Y(y_i, y_0)\right)
    \\
                     &\leq S.&&
  \end{alignat*}
  
  Thus, we have indeed $d_Y(f_n(x), y_0) < S$ for all $x \in X$ and for
  all $n \geq 1$, which means that $\seq{f_n}{1}$ is uniformly bounded.
\end{exo}
\bigskip

\begin{exo}{14.3.1}{Prove Theorem 14.3.1. Explain brieﬂy why your proof
    requires uniform convergence, and why pointwise convergence would
    not suffice.}

  Let be $\epsilon > 0$ an arbitrary real number. By our initial hypothesis,
  the sequence $\seq{f^{(n)}}{1}$ converges uniformly to $f$. Thus,
  there exists an $N \geq 1$ such that:
  \begin{equation}
    \label{eq:14.3.1a}
    (n > N) \text{ and } (x \in X) \implies d_Y(f^{(n)}(x), f(x)) < \epsilon/3.
  \end{equation}
  
  Furthermore, each function in the sequence $\seq{f^{(n)}}{1}$ is
  continuous at $x_0$. In particular, the function $f^{N+1}$ is
  continuous at $x_0$: there exists a $\delta > 0$ such that
  \begin{equation}
    \label{eq:14.3.1b}
    (x \in X) \text{ and } (d_X(x, x_0) < \delta) \implies d_Y(f^{(N+1)}(x), f^{(N+1)}(x_0)) < \epsilon/3.
  \end{equation}

  Since $N + 1 > N$, we have, for all $x \in X$ such that
  $d_X(x, x_0) < \delta$,
  \begin{align*}
    d_Y\left(f(x), f(x_0) \right)
    &\leq \underbrace{d_Y\left(f(x), f^{(N+1)}(x) \right)}_{< \epsilon/3
      \text{ by \eqref{eq:14.3.1a}}}
      + \underbrace{d_Y\left(f^{(N+1)}(x), f^{(N+1)}(x_0) \right)}_{< \epsilon/3
      \text{ by \eqref{eq:14.3.1b}}}\\
    &+ \underbrace{d_Y\left(f^{(N+1)}(x_0), f(x_0) \right)}_{< \epsilon/3
      \text{ by \eqref{eq:14.3.1a}}} \\
    &< \epsilon.
  \end{align*}
  Since this is true for any arbitrary $\epsilon > 0$, the function $f$ is
  indeed continuous at $x_0$.

  Note that the hypothesis of \emph{uniform} convergence was indeed
  necessary. In the previous inequalities, one single $N \geq 1$
  was convenient to have both
  \[
    d_Y\left(f(x), f^{(N+1)}(x) \right) < \epsilon/3
    \;\;\;\; \text{ and } \;\;\;\;
    d_Y\left(f^{(N+1)}(x_0), f(x_0) \right) < \epsilon/3,
  \]
  at the same time at $x$ and at $x_0$. Example 14.2.4 is a
  counter-example when we only suppose pointwise convergence.
\end{exo}
\bigskip

\begin{exo}{14.3.2}{Prove Proposition 14.3.3}

  Let be $\epsilon > 0$, and $x_0$ an adherent point to $E$. Hereafter, we
  denote by convenience
  $\ell_n := \lim_{x \to x_0; x \in E}{f^{(n)}(x)}$, for all
  ${n \geq 1}$. We thus have to prove that
  $\lim_{n \to \infty}{\ell_n} = \lim_{x \to x_0; x \in E}{f(x)}$.

  However, we must begin by proving that the sequence
  $\seq{\ell_n}{1}$ is indeed convergent. Since this sequence is
  entirely composed of elements of $Y$, and since $Y$ is complete, it
  is sufficient to prove that $\seq{\ell_n}{1}$ is a Cauchy sequence.

  To that end, consider two integers $j,k \geq 1$ and an element
  $x \in E$. We note that we can always write
  \begin{align*}
    d_Y(\ell_j, \ell_k) &\leq d_Y(\ell_j, f^{(j)}(x)) + d_Y(f^{(j)}(x), f(x))\\
                  &\quad + d_Y(f(x), f^{(k)}(x)) + d(f^{(k)}(x), \ell_k).
  \end{align*}
  Let's prove that each term of this sum is inferior to $\epsilon/4$ under
  certain conditions.

  First, the sequence $\seq{f^{(n)}}{1}$ is supposed to converge
  uniformly to $f$. Thus, there exists an $N_1 \geq 1$ such that
  \begin{equation}
    \label{eq:14.3.2a}
    (n > N) \text{ and } (x \in E) \implies
    d_Y(f^{(n)}(x), f(x)) < \epsilon/4.
  \end{equation}

  Let be two integers $j,k > N_1$. By definition of $\ell_n$, there exist
  two real numbers $\delta_1, \delta_2 > 0$ such that
  \begin{align}
    (x \in E) \text{ and } (d_X(x, x_0) < \delta_1)
    &\implies d_Y(f^{(j)}(x), \ell_j) < \epsilon/4,\label{eq:14.3.2b}\\
    (x \in E) \text{ and } (d_X(x, x_0) < \delta_2)
    &\implies d_Y(f^{(k)}(x), \ell_k) < \epsilon/4\label{eq:14.3.2c}.
  \end{align}

  Now we set $\delta := \min(\delta_1, \delta_2)$. Thus, for all
  $j,k > N_1$ and for all $x \in E$ such that $d_X(x, x_0) < \delta$, we have
  \begin{align*}
    d_Y(\ell_j, \ell_k)
    &\leq \underbrace{d_Y(\ell_j, f^{(j)}(x))}_{<\epsilon/4 \text{ by \eqref{eq:14.3.2b}}}
      + \underbrace{d_Y(f^{(j)}(x), f(x))}_{<\epsilon/4 \text{ by \eqref{eq:14.3.2a}}}\\
    &\quad + \underbrace{d_Y(f(x), f^{(k)}(x))}_{<\epsilon/4 \text{ by \eqref{eq:14.3.2a}}}
      + \underbrace{d(f^{(k)}(x), \ell_k)}_{<\epsilon/4 \text{ by
      \eqref{eq:14.3.2c}}}\\
    &< \epsilon.
  \end{align*}
  Thus, $\seq{\ell_n}{1}$ is a Cauchy sequence, and thus converges in
  $Y$; in other words, there exists a $\ell \in Y$ such that
  $\ell = \lim_{n \to \infty}{\ell_n}$.

  To close the proof, we still have to show that
  $\lim_{x \to x_0; x \in E}{f(x)} = \ell$. Since
  $\ell = \lim_{n \to \infty}{\ell_n}$, there exists an integer
  $N_2 \geq 1$ such that $d_Y(\ell, \ell_n) < \epsilon/4$ for all
  $n \geq N_2$. Let be $N := \max(N_1, N_2) + 1$. We thus have, for all
  $x \in E$ such that $d_X(x, x_0) < \delta$, and all $n \geq N$,
  \begin{align*}
    d_Y(f(x), \ell) &\leq d_Y(f(x), f^{(n)}(x)) + d_Y(f^{(n)}(x), \ell_n)
                   + d_Y(\ell_n, \ell)\\
                 &< \epsilon/4 + \epsilon/4 + \epsilon/4 \\
                 &< \epsilon.
  \end{align*}
  This shows that $\lim_{x \to x_0; x \in E}{f(x)} = \ell$, and thus closes
  the proof:
  \begin{align*}
    \lim_{x \to x_0; x \in E} \lim_{n \to \infty} f^{(n)}(x)
    &= \lim_{x \to x_0; x \in E}{f(x)} = \ell = \lim_{n \to \infty}{\ell_n} \\
    &= \lim_{n \to \infty} \lim_{x \to x_0; x \in E} f^{(n)}(x).
  \end{align*} 
\end{exo}

\begin{exo}{14.3.5}{Give an example to show that Proposition 14.3.4
    fails if the phrase ``converges uniformly'' is replaced by
    ``converges pointwise''. (Hint: some of the examples already given
    earlier will already work here.)}

  Consider once again the sequence of functions $\seq{f^{(n)}}{1}$,
  such that for all $n \geq 1$, the function $f^{(n)} : [0,1] \to \rr$ is
  defined by $f^{(n)}(x) := x^n$. This sequence converges pointwise to
  the function $f : [0,1] \to \rr$ defined by $f(x) := 0$ when
  $x \in [0,1[$, and $f(1) := 1$. Furthermore, consider the sequence
  $\seq{x^{(n)}}{1}$ defined by $x^{(n)} := (1/2)^{1/n}$.

  By Lemma 6.5.3, we have $\lim_{n \to \infty}{x^{(n)}} = 1$. However,
  for all $n \geq 1$, we have $f^{(n)}(x^{(n)}) = 1/2$. Thus, it is clear
  that this constant sequence $1/2$ does not converge to $f(1) = 1$.
\end{exo}
\bigskip

\begin{exo}{14.3.6}{Prove Proposition 14.3.6. Discuss how this
    proposition differs from Exercise 14.2.4}

  Consider for instance $\epsilon := 1$. Since the functions $f^{(n)}$
  converge uniformly to $f$, there exists an integer $N \geq 1$ such
  that, for all $n > N$ and all $x \in X$, we have
  $d_Y(f^{(n)}(x), f(x)) < 1$.

  Furthermore, since all the functions $f^{(n)}$ are bounded, then in
  particular $f^{(N+1)}$ is bounded: there exists a ball
  $B_Y(y_{N+1}, R_{N+1})$ such that
  $f^{(N+1)}(x) \in B_Y(y_{N+1}, R_{N+1})$ for all $x \in X$. In other
  words, we have $d_Y(f^{(N+1)}(x), y_{N+1}) < R_{N+1}$ for all
  $x \in X$.

  Thus, by triangle inequality, we have for all $x \in X$,
  \begin{align*}
    d_Y(f(x), y_{N+1}) &\leq d_Y(f(x), f^{(N+1)}(x)) + d_Y(f^{(N+1)},
                         y_{N+1}) \\
                       &< 1 + R_{N+1}.
  \end{align*}
  We thus have $f(x) \in B_Y(y_{N+1}, 1 + R_{N+1})$ for all $x \in X$,
  which shows $f$ is bounded on $X$.

  This provides some sort of reciprocal to Exercise 13.2.4, where we
  supposed that the uniform limit $f$ was bounded to show that the
  $f^{(n)}$ were themselves bounded.
\end{exo}
\bigskip

\begin{exo}{14.3.7}{Give an example to show that Proposition 14.3.6
    fails if ``converge uniformly'' is replaced by ``converges
    pointwise''.}

  Consider once again the functions from Exercise 14.2.2(c), i.e., a
  sequence of functions $g^{(n)} := \sum_{k=1}^{n} x^k$ defined on
  $(-1,1)$ and that converge pointwise to $g(x) := x / (1-x)$. It is
  clear that each $g^{(n)}$ is bounded on $(-1,1)$: we have
  $g^{(n)}(x) \in B(0, n)$ for all $n$. However, the function $g(x)$ is
  not bounded on $(-1,1)$.
\end{exo}
\bigskip

\begin{exo}{14.3.8}{Let $(X, d)$ be a metric space, and for every
    positive integer $n$, let $\seq{f^{(n)}}{1} : X \to \rr$ and
    $\seq{g^{(n)}}{1} : X \to \rr$ be functions. Suppose that
    $\seq{f^{(n)}}{1}$ converges uniformly to another function
    $f : X \to \rr$, and that $\seq{g^{(n)}}{1}$ converges uniformly to
    another function $g : X \to \rr$. Suppose also that the functions
    $\seq{f^{(n)}}{1}$ and $\seq{g^{(n)}}{1}$ are uniformly
    bounded. Prove that the functions $f_n g_n : X \to \rr$ converge
    uniformly to $fg : X \to \rr$.}

  Let be an arbitrary $\epsilon > 0$. To show that
  $\seq{f_ng_n}{1}$ converge uniformly to $fg$, we must show that
  there exists an integer $N \geq 1$ such that
  $|f_n(x)g_n(x) - f(x)g(x)| < \epsilon$ for all $n > N$ and all
  $x \in \rr$. Note that
  \begin{align*}
    &|f_n(x)g_n(x) - f(x)g(x)|\\
    &= |f_n(x)g_n(x) - f_n(x)g(x) + f_n(x)g(x) - f(x)g(x)| \\
    &\leq |f_n(x)g_n(x) - f_n(x)g(x)| + |f_n(x)g(x) - f(x)g(x)| \\
    &\leq |f_n(x)| \times |g_n(x) - g(x)| \, + \, |g(x)| \times |f_n(x) - f(x)|.
  \end{align*}
  We can find an upper bound for each term in this expression.
  
  \begin{itemize}
  \item First, $\seq{f_n}{1}$ is uniformly bounded, so that there
    exists a $M_1 \geq 0$ such that $|f_n(x)| \leq M_1$ for all
    $n \geq 1$ and all $x \in \rr$.
  \item Similarly, $\seq{g_n}{1}$ is uniformly bounded. Thus, by
    Proposition 14.3.6, the uniform limit $g$ is also bounded: there
    exists a $M_2 \geq 0$ such that $|g(x)| \leq M_2$ for all $x \in \rr$.
  \item Furthermore, $\seq{f_n}{1}$ converges uniformly to $f$:
    there exists an integer $N_1 \geq 1$ such that
    $|f_n(x) - f(x)| < \epsilon/2M_2$ for all $n > N_1$ and all $x \in \rr$.
  \item Finally, $\seq{g_n}{1}$ converges uniformly to $g$: there
    exists an integer $N_2 \geq 1$ such that
    $|g_n(x) - g(x)| < \epsilon/2M_1$ for all $n > N_2$ and all $x \in \rr$.
  \end{itemize}
  
  Let be $N := \max(N_1, N_2)$. We thus have, for all $x \in \rr$ and
  all $n > N$,
  \begin{align*}
    &|f_n(x)g_n(x) - f(x)g(x)| \\
    &\leq |f_n(x)| \times |g_n(x) - g(x)| \, + \, |g(x)| \times |f_n(x) - f(x)|
    \\
    &< M_1 \times \frac{\epsilon}{2M_1} + M_2 \times \frac{\epsilon}{2M_1} \\
    &< \epsilon.
  \end{align*}
  It means that $\seq{f_ng_n}{1}$ converges uniformly to $fg$, as
  expected.
\end{exo}
\bigskip

\begin{exo}{14.4.1}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Show
    that the space $B(X \to Y)$ defined in Definition 14.4.2, with the
    metric $d_{B(X \to Y)}$, is indeed a metric space.}

  Here we must prove that $d_\infty$ satisfies all four properties of a
  distance on $B(X \to Y)$, as stated in Definition 12.1.2.

  \begin{enumerate}[label=(\alph*)]
  \item For all $f \in B(X \to Y)$ and all $x \in X$, we clearly have
    $d_Y(f(x), f(x)) = 0$, since $d_Y$ is a distance on $Y$. Thus it
    is clear that
    \[d_\infty(f, f) = \sup_{X \in X} d_Y(f(x), f(x)) = \sup \{0 \, : \, x
      \in X\} = 0.\]
  \item For all distinct $f, g \in B(X \to Y)$, there exists an
    ${x_0 \in X}$ such that $f(x_0) \neq g(x_0)$. We thus have
    $d_Y(f(x_0), g(x_0)) > 0$, since $d_Y$ is a distance on $Y$. Thus,
    by Definition 5.5.5 of a supremum, we have
    $0 < d_Y(f(x_0), g(x_0)) \leq \sup_{x \in X}{d_Y(f(x), g(x))}$, which
    implies $d_\infty(f,g) > 0$.
  \item Since $d_Y$ is a distance on $Y$, we have
    $d_Y(f(x), g(x)) = d_Y(g(x), f(x))$ for all $f,g \in B(X \to Y)$ and
    all $x \in X$. We thus have
    \[
      d_\infty(f,g) := \sup_{x \in X}{d_Y(f(x), g(x))}
      = \sup_{x \in X}{d_Y(g(x), f(x))}
      =: d_\infty(g,f).
    \]
  \item Let $f, g, h \in B(X \to Y)$ be functions. Since $d_Y$
    is a distance on $Y$, it satisfies the triangle inequality on
    $Y$: for all ${x \in X}$, we have $d_Y(f(x), h(x)) \leq d_Y(f(x),
    g(x)) + d_Y(g(x), h(x))$.
    
    In particular, for all $x \in X$, we have
    \[
      d_Y(f(x), h(x)) \leq \sup_{X \in X} d_Y(f(x), g(x)) + \sup_{x \in X}
      d_Y(g(x), h(x)),
    \]
    or in other words, $d_\infty(f,g) + d_\infty(g,h)$ is an upper bound of the
    set $\{d_Y(f(x), h(x)) \, : \, x \in X\}$. Thus, by Definition 5.5.5
    of a supremum, we have
    \[
      d_\infty(f,h) := \sup_{x \in X} \{d_Y(f(x), h(x)) \, : \, x \in X\}
      \leq d_\infty(f,g) + d_\infty(g,h),
    \]
    as expected.
  \end{enumerate}
\end{exo}

\begin{exo}{14.5.1}{Let $f^{(1)}, \ldots, f^{(n)}$ be a sequence of bounded
    functions from a metric space $(X, d_X)$ to $\rr$. Show that
    $\sum_{i=1}^{n} f^{(i)}$ is also bounded. Prove a similar claim when
    ``bounded'' is replaced by ``continuous''. What if ``continuous''
    was replaced by ``uniformly continuous''?}
  
  Let $F : X \to \rr$ be the finite sum defined by
  $F(x) := \sum_{i=1}^{n} f^{(i)}(x)$.

  \begin{itemize}
  \item If the functions $f^{(1)}, \ldots, f^{(n)}$ are bounded, then there exist
    $n$ positive real numbers $M_1, \ldots, M_n$ such that
    $|f^{(i)}(x)| \leq M_i$ for all $x \in X$ and all $1 \leq i \leq
    n$. Let be $M := \sum_{i=1}^{n} M_i$.

    We thus have, by Lemma 7.1.4(e),
    \[
      \left| F(x) \right| = \left| \sum_{i=1}^{n} f^{(i)}(x) \right|
      \leq
      \sum_{i=1}^{n} \left|f^{(i)}(x) \right|
      \leq
      \sum_{i=1}^{n} M_i
      =
      M.
    \]
    The finite sum $F$ is thus bounded (by $M$).
    
  \item Suppose that the functions $f^{(1)}, \ldots, f^{(n)}$ are
    continuous at some point $x_0 \in X$, and let be an arbitrary
    $\epsilon > 0$. By definition, for all $1 \leq i \leq n$, there exists a
    $\delta_i > 0$ such that
    $d_X(x, x_0) \leq \delta_i \implies |f^{(i)}(x) - f^{(i)}(x_0)| \leq
    \epsilon/n$. Let be $\delta := \min(\delta_1, \ldots, \delta_n)$.

    For all $x \in X$ such that $d_X(x, x_0) \leq \delta$, we thus have
    \begin{align*}
      |f(x) - f(x_0)|
      &= \left| \sum_{i=1}^{n} f^{(i)}(x) - \sum_{i=1}^{n} f^{(i)}(x_0)
        \right| \\
      &\leq \sum_{i=1}^{n} \left| f^{(i)}(x) - f^{(i)}(x_0) \right|
      & \text{(Lemma 7.1.4(e))} \\
      &\leq \sum_{i=1}^{n} \frac{\epsilon}{n} \\
      &\leq \epsilon.
    \end{align*}
    The finite sum $F$ is thus also continuous at $x_0$.  And since
    $x_0$ was arbitrary, we have the more general result that if
    $f^{(1)}, \ldots, f^{(n)}$ are continuous on $X$, then $F$ is
    continuous on $X$.
    
  \item Similarly, if $f^{(1)}, \ldots, f^{(n)}$ are uniformly continuous
    on $X$, then $F$ is also uniformly continuous on $X$ (The proof is
    almost identical to the previous one.)
  \end{itemize}
\end{exo}

\begin{exo}{14.5.2}{Prove Theorem 3.5.7.}

  Let $\seq{f^{(n)}}{1}$ be a sequences of functions of
  $C(X \to \rr)$, and suppose that the series (of non-negative real
  numbers) $\sum_{n=1}^{\infty} \ninf{f^{(n)}}$ converges. Let be
  $\epsilon > 0$ arbitrary. By Proposition 7.2.5, there exists a
  $K \geq 1$ such that $\sum_{n=i}^{j} \ninf{f^{(n)}} \leq \epsilon$ for all
  $i,j \geq K$.

  For ${N \geq 1}$, let be $F^{(N)}$ the partial sums defined by
  $F^{(N)}(x) := \sum_{n=1}^{N} f^{(n)}(x)$ for all $x \in
  X$. By Exercise 14.5.1, for all ${N \geq 1}$, the partial sum
  $F^{(N)}$ is itself an element of $C(X \to
  \rr)$. Furthermore, let's show that $(F^{(N)})_{N=1}^\infty$ is a Cauchy
  sequence. For all $x \in X$ and all $p > q \geq K$, we have
  \begin{align*}
    \aval{F^{(p)}(x) - F^{(q)}(x)}
    &= \aval{\sum_{n=1}^{p} f^{(n)}(x) - \sum_{n=1}^{q}f^{(n)}(x)} \\
    &= \aval{\sum_{n=q+1}^{p} f^{(n)}(x)} \\
    &\leq \sum_{n=q+1}^{p} \aval{f^{(n)}(x)} &\text{(Lemma 7.1.4(e))}\\
    &\leq \sum_{n=q+1}^{p} \ninf{f^{(n)}}\\
    &\leq \epsilon.
  \end{align*}
  In particular, we thus have for all $p > q \geq K$,
  \[
    \ninf{F^{(p)} - F^{(q)}} \leq \epsilon.
  \]
  The sequence $(F^{(N)})_{N=1}^\infty$ is thus a Cauchy sequence in
  $C(X \to Y)$. By Theorem 14.4.5, this sequence
  converges to a function $F \in C(X \to Y)$, as expected, and this
  convergence is uniform by Proposition 14.4.4.
\end{exo}
\bigskip

\begin{exo}{14.8.5}{Let $f : \rr \to \rr$ and $g : \rr \to \rr$ be
    continuous, compactly supported functions. Suppose that $f$ is
    supported on the interval $[0, 1]$, and $g$ is constant on the
    interval $[0, 2]$ (i.e., there is a real number $c$ such that
    $g(x) = c$ for all $x \in [0, 2]$). Show that the convolution
    $f * g$ is constant on the interval $[1, 2]$.}

  Since $f$ is supported on $[0,1]$, the function
  $y \mapsto f(y) g(x-y)$ is itself null outside of $[0,1]$ (where
  $x \in \rr$ is a fixed real number). Furthermore, since $g$ is
  constant on $[0,2]$, we have $g(x) = c$ for all $x \in \rr$.

  Now let be $x \in [1,2]$. Note that if $1 \leq x \leq 2$ and
  $0 \leq y \leq 1$, we have $0 \leq x-y \leq 2$, so that
  $g(x-y) = c$. Thus, we have
  \begin{align*}
    f*g(x)
    &:= \int_{-\infty}^{+\infty} f(y) g(x-y) \, dy \\
    &= \int_{0}^{1} f(y) g(x-y) \, dy \\
    &= \int_{0}^{1} f(y) \times c \, dy \\
    &= c \int_{-\infty}^{+\infty} f(y) \, dy.
  \end{align*}
  
  Thus, the function $f*g$ is indeed constant on $[1,2]$.  
\end{exo}
\bigskip

\begin{exo}{14.8.6}{Prove Lemma 14.8.14.}

  \begin{enumerate}[label=(\alph*)]
  \item Let $g$ be an $(\epsilon, \delta)$-approximation of the identity. Thus, by
    definition, $g$ is a positive function on $[-1, 1]$, and in
    particular,
    $0 \leq \int_{[-\delta, \delta]} g \leq \int_{[-1, 1]} g = 1$ (by Theorem 11.4.1 (d,h)).

    Furthermore, we have $g(x) \leq \epsilon$ whenever
    $\delta \leq |x| \leq 1$. So, we have:
    \begin{align*}
      \int_{[-1, 1]} g
      &= \int_{[-1, -\delta]} g + \int_{[-\delta, \delta]} g + \int_{[\delta, 1]} g \\
      &\leq (- \delta + 1) \epsilon + \int_{[-\delta, \delta]} g + (1- \delta) \epsilon \\
      &\leq (2 - 2\delta) \epsilon + \int_{[-\delta, \delta]} g.
    \end{align*}
    And since $\int_{[-1, 1]} g = 1$ by definition, we get
    \[
      \int_{[-\delta, \delta]} g \geq \int_{[-1, 1]} g - (2 - 2\delta) \epsilon \geq 1 - 2 \epsilon.
    \]
    
    Thus, we indeed have the inequalities
    \[
      1 - 2 \epsilon \leq \int_{[-\delta, \delta]} g \leq 1,
    \]
    as expected.
    
  \item First we note that, by Proposition 14.8.11(b), and then by
    Theorem 11.4.1, we have for all $x \in [0,1]$ :
    \begin{align*}
      f*g(x)
      &:= \int_{-\infty}^{+\infty} f(y) g(x-y) \, dy \\
      &= \int_{-\infty}^{+\infty} f(x-y) g(y) \, dy \\
      &= \int_{-1}^{1} f(x-y) g(y) \, dy \\
      &= \underbrace{\int_{[-1, \delta]} f(x-y) g(y) \, dy}_{:= A} +
        \underbrace{\int_{[-\delta, \delta]} f(x-y) g(y) \, dy}_{:= B} +
        \underbrace{\int_{[\delta, 1]} f(x-y) g(y) \, dy}_{:= C}.
    \end{align*}

    First let's show that $B$ is $\epsilon$-close to $f(x)$.

    Since $-\delta \leq y \leq \delta$, we have in particular
    $-\delta \leq x - (x-y) \leq \delta$. Thus, by the initial hypothesis,
    $|f(x) - f(x-y)| \leq \epsilon$, or in other words,
    \begin{equation}
      \label{eq:14.8.6b}
      f(x) - \epsilon \leq f(x-y) \leq f(x) + \epsilon,
    \end{equation}
    for all $y \in [-\delta, \delta]$ and all $x \in [0,1]$.

    Furthermore, by (a), we also know that
    $1 - 2 \epsilon \leq \int_{[-\delta, \delta]} g \leq 1$. Let's use \eqref{eq:14.8.6b} to
    make the integration variable $y$ partly ``vanish'' in $B$:

    {\everymath={\displaystyle}
      \[
        \displaystyle
        \begin{array}{rcccl}
          \int_{[-\delta, \delta]} (f(x)-\epsilon) g(y) \, dy
          &\leq& \int_{[-\delta, \delta]} f(x-y) g(y) \, dy
          &\leq& \int_{[-\delta, \delta]} (f(x)+\epsilon) g(y) \, dy \\
          (f(x)-\epsilon) \int_{[-\delta, \delta]} g(y) \, dy
          &\leq& \int_{[-\delta, \delta]} f(x-y) g(y) \, dy
          &\leq& (f(x)+\epsilon) \int_{[-\delta, \delta]} g(y) \, dy \\
          (f(x)-\epsilon) (1-2 \epsilon)
          &\leq& \int_{[-\delta, \delta]} f(x-y) g(y) \, dy
          &\leq& (f(x)+\epsilon).
        \end{array}
      \]
    }
    
    Now let's show that $|A| \leq M\epsilon$. Recall that for all
    $x \in \rr$, we have ${-M \leq f(x) \leq M}$; and that for all
    $x \in [\delta, 1]$, we have $g(x) \leq \epsilon$. Thus,
    \[- M \epsilon \leq -M(1-\delta) \epsilon \leq -M \int_{[\delta, 1]} g
      \leq \int_{[\delta, 1]} f(x-y) g(y) \, dy
      \leq M \int_{[\delta, 1]} g \leq M(1-\delta) \epsilon \leq M \epsilon, \]
    and in particular, $-M \epsilon \leq A \leq M \epsilon$.
    
    A similar proof would also show that $-M \epsilon \leq C \leq M \epsilon$.

    Thus, since $f*g(x) := A + B + C$, we can combine the three
    previous results about $A, B, C$ to get:
    \[
      (f(x) - \epsilon)(1 - 2 \epsilon) - 2M \epsilon \leq f*g(x) \leq f(x) + \epsilon(1+2M),
    \]
    i.e.,
    \[
      - \epsilon(1+4M) \leq -2 \epsilon f(x) - \epsilon + 2 \epsilon^2 - 2M \epsilon
      \leq f*g(x) - f(x)
      \leq \epsilon(1+2M) \leq \epsilon(1+4M).
    \]
    Thus finally shows Lemma 14.8.14.
  \end{enumerate}
\end{exo}

\begin{exo}{14.8.7}{Prove Corollary 14.8.15.}

  Since $f : \rr \to \rr$ is continuous and supported on $[0,1]$, the
  function $f$ is bounded and uniformly continuous on $\rr$ (Exercise
  14.8.3). So, there exists a real number $M$ such that
  $|f(x)| \leq M$ for all $x \in \rr$.

  Let be $\epsilon > 0$ a positive real number. Let's take
  $\epsilon' := \epsilon/(1+4M)$. By definition, there exists a real number
  $\delta > 0$ such that $|f(x) - f(y)| \leq \epsilon'$ for all real numbers
  $x,y$ such that $|x-y| < \delta$. Let be
  $\delta' := \min(\delta, 1/2)$, so that we have in particular $0 < \delta' < 1$.

  By Lemma 14.8.8, there exists a function $P$ which is a
  $(\epsilon', \delta')$-approximation of the identity, and which is a polynomial
  on $[-1, 1]$.

  Thus, by Lemma 14.8.13, the function $P' := f * P$ is a
  polynomial on $[0,1]$.

  And finally, by Lemma 14.8.14, we have
  $|P'(x) - f(x)| \leq (1+4M) \epsilon' = \epsilon$ for all
  $x \in [0,1]$, which proves Corollary 14.8.15.
\end{exo}
\bigskip

\begin{exo}{14.8.8}{Let $f : [0, 1] \to \rr$ be a continuous function,
    and suppose that $\int_{[0,1]} f(x)x^n \, dx = 0$ for all
    non-negative integers $n$. Show that $f$ must be the zero function
    $f = 0$.}

  Let be $P$ a polynomial on $[0,1]$: by Definition 14.8.1, there
  exists a natural number $n$ such that
  $P(x) := \sum_{j=0}^{n} c_j x^j$ for all $x \in [0,1]$. By linearity of
  the Riemann integral (Theorem 11.4.1), we thus have
  \begin{equation}
    \label{eq:14.8.8}
    \int_{[0,1]} f(x)P(x) \, dx =
    \sum_{j=0}^{n} c_j \int_{[0,1]} f(x) x^j \, dx =
    \sum_{j=0}^{n} c_j \times 0 = 0.
  \end{equation}

  Furthermore, since $[0,1]$ is a compact interval, the function $f$
  is bounded, by the maximum principle. There exists a real number $M$
  such that $|f(x)| \leq M$ for all $x \in [0,1]$.
    
  Let be $\epsilon > 0$ a real number, and let be
  $\epsilon' := \epsilon/M$. By the Weierstrass approximation theorem (Corollary
  14.8.19), there exists a polynomial $P : [0,1] \to \rr$ such that
  $|P(x) - f(x)| \leq \epsilon'$ for all $x \in [0,1]$. Thus, we also have
  \begin{equation*}
    |f(x)| \times |P(x)-f(x)| = |f(x) (P(x)-f(x))| \leq M \times \epsilon' = \epsilon,
  \end{equation*}
  which also implies
  \begin{equation}
    \label{eq:14.8.8b}
    -\epsilon \leq f(x)P(x) - f(x)f(x) \leq \epsilon
  \end{equation}
  for all $x \in [0,1]$.

  So, we have
  \begin{equation*}
    \int_{[0,1]} -\epsilon \, dx
    \leq  \int_{[0,1]} f(x)P(x) - f(x)f(x) \, dx
    \leq  \int_{[0,1]} \epsilon \, dx
  \end{equation*}
  and, by \eqref{eq:14.8.8} and linearity of the Riemann integral, we
  finally get
  \begin{equation}
    \label{eq:14.8.8b}
    -\epsilon \leq \int_{[0,1]} f(x)f(x) \, dx \leq \epsilon.
  \end{equation}

  Since this is true for all $\epsilon > 0$, we can conclude that
  $\int_{[0,1]} f(x) f(x) \, dx = 0$. But since $f^2$ is a continuous and
  non-negative function, we can use Exercise 11.4.2 and conclude that
  $f^2 = 0$. Thus, we indeed have $f = 0$ on $[0,1]$, as expected.
\end{exo}
\pagebreak

\setcounter{section}{16}
\section{Several variables differential calculus}

\begin{exo}{17.1.1}{Prove Lemma 6.1.2}

  Let be $x := \vecn{x}$, $y = \vecn{y}$ and $z =
  \vecn{z}$ three elements of $\rr^n$ ; let be $c,d \in \rr$. We will
  just prove here the first three properties of Lemma 6.1.2.
  
  \begin{itemize}
  \item \textit{Commutativity of addition}. We have:
    \begin{align*}
      x + y &= \vecn{x} + \vecn{y} \\
            &= (x_i + y_i)_{1 \leq i \leq n} \\
            &= (y_i + x_i)_{1 \leq i \leq n}
            &\text{(commutativity of $+$ on $\rr$)}\\
            &= \vecn{y} + \vecn{x}\\
            &= y + x.
    \end{align*}
    
  \item \textit{Associativity of addition}. We have:
    \begin{align*}
      (x+y)+z &= (\vecn{x} + \vecn{y}) + \vecn{z} \\
              &= ((x_i + y_i) + z_i)_{1 \leq i \leq n} \\
              &= (x_i + (y_i + z_i))_{1 \leq i \leq n}
              & \text{(associativity of $+$ on $\rr$)}\\
              &= \vecn{x} + (\vecn{y} + \vecn{z}) \\
              &= x + (y+z).
    \end{align*}
    
  \item \textit{Additive identity}. The part $x+0 = 0+x$ comes
    directly from commutativity of addition proved above; and by
    definition,
    \[
      x + 0 = \vecn{x} + (0)_{1 \leq i \leq n} = (x_i+0)_{1 \leq i \leq n} =
      \vecn{x} = x.\]      
  \end{itemize}

  All other properties can be proved similarly from the algebraic
  properties of real numbers (Proposition 5.3.11).
\end{exo}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
