\documentclass[11pt]{article}
\title{Propositions of solutions for \textit{Analysis II} by Terence Tao}
\author{Frédéric Santos}
% General packages:
\usepackage{a4wide}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{enumitem}
% Fonts and math packages:
\usepackage{lmodern}
\usepackage{amsmath}
\numberwithin{equation}{section}
\usepackage[matha,mathb]{mathabx}
\usepackage{mbboard}
\usepackage{stmaryrd}
\usepackage{hyperref}
% Macros:
\newcommand{\successor}[1]{#1 \! +\!\!\!+}
\newcommand{\aval}[1]{\left\lvert #1 \right\rvert}
\newcommand{\intset}[2]{\llbracket #1, #2 \rrbracket}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\partsof}[1]{\mathcal{P}\left( #1 \right)}
\newcommand{\minus}{\, \textrm{---}\!\textrm{--} \:}
\newcommand{\quot}{\, \textrm{/}\!\textrm{/} \:}
\newcommand{\nn}{\mathbb{N}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\qq}{\mathbb{Q}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\formallimit}[1]{\text{LIM}_{n \to \infty} #1}
\newcommand{\seq}[2]{(#1)_{n=#2}^\infty}
\newcommand{\limit}[1]{\text{lim}_{n \to \infty} #1}
\newcommand{\extrr}{\overline{\rr}}
\newcommand{\adh}[1]{\overline{#1}}
\newcommand{\liminfp}[2]{\inf (#1^+_N)_{N=#2}^{\infty}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\Q}{\mathbf{Q}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\infint}[2]{\underline{\int}_{#2} \, #1}
\newcommand{\supint}[2]{\overline{\int}_{#2} \, #1}
\newcommand{\ddisc}{d_{\text{disc}}}
\newcommand{\inter}{\text{int}}
\newcommand{\ext}{\text{ext}}
% Lemmas:
\usepackage{amsthm}
\newtheorem*{lem}{Lemma}
\newtheorem*{theorem}{Theorem}
% Environment:
\newenvironment{exo}[2]{\noindent \textsc{Exercise #1}. ---
  \textit{#2} \vspace{3mm}}

%%%%%%%%%%%%%%
%%% Begin doc:
\begin{document}
\maketitle
\tableofcontents

\vskip 15mm

\noindent \textbf{Remarks.} The numbering of the Exercises follows the
fourth edition of \textit{Analysis II}. In order to make the
references to \textit{Analysis I} easier, we consider that we begin
with Chapter 12 here, as in earlier editions of the textbook. Thus, in
particular, a reference to ``Exercise 4.3.3'' (for instance) will
always mean ``Exercise 4.3.3 from \textit{Analysis I}''.

\pagebreak
\setcounter{section}{11}
\section{Metric spaces}
\label{sec:metric-spaces}
\begin{exo}{12.1.1}{Prove Lemma 12.1.1}

  Consider the sequence $\seq{a_n}{m}$ defined by
  $a_n := d(x_n, x) = |x_n - x|$ for all $n \geq m$. We have to prove
  that $\lim_{n \to \infty} a_n = 0$ if and only if
  $\lim_{n \to \infty} x_n = x$.

  \begin{itemize}
  \item Let be $\epsilon > 0$. If $\lim_{n \to \infty} a_n = 0$, then
    there exists an $N \geq m$ such that $|a_n| < \epsilon$ whenever
    $n \geq N$. Thus, there exists an $N \geq m$ such that
    $|x_n - x| < \epsilon$ whenever $n \geq N$, which means that
    $\lim_{n \to \infty} x_n = x$.
  \item Let be $\epsilon > 0$. Conversely, if $\lim_{n \to \infty} x_n
    = x$, then there exists an $N \geq m$ such that $|x_n - x| <
    \epsilon$ whenever $n \geq N$. But since $|a_n| := |x_n - x|$, it
    means that $\lim_{n \to \infty} a_n = 0$, as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.2}{Show that the real line with the metric $d(x, y) :=
    |x-y|$ is indeed a metric space.}

  Using Proposition 4.3.3, this claim is obvious. All claims (a)--(d)
  of Definition 12.1.2 are satisfied because:
  \begin{enumerate}[label=(\alph*)]
  \item comes from Proposition 4.3.3(e)
  \item also comes from Proposition 4.3.3(e)
  \item comes from Proposition 4.3.3(f)
  \item comes from Proposition 4.3.3(g).
  \end{enumerate}
\end{exo}

\begin{exo}{12.1.3}{Let $X$ be a set, and let
    $d : X \times X \to [0, \infty)$ be a function. With respect to
    Definition 12.1.2, give an example of a pair $(X,d)$ which...
    \vskip -4mm}
  
  \begin{enumerate}[label=(\alph*)]
  \item obeys the axioms (bcd) but not (a).

    Consider $X = \rr$, and $d$ defined by $d(x,x) = 1$ and $d(x,y) =
    5$ for all $x \neq y \in \rr$.
  \item obeys the axioms (acd) but not (b).

    Consider $X = \rr$, and $d$ defined by $d(x,y) = 0$ for all $x, y
    \in \rr$.
  \item obeys the axioms (abd) but not (c).

    Consider $X = \rr$, and $d$ defined by $d(x,y) = \max(x-y,0)$ for
    all $x,y \in \rr$.
  \item obeys the axioms (abc) but not (d).

    Consider the finite set $X := \{1,2,3\}$ and the application $d$
    defined by $d(1,2) = d(2,1) = d(2,3) = d(3,2) := 1$, and
    $d(1,3) = d(3,1) := 5$, and $d(x,x) = 0$ for all $x \in X$.
  \end{enumerate}  
\end{exo}

\begin{exo}{12.1.4}{Show that the pair $(Y, d|_{Y \times Y})$ deﬁned
    in Example 12.1.5 is indeed a metric space.}

  By definition, since $Y \subseteq X$, we have $x,y \in X$ whenever
  $x,y \in Y$. And furthermore, since $d|_{Y \times Y}(x,y) :=
  d(x,y)$, then the application $d|_{Y \times Y}$ obeys all four
  statements (a)--(d) of Definition 12.1.2. Thus, $(Y, d|_{Y \times
    Y})$ is indeed a metric space.
\end{exo}

\begin{exo}{12.1.5}{Let $n \geq 1$, and let $a_1, a_2, \ldots, a_n$
    and $b_1, b_2, \ldots, b_n$ be real numbers. Verify the identity
    $\left(\sum_{i=1}^n a_i b_i\right)^2 + \frac{1}{2} \sum_{i=1}^{n}
    \sum_{j=1}^{n} (a_i b_j - a_j b_i)^2 = \sum_{i=1}^{n} a_i^2
    \sum_{j=1}^{n} b_j^2$, and conclude the Cauchy-Schwarz inequality.
    Then use the Cauchy-Schwarz inequality to prove the triangle
    inequality.}

  Let's prove these three statements.

  \begin{enumerate}[label=(\roman*)]
  \item To prove the first identity, let's use induction on $n$.

    The base case $n=1$ is obvious: on the left-hand side, we just get
    $(a_1 b_1)^2$, and on the right-hand side, we get $a_1^2 b_1^2$,
    hence the statement.

    Now let's suppose inductively that this identity is true for a
    given positive integer $n \geq 1$, and let's prove that it is
    still true for $n+1$. We have to prove that
    \begin{equation}
      \label{eq:12.1.5a}
      \underbrace{\left(\sum_{i=1}^{n+1} a_i b_i\right)^2}_{:=A} +
      \underbrace{\frac{1}{2} \sum_{i=1}^{n+1} \sum_{j=1}^{n+1} (a_i
        b_j - a_j b_i)^2}_{:= B}
      = \underbrace{\left(\sum_{i=1}^{n+1} a_i^2\right)
        \left(\sum_{j=1}^{n+1} b_j^2\right)}_{:=C}
    \end{equation}
    where we gave a name to each part of the identity for an easier
    computation below. Indeed,
    \begin{itemize}
    \item for $A$, we have
      \begin{align*}
        A &:= \left(\sum_{i=1}^{n+1} a_i b_i\right)^2 \\
          &= \left(a_{n+1} b_{n+1} + \sum_{i=1}^{n} a_i b_i\right)^2
        \\
          &= (a_{n+1} b_{n+1})^2 + \left(\sum_{i=1}^{n} a_i
            b_i\right)^2 + 2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i
      \end{align*}
      
    \item for $B$, we have
      \begin{align*}
        B &:= \frac{1}{2} \sum_{i=1}^{n+1} \sum_{j=1}^{n+1} (a_i
            b_j - a_j b_i)^2 \\
          &= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n+1} (a_i b_j -
            a_j b_i)^2 + \frac{1}{2} \sum_{j=1}^{n+1} (a_{n+1} b_j -
            a_j b_{n+1})^2\\
          &= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
            b_i)^2 +
            \underbrace{\frac{1}{2} \sum_{i=1}^{n} (a_i b_{n+1} -
            a_{n+1} b_i)^2}_{:= 1/2 \times S} +
            \underbrace{\frac{1}{2} \sum_{j=1}^{n} (a_{n+1} b_{j} - a_{j}
            b_{n+1})^2}_{:= 1/2 \times S} \\
          &\quad + \underbrace{\frac{1}{2} (a_{n+1}b_{n+1} -
            b_{n+1}a_{n+1})^2}_{=0} \\
          &= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
            b_i)^2 + \sum_{k=1}^{n} (a_kb_{n+1} -
            a_{n+1}b_k)^2
      \end{align*}
      
    \item and thus, for $A + B$, we now use the induction hypothesis (IH)
      to get:
      \begin{align*}
        A + B
        &:= (a_{n+1} b_{n+1})^2 + \left(\sum_{i=1}^{n} a_i
          b_i\right)^2 +
          2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i\\
        &\quad + \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
          b_i)^2 + \sum_{k=1}^{n} (a_kb_{n+1} - a_{n+1}b_k)^2\\
        &= \underbrace{\left(\sum_{i=1}^{n} a_i b_i\right)^2 +
          \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} (a_i b_j - a_j
          b_i)^2}_{\text{apply (IH) here}} \\
        &\quad + (a_{n+1} b_{n+1})^2 +
          2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i
          + \sum_{k=1}^{n} (a_kb_{n+1} - a_{n+1}b_k)^2\\
        &= \left(\sum_{i=1}^{n} a_i^2\right) \left(\sum_{j=1}^{n}
          b_j^2\right)\\
        &\quad + (a_{n+1} b_{n+1})^2 +
          2 (a_{n+1} b_{n+1}) \sum_{i=1}^{n} a_i b_i
          + \sum_{k=1}^{n} (a_kb_{n+1} - a_{n+1}b_k)^2\\
        &= \left(\sum_{i=1}^{n} a_i^2\right) \left(\sum_{j=1}^{n}
          b_j^2\right) + (a_{n+1} b_{n+1})^2\\
        &\quad + 2 \sum_{i=1}^{n} a_i a_{n+1} b_i b_{n+1} +
          \sum_{i=1}^{n}(a_i^2 b_{n+1}^2 - 2a_ib_{n+1}a_{n+1}b_i + a_{n+1}^2b_i^2)\\
        &= \left(\sum_{i=1}^{n} a_i^2\right) \left(\sum_{j=1}^{n}
          b_j^2\right) + \sum_{i=1}^{n}(a_i^2 b_{n+1}^2 + a_{n+1}^2
          b_i^2) \\
        &= \left(\sum_{i=1}^{n+1} a_i^2\right) \left(\sum_{j=1}^{n+1}
          b_j^2\right) \\
        &= C
      \end{align*}
      so that the identity is indeed true for all natural number $n$.
    \end{itemize}
    
  \item We can use this identity to prove the Cauchy-Schwarz identity,
    \begin{equation}
      \label{eq:12.1.5b}
      \left| \sum_{i=1}^{n} a_i b_i\right| \leq \left( \sum_{i=1}^{n}
        a_i^2 \right)^{1/2} \left( \sum_{i=1}^{n}
        b_i^2 \right)^{1/2}.
    \end{equation}

    Indeed, since $B \geq 0$ in the identity \eqref{eq:12.1.5a}, we
    have
    \[\left(\sum_{i=1}^{n} a_i b_i\right)^2 \leq \left(\sum_{i=1}^{n} a_i^2\right)
      \left(\sum_{j=1}^{n} b_j^2\right)\]
    and thus, taking the square root on both sides, we get
    \eqref{eq:12.1.5b}, as expected.
    
  \item Finally, we can use the Cauchy-Schwarz inequality to prove the
    triangle inequality.

    We have
    \begin{align*}
      \sum_{i=1}^{n} (a_i^2 + b_i^2)
      &= \sum_{i=1}^{n} a_i^2 + \sum_{i=1}^{n} b_i^2 + 2
        \sum_{i=1}^{n} a_i b_i
      &\\
      &\leq \sum_{i=1}^{n} a_i^2 + \sum_{i=1}^{n} b_i^2 + 2
        \left(\sum_{i=1}^{n} a_i^2\right)^{1/2} \left(\sum_{i=1}^{n}
        b_i^2\right)^{1/2}
      &\text{ (by eq. \eqref{eq:12.1.5b})}\\
      &\leq \left( \left( \sum_{i=1}^{n} a_i^2 \right)^{1/2}
        + \left( \sum_{i=1}^{n} b_i^2 \right)^{1/2}\right)^2&
    \end{align*}
    and, since everything is positive, we get the triangle inequality
    by taking square roots on both sides.
  \end{enumerate}  
\end{exo}

\begin{exo}{12.1.6}{Show that $(\rr^n , d_{l^2}$) in Example 12.1.6 is
    indeed a metric space.}

  We have to show the four axioms of Definition 12.1.2.

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in \rr^n$, we have
    $d_{l^2}(x,x) = \sqrt{\sum_{i=1}^{n} (x_i - x_i)^2} = 0$, as expected.
  \item Positivity: for all $x \neq y \in \rr^n$, there exists at
    least one $1 \leq i \leq n$ such that $x_i \neq y_i$, so that
    $(x_i-y_i)^2 > 0$, and
    $d_{l^2}(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} > 0$, as
    expected.
  \item Symmetry: for all $x,y \in \rr^n$, we have
    \[d_{l^2}(y,x) = \sqrt{\sum_{i=1}^{n} (y_i - x_i)^2} =
      \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} = d_{l^2}(x,y)\] as expected.
  \item Triangle inequality: for all $x,y,z \in \rr^n$, we have
    \begin{align*}
      d_{l^2}(x,z)
      &:= \left(\sum_{i=1}^{n} (x_i - z_i)^2\right)^{1/2} &\\
      &= \left(\sum_{i=1}^{n} (a_i + b_i)^2\right)^{1/2}
      &\text{with $a_i := x_i-y_i$ and $b_i := y_i-z_i$}\\
      &\leq \left(\sum_{i=1}^{n} a_i^2\right)^{1/2} +
        \left(\sum_{i=1}^{n} b_i^2\right)^{1/2}
      &\text{(Exercise 12.1.5(iii))}\\
      &\leq \left(\sum_{i=1}^{n} (x_i - y_i)^2\right)^{1/2} +
        \left(\sum_{i=1}^{n} (y_i-z_i)^2\right)^{1/2}& \\
      &\leq d_{l^2}(x,y) + d_{l^2}(y,z)&
    \end{align*}
    as expected.
  \end{enumerate}
  Thus, $(\rr^n, d_{l^2})$ is indeed a metric space.
\end{exo}

\pagebreak
\begin{exo}{12.1.7}{Show that $(\rr^n , d_{l^1}$) in Example 12.1.7 is
    indeed a metric space.}

  Once again, let's show the four axioms of Definition 12.1.2.

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in \rr^n$, we have
    $d_{l^1}(x,x) = \sum_{i=1}^{n} |x_i - x_i| = 0$, as expected.
  \item Positivity: for all $x \neq y \in \rr^n$, there exists at
    least one $1 \leq i \leq n$ such that $x_i \neq y_i$, so that
    $|x_i-y_i| > 0$, and
    $d_{l^1}(x,y) = \sum_{i=1}^{n} |x_i - y_i| > 0$, as
    expected.
  \item Symmetry: for all $x,y \in \rr^n$, we have
    \[d_{l^1}(y,x) = \sum_{i=1}^{n} |y_i - x_i| =
      \sum_{i=1}^{n} |x_i - y_i| = d_{l^1}(x,y)\]
    as expected.
  \item Triangle inequality: we already know from Proposition 4.3.3(g)
    (generalized to real numbers) that we have the triangle inequality
    $|a-c| \leq |a-b| + |b-c|$ for all $a,b,c \in \rr$. Thus, for all
    $x,y,z \in \rr^n$, we have
    \begin{equation*}
      d_{l^1}(x,z) := \sum_{i=1}^{n} |x_i - z_i| \leq \sum_{i=1}^{n}
      (|x_i - y_i| + |y_i - z_i|) =: d_{l^1}(x,y) + d_{l^1}(y,z)
    \end{equation*}
    as expected.
  \end{enumerate}
  Thus, $(\rr^n, d_{l^1})$ is indeed a metric space.
\end{exo}

\bigskip
\begin{exo}{12.1.8}{Prove the two inequalities in equation (12.1).}

  We have to prove that for all $x,y \in \rr^n$, we have
  \begin{equation}
    \label{eq:12.1.8goal}
    d_{l^2}(x,y) \leq d_{l^1}(x,y) \leq \sqrt{n} \, d_{l^2}(x,y)
  \end{equation}

  \begin{itemize}
  \item The first inequality, since everything is non-negative, is
    equivalent to $d_{l^2}(x,y)^2 \leq d_{l^1}(x,y)^2$, and we will prove
    it in this form.

    Indeed, using a trivial product expansion, we have
    \begin{align*}
      d_{l_1}(x,y)^2
      &:= \left(\sum_{i=1}^{n} |x_i - y_i|\right)^2 \\
      &= \left(\sum_{i=1}^{n} |x_i - y_i|\right) \times
        \left(\sum_{i=1}^{n} |x_i - y_i|\right) \\
      &= \sum_{i=1}^{n} |x_i - y_i|^2 + \overbrace{\sum_{1 \leq i,j \leq n ; \,
        i\neq j} |x_i-y_i| \times |x_j - y_j|}^{\geq 0} \\
      &\geq \sum_{i=1}^{n} |x_i - y_i|^2 =: d_{l^2}(x,y)^2
    \end{align*}
    as expected.
  \item For the second inequality, we use the Cauchy-Schwarz
    inequality, which says that
    \begin{align*}
      d_{l^1}(x,y) &:= \sum_{i=1}^{n} |x_i - y_i|\\
      &= \left| \sum_{i=1}^{n} |x_i - y_i| \times 1 \right|\\
      &\leq \left(\sum_{i=1}^{n} |x_i - y_i|^2 \right)^{1/2}
        \left(\sum_{i=1}^{n} 1^2 \right)^{1/2} \\
      &\leq d_{l^2}(x,y) \times \sqrt{n}
    \end{align*}
    as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.9}{Show that the pair $(\rr^n, d_{l^\infty})$ in
    Example 12.1.9 is a metric space.}

  Once again, let's show the four axioms of Definition 12.1.2. 

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in \rr^n$, we clearly have
    $d_{l^\infty}(x,x) = \sup \{|x_i - x_i| : 1 \leq i \leq n\} = 0$,
    as expected.
  \item Positivity: for all $x \neq y \in \rr^n$, there exists at
    least one $1 \leq j \leq n$ such that $x_j \neq y_j$. Thus
    $|x_j-y_j| > 0$, and
    $d_{l^\infty}(x,y) = \sup \{|x_i - y_i| : 1 \leq i \leq n\} \geq
    |x_j - y_j| > 0$, as expected.
  \item Symmetry: for all $x,y \in \rr^n$, we have
    \[d_{l^\infty}(x, y) = \sup \{|x_i - y_i| : 1 \leq i \leq n\} =
      \sup \{|y_i - x_i| : 1 \leq i \leq n\} = d_{l^\infty}(y, x)\]
    as expected.
  \item Triangle inequality. Let be $x,y,z \in \rr^n$. We have
    $|x_i - z_i| \leq |x_i - y_i| + |y_i - z_i|$ for all
    $1 \leq i \leq n$, by Proposition 4.3.3(g). But, by definition of
    the supremum, we have $|x_i - y_i| \leq d_{l^\infty}(x,y)$
    and $|y_i - z_i| \leq d_{l^\infty}(y,z)$ for all $1 \leq i \leq
    n$. Thus, we have $|x_i - z_i| \leq d_{l^\infty}(x,y) +
    d_{l^\infty}(y,z)$ for all $1 \leq i \leq n$; i.e., $d_{l^\infty}(x,y) +
    d_{l^\infty}(y,z)$ is an upper bound of the set $\{|x_i -
    z_i| : 1 \leq i \leq n\}$. By definition of the supremum, it
    implies that
    \[d_{l^\infty}(x,z) := \sup \{|x_i - z_i| : 1 \leq i \leq n\} \leq d_{l^\infty}(x,y) +
      d_{l^\infty}(y,z)\]
    as expected.
  \end{enumerate}
  Thus, $(\rr^n, d_{l^1})$ is indeed a metric space.  
\end{exo}

\bigskip
\begin{exo}{12.1.10}{Prove the two inequalities in equation (12.2).}

  We have to prove that for all $x,y \in \rr^n$,
  \[ \frac{1}{\sqrt{n}} d_{l^2}(x,y) \leq d_{l^\infty} (x,y) \leq
    d_{l^2}(x,y).\]

  First, a preliminary remark. By definition, we have
  $d_{l^\infty}(x,y) := \sup \{|x_i - y_i| : 1 \leq i \leq n\}$ for
  all $x,y \in \rr^n$. Since this distance is defined as the supremum
  of a finite set, we know (see Chapter 8 of \textit{Analysis I}) that
  there exists a $1 \leq m \leq n$ such that
  $d_{l^\infty}(x,y) = |x_m - y_m|$ (the supremum belongs to the set).
  The index ``$m$'' will have this meaning below.

  \begin{itemize}
  \item Let's prove the first inequality.
    \begin{align*}
      \frac{1}{\sqrt{n}} d_{l^2}(x,y)
      &:= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - y_i)^2} \\
      &\leq \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_m - y_m)^2} \\
      &\leq \sqrt{\frac{n}{n} (x_m - y_m)^2} \\
      &= |x_m - y_m| =: d_{l^\infty} (x,y)
    \end{align*}
    as expected.
  \item Now we prove the second one. We have
    \begin{align*}
      d_{l^2}(x,y) &:= \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} \\
                   &= \sqrt{(x_m - y_m)^2 + \sum_{1 \leq i \leq n ; \, i
                     \neq m} (x_i - y_i)^2}\\
                   &\geq \sqrt{(x_m - y_m)^2} = |x_m - y_m| =: d_{l^\infty}(x,y)
    \end{align*}
    as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.11}{Show that the discrete metric $(X, \ddisc)$ in
    Example 12.1.11 is indeed a metric space.}

  Once again, let's show the four axioms of Definition 12.1.2. 

  \begin{enumerate}[label=(\alph*)]
  \item For all $x \in X$, we have
    $\ddisc(x,x) := 0$ by definition, so that there is nothing to
    prove here.
  \item Positivity: for all $x \neq y \in X$, we have
    $\ddisc(x,y) := 1 > 0$ by definition, so that there's still
    nothing to prove.
  \item Symmetry: for all $x,y \in X$, we have $\ddisc(x,y) =
    \ddisc(y,x) = 1$, so that $\ddisc$ obeys the symmetry property.
  \item Triangle inequality. Let be $x,y,z \in X$, and let's consider
    $\ddisc(x,z)$.
    \begin{itemize}
    \item If $x=z$, then $\ddisc(x,z) = 0$. And since $\ddisc$ is a
      non-negative application, we clearly have $0 =: \ddisc(x,z) \leq
      \ddisc(x,y) + \ddisc(y,z)$ for all $y \in X$.
    \item If $x \neq z$, then we cannot have both $x=y$ and $y=z$ (it
      would be a clear contradiction with $x \neq z$). Thus, at least
      one of the propositions ``$x \neq y$'', ``$y \neq z$'' is true.
      Another way to say that is $\ddisc(x,y) + \ddisc(y,z) \geq 1$.
      But since $\ddisc(x,z) := 1$, we have actually $\ddisc(x,y) +
      \ddisc(y,z) \geq \ddisc(x,z)$, as expected.
    \end{itemize}
  \end{enumerate}
\end{exo}

\begin{exo}{12.1.12}{Prove Proposition 12.1.18.}

  First, recall that for all $x,y \in \rr^n$, we have, from Examples
  12.1.7 and 12.1.9,
  \begin{equation*}
    \frac{1}{\sqrt{n}} \, d_{l^2} (x,y) \leq
    d_{l^\infty}(x,y) \leq d_{l^2}(x,y) \leq
    d_{l^1}(x,y) \leq \sqrt{n} \, d_{l^2}(x,y).
  \end{equation*}

  Note that $n$ is a real constant here.
  
  \begin{itemize}
  \item Let's prove that $(a) \implies (b)$. If
    $\lim_{k \to \infty} d_{l^2}(x^{(k)}, x) = 0$, then by the limit
    laws, the sequence $t_k := \sqrt{n} \, d_{l^2}(x^{(k)}, x)$ also
    converges to $0$ as $k \to \infty$, since $\sqrt{n}$ is a constant
    real number. Thus, we have
    \[d_{l^2}(x^{(k)}, x) \leq d_{l^1}(x^{(k)}, x) \leq \sqrt{n} \,
      d_{l^2}(x^{(k)}, x)\] and, by the squeeze test, this implies
    that $\lim_{k \to \infty} d_{l^1}(x^{(k)}, x)$ as expected.
  \item Let's prove that $(b) \implies (c)$. If
    $\lim_{k \to \infty} d_{l^1}(x^{(k)}, x) = 0$, then we have
    \[0 \leq d_{l^\infty}(x^{(k)}, x) \leq d_{l^1}(x^{(k)}, x)\]
    and, by the squeeze test, this implies
    that $\lim_{k \to \infty} d_{l^\infty}(x^{(k)}, x)$ as expected.
  \item Let's prove that $(c) \implies (d)$. Suppose that
    $\lim_{k \to \infty} d_{l^\infty}(x^{(k)}, x) = 0$. Then, for all
    $1 \leq j \leq n$, we have
    $0 \leq |x_j^{k} - x_j| \leq d_{l^\infty}(x^{(k)}, x)$. Still by
    the squeeze test, this implies that
    $\lim_{k \to \infty} |x_j^{k} - x_j| = 0$, i.e. that
    $(x_j^{k})_{k=m}^\infty$ converges to $x_j$ as $k \to \infty$ (by
    Lemma 12.1.1), as expected.
  \item Finally, let's prove that $(d) \implies (a)$. Using the
    definition of convergence is more appropriate here. Let be
    $\epsilon > 0$ a positive real number, and let be
    $1 \leq j \leq n$. By definition, there exists a natural number
    $N \geq m$ such that $|x_j^{(k)} - x_j| \leq \epsilon / \sqrt{n}$
    whenever $k \geq N$. Thus, if $k \geq N$, we have
    \[d_{l^2}(x^{(k)}, x) := \sqrt{\sum_{j=1}^{n} (x^{(k)}_j - x_j)^2}
      \leq \sqrt{\sum_{j=1}^{n} \frac{\epsilon^2}{n}} \leq \epsilon\]
    so that $\lim_{k \to \infty} d_{l^2}(x^{(k)}, x) = 0$, i.e.,
    $(x^{k})_{k=m}^\infty$ converges to $x$ as $k \to \infty$ in the
    $l^2$ metric (by Lemma 12.1.1), as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.13}{Prove Proposition 12.1.19.}

  Let be $\seq{x^{(n)}}{m}$ a sequence of elements of a set $X$.

  \begin{itemize}
  \item First suppose that $\seq{x^{(n)}}{m}$ is eventually constant.
    Thus, by definition, there exists an $N \geq m$ and an element
    $x \in X$ such that $\seq{x^{(n)}}{m} = x$ for all $n \geq N$.
    This implies that we have $\ddisc(x^{(n)}, x) = 0$ for all $n \geq
    N$. In particular, for all $\epsilon > 0$, we have
    $\ddisc(x^{(n)}, x) \leq \epsilon$ whenever $n \geq N$, so that
    $\seq{x^{(n)}}{m}$ indeed converges to $x$ with respect to
    $\ddisc$.
  \item Conversely, suppose that $\seq{x^{(n)}}{m}$ converges to $x$
    with respect to $\ddisc$. Let be $\epsilon = 1/2$. By definition,
    there exists an $N \geq m$ such that $\ddisc(x^{(n)}, x) \leq 1/2$
    whenever $n \geq N$. Since $\ddisc(x^{(n)}, x)$ cannot be $1$, it
    is necessarily equal to $0$, so that $x^{(n)} = x$ whenever
    $n \geq N$. Thus, the sequence $x^{(n)}$ is indeed eventually
    constant.
  \end{itemize}
\end{exo}

\begin{exo}{12.1.14}{Prove Proposition 12.1.20.}

  Suppose that we have $\lim_{n \to \infty} d(x^{(n)}, x) = 0$ and
  $\lim_{n \to \infty} d(x^{(n)}, x') = 0$. Suppose, for the sake
  of contradiction, that we have $x \neq x'$. Thus, the real number
  $\epsilon := \frac{d(x,x')}{3}$ is positive.

  Since $x^{(n)}$ converges to $x$, there exists a $N_1 \geq m$ such
  that $d(x^{(n)}, x) \leq \epsilon$ whenever $n \geq N_1$.

  Similarly, since $x^{(n)}$ converges to $x'$, there exists a
  $N_2 \geq m$ such that $d(x^{(n)}, x') \leq \epsilon$ whenever
  $n \geq N_2$.

  By the triangle inequality, we thus have, for all $n \geq \max(N_1,
  N_2)$,
  \[d(x, x') \leq d(x, x^{(n)}) + d(x^{(n)}, x') \leq \epsilon +
    \epsilon = \frac{2}{3}d(x,x')\]
  which is a contradiction (since $d(x,x') > 0$ by hypothesis).

  Thus, the limit is unique, and we must have $x=x'$.
\end{exo}

\bigskip
\begin{exo}{12.1.15}{Let be
    $X := \{\seq{a_n}{0} : \sum_{n=0}^{\infty} |a_n| < \infty\}$. We
    define on this space the metrics
    $d_{l^1}(\seq{a_n}{0}, \seq{b_n}{0}) := \sum_{n=0}^{\infty} |a_n -
    b_n|$, and $d_{l^\infty}(\seq{a_n}{0}, \seq{b_n}{0}) := \sup_{n \in
      \nn} |a_n - b_n|$. Then...}

  We have to prove the following statements.

  \begin{enumerate}
  \item $d_{l^1}$ is a metric on $X$.

    We have to prove the four axioms of Definition 12.1.2.

    \begin{enumerate}[label=(\alph*)]
    \item Let be $\seq{a_n}{0} \in X$. We have $d_{l^1}(\seq{a_n}{0},
      \seq{a_n}{0}) = \sum_{n=0}^{\infty} |a_n - a_n| = 0$, as
      expected.
    \item Let be $\seq{a_n}{0}, \seq{b_n}{0}$ two distinct elements of
      $X$. Since they are distinct, there exists at least one $m \in
      \nn$ such as $|a_m - b_m| > 0$. Thus, $d_{l^1}(\seq{a_n}{0},
      \seq{b_n}{0}) = \sum_{n=0}^{\infty} |a_n - b_n| \geq |a_m - b_m|
      > 0$, as expected.
    \item Symmetry: we clearly have
      \[d_{l^1}(\seq{b_n}{0}, \seq{a_n}{0}) = \sum_{n=0}^{\infty}
        |b_n - a_n| = \sum_{n=0}^{\infty} |a_n - b_n| =
        d_{l^1}(\seq{a_n}{0}, \seq{b_n}{0}).\]
    \item Finally, let's prove the triangle inequality. Let be
      $\seq{a_n}{0}, \seq{b_n}{0}, \seq{c_n}{0} \in X$. Since we have
      the triangle inequality for the usual distance $d$ on $\rr$
      (i.e., we have $|a_n - c_n| \leq |a_n - b_n| + |b_n - c_n|$ for
      all $n \in \nn$), we have immediately
      \begin{align*}
        d_{l^1}(\seq{a_n}{0}, \seq{c_n}{0})
        &:= \sum_{n=0}^{\infty} |a_n - c_n|\\
        &\leq \sum_{n=0}^{\infty} (|a_n - b_n| + |b_n - c_n|)
          \; \text{ (consequence of Prop. 7.1.11(h))}\\
        &\leq \sum_{n=0}^{\infty} |a_n - b_n| + \sum_{n=0}^{\infty}
          |b_n - c_n|
          \; \text{ (by Proposition 7.2.14(a))}\\
        &\leq d_{l^1}(\seq{a_n}{0}, \seq{b_n}{0}) +
          d_{l^1}(\seq{b_n}{0}, \seq{c_n}{0}).
      \end{align*}
    \end{enumerate}

    Thus, $d_{l^1}$ is indeed a metric on $X$.
  \item $d_{l^\infty}$ is a metric on $X$.

    Once again, we have to prove the four axioms of Definition 12.1.2.

    \begin{enumerate}[label=(\alph*)]
    \item Let be $\seq{a_n}{0} \in X$. We have
      $d_{l^\infty}(\seq{a_n}{0}, \seq{a_n}{0}) = \sup_{n \in \nn}
      |a_n - a_n| = 0$, as expected.
    \item Let be $\seq{a_n}{0}, \seq{b_n}{0}$ two distinct elements of
      $X$. Since they are distinct, there exists at least one $m \in
      \nn$ such as $|a_m - b_m| > 0$. Thus, $d_{l^\infty}(\seq{a_n}{0},
      \seq{b_n}{0}) = \sup_{n \in \nn} |a_n - b_n| \geq |a_m - b_m|
      > 0$, as expected.
    \item Symmetry: we clearly have
      \[d_{l^\infty}(\seq{b_n}{0}, \seq{a_n}{0}) = \sup_{n \in \nn}
        |b_n - a_n| = \sup_{n \in \nn} |a_n - b_n| =
        d_{l^\infty}(\seq{a_n}{0}, \seq{b_n}{0}).\]
    \item Finally, let's prove the triangle inequality. Let be
      $\seq{a_n}{0}, \seq{b_n}{0}, \seq{c_n}{0} \in X$. Since we have
      the triangle inequality for the usual distance $d$ on $\rr$
      (i.e., we have $|a_n - c_n| \leq |a_n - b_n| + |b_n - c_n|$ for
      all $n \in \nn$), we have immediately
      $|a_m - c_m| \leq \sup_{n \in \nn} |a_n - b_n| + \sup_{n \in
        \nn} |b_n - c_n|$ for all $m \in \nn$, by definition of the
      supremum. In other words,
      $(\sup_{n \in \nn} |a_n - b_n| + \sup_{n \in \nn} |b_n - c_n|)$
      is an upper bound for the set $\{|a_m - c_m| : m \in \nn\}$.
      Thus we have, still by definition of the supremum,
      $\sup_{n \in \nn} |a_n - c_n| \leq \sup_{n \in \nn} |a_n - b_n| +
      \sup_{n \in \nn} |b_n - c_n|$, as expected.
    \end{enumerate}
    Thus, $d_{l^\infty}$ is indeed a metric on $X$.
  \item There exist sequences $x^{(1)}$, $x^{(2)}$, ..., of elements
    of $X$ (i.e., sequences of sequences) which are convergent with
    respect to $d_{l^\infty}$, but are not convergent with respect to
    $d_{l^1}$.

    Here we are dealing with sequences of sequences: we have a
    sequence $(x^{(k)})_{k=1}^\infty$ where each $x^{(k)}$ is
    itself a sequence of real numbers. Thus, let's define
    $(x^{(k)})_{k=1}^\infty$ as follows:
    \[x^{(k)}_n := \left\{
        \begin{array}{ll}
          1/(k+1) & \text{ if } 0 \leq n \leq k \\
          0 & \text{ if } n > k.
        \end{array}
      \right.\]
    Just to make things clearer, we have for instance
    \begin{align*}
      x^{(1)} &:= \frac{1}{2}, \; \frac{1}{2}, \; 0, \; 0, \; 0, \; \ldots \\
      x^{(2)} &:= \frac{1}{3}, \; \frac{1}{3}, \; \frac{1}{3}, \; 0, \; 0, \; \ldots \\
      x^{(3)} &:= \frac{1}{4}, \; \frac{1}{4}, \; \frac{1}{4}, \; \frac{1}{4}, \; 0, \; \ldots
    \end{align*}
    Also, let be the null sequence $\seq{a_n}{0}$ defined by $a_n := 0$ for
    all $n \in \nn$. Thus:
    \begin{itemize}
    \item $(x^{(k)})_{k=1}^\infty$ converges to $\seq{a_n}{0}$ w.r.t.
      the metric $d_{l^\infty}$. Indeed, if we consider a given
      positive integer $k$ (fixed), we have
      \begin{equation*}
        |x^{(k)} - a_n| = |x^{(k)}| = \left\{
          \begin{array}{ll}
            1/(k+1) & \text{ if } 0 \leq n \leq k \\
            0 & \text{ if } n > k.
          \end{array}
        \right.
      \end{equation*}
      so that $d_{l^\infty}\left(\seq{x^{(k)}_n}{0},
        \seq{a_n}{0}\right) := \sup_{n \in \nn} |x^{(k)} - a_n| =
      \frac{1}{k+1}$.

      Thus,
      $\lim_{k \to \infty} d_{l^\infty}\left(\seq{x^{(k)}_n}{0},
        \seq{a_n}{0}\right) = 0$, or in other words,
      $(x^{(k)})_{k=1}^\infty$ converges to $\seq{a_n}{0}$ w.r.t. the
      metric $d_{l^\infty}$ in $X$.
      
    \item But $(x^{(k)})_{k=1}^\infty$ does not converges to
      $\seq{a_n}{0}$ w.r.t. the metric $d_{l^1}$. Indeed, we have, for
      each given (fixed) $k$,
      \begin{align*}
        d_{l^1} \left(\seq{x^{(k)}_n}{0}, \seq{a_n}{0}\right)
        = \sum_{n=0}^{k} \frac{1}{k+1} = 1
      \end{align*}
      Thus, we clearly do not have
      $\lim_{k \to \infty} d_{l^1}\left(\seq{x^{(k)}_n}{0},
        \seq{a_n}{0}\right) = 0$, i.e., $(x^{(k)})_{k=1}^\infty$ does
      not converge to $\seq{a_n}{0}$ w.r.t. the  metric $d_{l^1}$.
    \end{itemize}
  \item Conversely, any sequence which converges with respect to
    $d_{l^1}$ also converges with respect to $d_{l^\infty}$.

    Suppose, for the sake of contradiction, that
    $(x^{(k)})_{k=1}^\infty$ does not converge to $\seq{a_n}{0}$
    w.r.t. the  metric $d_{l^\infty}$, but does converge to $\seq{a_n}{0}$
    w.r.t. the  metric $d_{l^1}$.

    In this case, there exists a $\epsilon > 0$ such that, for all
    $k \geq 1$, we have
    $(\sup_{n \geq 0} |x^{(k)}_n - a_n|) > \epsilon$. In particulier,
    for all $k \geq 1$ and all $n \geq 0$, we have
    $|x^{(k)}_n - a_n| > \epsilon$. Thus,
    $\sum_{n=0}^{\infty} |x^{(k)}_n - a_n|$ is not even a convergent
    series, and we cannot have
    $\lim_{k \to \infty} \left(\sum_{n=0}^{\infty} |x^{(k)}_n - a_n|
    \right) = 0$.
  \end{enumerate}

  Note that this exercise actually shows that in this space $X$, the
  metrics are not equivalent; instead, the convergence in the taxi cab
  metric is stronger than the convergence in the sup norm metric.
  Thus, Proposition 12.1.18 is not true for \emph{any} metric space.
\end{exo}

\bigskip
\begin{exo}{12.1.16}{Let $\seq{x_n}{1}$ and $\seq{y_n}{1}$ be two
    sequences in a metric space $(X, d)$. Suppose that $\seq{x_n}{1}$
    converges to a point $x \in X$, and $\seq{y_n}{1}$ converges to a
    point $y \in X$. Show that
    $\lim_{n \to \infty} d(x_n, y_n) = d(x, y)$.}

  On the one hand, the triangle inequality applied two times to $d$
  gives us
  \[d(x_n, y_n) \leq d(x_n, x) + d(x,y) + d(y, y_n)\]
  but this is only half of what we need to prove the result.
  
  Similarly, we have
  \[d(x, y) \leq d(x, x_n) + d(x_n,y_n) + d(y_n, y)\]
  so that we can combine the previous two inequalities to get
  \[-d(x_n, x) - d(y_n, y) \leq d(x_n, y_n) - d(x, y) \leq d(x_n, x)
    + d(y_n, y)\]
  i.e.,
  \[|d(x_n, y_n) - d(x, y)| \leq d(x_n, x) + d(y_n, y).\]
  
  Let be $\epsilon > 0$. By hypothesis, there exists a $N_1 \geq 1$
  such that $d(x_n, x) \leq \epsilon/2$ whenever $n \geq N_1$.
  Similarly, there exists a $N_2 \geq 1$ such that $d(y_n, y) \leq
  \epsilon/2$ whenever $n \geq N_2$. Thus, if we set $N :=
  \max(N_1,N_2)$, then for all $n \geq N$ we have
  \[|d(x_n, y_n) - d(x, y)| \leq d(x_n, x) + d(y_n, y) \leq 2
    \epsilon/2 \ leq \epsilon\]
  which shows that $\lim_{n \to \infty} d(x_n, y_n) = d(x, y)$, as
  expected.    
\end{exo}

\pagebreak
\begin{exo}{12.2.1}{Verify the claims in Example 12.2.8}

  Let be $(X, \ddisc)$ a metric space, and $E$ a subset of $X$.

  \begin{itemize}
  \item Let be $x \in E$. Then $x$ is an interior point of $E$.
    Indeed, we have $B(x, 1/2) = \{x\} \subseteq E$.
  \item Let be $y \notin E$. Then $y$ is an exterior point of $E$.
    Indeed, we have $B(y, 1/2) \cap E = \{y\} \cap E = \emptyset$.
  \item Finally, there are no boundary points of $E$ in $(X, \ddisc)$.
    Indeed, let be $r > 0$ and any $x \in X$. We will always have
    $B(x, r) = \{x\}$ by definition of the discrete metric $\ddisc$.
    Thus, we have either $x \in E$ and then $x \in \inter (E)$, or $x
    \notin E$ and then $x \in \ext(E)$. Thus, $E$ has no boundary
    points.
  \end{itemize}
\end{exo}

\begin{exo}{12.2.2}{Prove Proposition 12.2.10.}

  We have to prove the following implications:

  \begin{itemize}
  \item Let's show that $(a) \implies (b)$. We will use the
    contrapositive, assuming that $x_0$ is neither an interior point
    of $E$, nor a boundary point of $E$. By definition, it means that
    $x_0$ is an exterior point of $E$, i.e. that there exists $r > 0$
    such that $B(x_0, r) \cap E = \emptyset$. This is precisely the
    negation of $x_0$ being an adherent point of $E$. Thus, we have
    showed that if $x_0$ is an adherent of of $E$, it is either an
    interior point of a boundary point.
  \item Let's show that $(b) \implies (c)$. Let be a positive integer
    $n > 0$, and suppose that $x_0$ is either an interior point of
    $E$, or a boundary point of $E$. In either case, the set
    $A_n := B(x_0, 1/n) \cap E$ is non empty, i.e., there exists
    $a_n \in X$ such that $d(a_n, x_0) < 1/n$. By the (countable)
    axiom of choice, we can define a sequence $\seq{a_n}{1}$ such that
    $a_n \in A_n$ for all $n \geq 1$.

    Let be $\epsilon > 0$. There exists $N > 0$ such that $1/N <
    \epsilon$ (Exercise 5.4.4). Thus, for all $n \geq N$, we have
    \[d(a_n, x_0) < \frac{1}{n} \leq \frac{1}{N} < \epsilon\]
    i.e., the sequence $\seq{a_n}{1}$ converges to $x_0$ with respect
    to the metric $d$, as expected.
  \item Finally, let's show that $(c) \implies (a)$. Let be $r > 0$.
    If $\seq{a_n}{1}$ in $E$ converges to $x_0$ with respect to $d$,
    then there exists a $n$ such that $d(x_0, a_n) < r$. But since
    $a_n \in E$, it means that $B(x_0, r) \cap E$ is non empty, i.e.
    that $x_0$ is an adherent point of $E$.
  \end{itemize}
\end{exo}

\begin{exo}{12.2.3}{Prove Proposition 12.2.5.}

  Let be $(X,d)$ a metric space.

  \begin{enumerate}[label=(\alph*)]
  \item Let be $E \subseteq X$. First suppose that $E$ is open; this
    means that $E \cap \partial E = \emptyset$. Let be $x \in E$, then we
    have $x \notin \partial E$. But since $x \in E$, we have $x \in
    \adh{E}$, and thus $x \in \inter(E)$ by Proposition 12.2.10(b). We
    have shown that $x \in E \implies x \in \inter(E)$, and since the
    converse implication is trivial (Remark 12.2.6), we have $E =
    \inter(E)$ as expected.

    Now suppose that $E = \inter(E)$. Let be $x \in E$. We thus have
    $x \in \inter(E)$. By definition, $x$ is thus not a boundary point
    of $E$, i.e. x $\notin \partial E$. This means that $E \cap
    \partial E = \emptyset$, i.e. that $E$ is open, as expected.
    
  \item Let be $E \subseteq X$. First suppose that $E$ is closed; i.e.
    that $\partial E \subseteq E$. Let be $x \in \adh{E}$. By
    Proposition 12.2.10, we have $\adh{E} = \inter(E) \cup \partial
    E$; such that $\adh{E}$ is the union of two subsets of $E$, and
    thus is itself a subset of $E$, as expected.

    Conversely, suppose that $\adh{E} \subseteq E$. It means that
    $\inter(E) \cup \partial E \subseteq E$, and in particular that
    $\partial E \subseteq E$, i.e. that $E$ is closed, as expected.
    
  \item Let be $x_0 \in X$, $r > 0$ and $E := B(x_0, r)$. To show that
    $E$ is open, we must show that $E = \inter(E)$ (by Proposition
    1.2.15(a)), and in particular that $E \subseteq \inter(E)$ (the
    converse inclusion being trivial). Let be $x \in E$, and let's
    show that $x \in \inter(E)$. By definition, we have
    $d(x, x_0) < r$, so that $\epsilon := r - d(x, x_0)$ is a positive
    real number. Thus, let be $y \in B(x, \epsilon)$. By the triangle
    inequality, we have
    \begin{align*}
      d(x_0, y) &< d(x, x_0) + d(x,y) \\
                &< d(x, x_0) + \epsilon \\
                &< d(x, x_0) + r - d(x, x_0) = r
    \end{align*}
    so that $y \in E$. Thus, there exists $\epsilon > 0$ such that
    $B(x, \epsilon) \subseteq E$, i.e., $x$ is an interior point of
    $E$. This shows that $E \subseteq \inter(E)$, as expected.

    Now let be $F := \{x \in X : d(x, x_0) \leq r\}$, and let be
    $\seq{a_n}{1}$ a convergent sequence in $F$. To show that $F$ is
    closed, we have to show that $\ell := \lim_{n \to \infty} a_n$
    lies in $F$ (Proposition 12.2.15(b)). Suppose, for the sake of
    contradiction, that $\ell \notin F$. We thus have $d(\ell, x_0) >
    r$, so that $\epsilon := d(\ell, x_0) - r$  is a positive real
    number. Since $\seq{a_n}{1}$ converges to $\ell$, there exists a
    $N > 0$ such that $d(a_n, \ell) < \epsilon$ whenever $n \geq N$.
    By the triangle inequality, for $n \geq N$, we have
    \begin{align*}
      d(x_0, \ell) &\leq d(x_0, a_n) + d(a_n, \ell) \\
      d(x_0, a_n) &\geq d(x_0, \ell) - d(a_n, \ell) \\
                   &\geq d(x_0, \ell) - \epsilon \\
                   &\geq d(x_0, \ell) + r - d(\ell, x_0) \\
                   &\geq r
    \end{align*}
    and thus, $a_n \notin B(x_0, r)$, a contradiction. Thus, we must
    have $\ell \in F$, so that $F$ is indeed a closed set.
    
  \item Let be $\{x_0\}$ a singleton with $x_0 \in X$. To show that
    $E$ is closed, we may use Proposition 12.2.15(b), and show that
    $\{x_0\}$ contains all its adherent points. Let be $\seq{a_n}{1}$
    a convergent sequence in $\{x_0\}$; it can only be the constant
    sequence $x_0, x_0, \ldots$. Since it is a constant sequence, its
    limit can only be $x_0$ itself, and this limit belongs to
    $\{x_0\}$. Thus, $\{x_0\}$ is a closed set.
    
  \item First we can form a lemma: for any subset $E \subseteq X$, we
    have $\inter(E) = \ext(X \backslash E)$. This is a direct
    consequence of Definition 12.2.5. Indeed, $x \in \inter(E)$ iff
    there exists a $r > 0$ such that $B(x,r) \subseteq E$, which is
    equivalent to
    ``$\exists r>0 : B(x,r) \cap (X \backslash E) = \emptyset$'',
    which is equivalent to $x \in \ext(X \backslash E)$.

    This implies that the interior points of $E$ are the exterior points
    of $X \backslash E$, and conversely, that the exterior points of
    $E$ are the interior points of $X \backslash E$. Thus, in
    particular, we have this useful fact:
    \begin{equation}
      \label{eq:12.2.3}
      \partial E = \partial(X \backslash E).
    \end{equation}

    Now we go back to the main proof. First suppose that $E$ is open.
    Thus, by Definition 12.2.12, we have
    $E \cap \partial E = \emptyset$, so that
    $\partial E \subseteq X \backslash E$, which means that
    $X \backslash E$ is a closed set. The converse also applies: if we
    suppose that $X \backslash E$ is closed, then
    $\partial (X \backslash E) \subseteq X \backslash E$. By equation
    \eqref{eq:12.2.3} above, this is equivalent to
    $\partial E \subseteq X \backslash E$, and thus we have
    $\partial E \cap E = \emptyset$. This means that $E$ is open, as
    expected.\footnote{This important result will be used in future
      proofs to turn any statement on closed sets into a statement on
      open sets.}
    
  \item Let $E_1, \ldots, E_n$ be open sets. Thus, for all
    $1 \leq i \leq n$, if $x \in E_i$, there exists a $r_i > 0$ such
    that $B(x, r_i) \subseteq E_i$. Let's define $r := \min(r_1,
    \ldots, r_n)$. We have $B(x, r) \subseteq B(x, r_i) \subseteq E_i$
    for all $1 \leq i \leq n$, i.e. $B(x,r) \subseteq E_1 \cap \ldots
    \cap E_n$. Thus, $E_1 \cap \ldots \cap E_n$ is an open set.

    Also, let $F_1, \ldots, F_n$ be closed sets. By the previous
    result (e), the complementary sets
    $X\backslash F_1, \ldots X\backslash F_n$ are open sets. Thus, we
    have just proved that
    $(X \backslash F_1) \cap \ldots \cap (X \backslash F_n)$ is an
    open set. But we have
    $(X \backslash F_1) \cap \ldots \cap (X \backslash F_n) = X
    \backslash (F_1 \cup \ldots \cup F_n)$, and this set is open.
    Thus, by (e), its complementary set, $F_1 \cup \ldots \cup F_n$,
    is closed, as expected.
    
  \item Let $(E_\alpha)_{\alpha \in I}$ be open sets. Suppose that we
    have $x \in \bigcup_{\alpha \in I} E_\alpha$. By definition, there
    exists a $i \in I$ such that $x \in E_i$. Since $E_i$ is an open
    set, there exists $r_i > 0$ such that
    $B(x, r_i) \subseteq E_i \subseteq \bigcup_{\alpha \in I}
    E_\alpha$. Thus, by (a), $\bigcup_{\alpha \in I} E_\alpha$ is an
    open set, as expected.

    Now let be $(F_\alpha)_{\alpha \in I}$ be closed sets. Suppose
    that we have a convergent sequence $\seq{x_n}{1}$ such that
    $x_n \in \bigcap_{\alpha \in I} F_\alpha$ for all $n \geq 1$.
    Thus, for all $\alpha \in I$, the sequence $\seq{x_n}{1}$ entirely
    belongs to the closed set $F_\alpha$, so that its limit $\ell$
    also lies in $F_\alpha$ according to (b). Thus,
    $\ell \in \bigcup_{\alpha \in I} F_\alpha$, so that
    $\bigcap_{\alpha \in I} F_\alpha$ is a closed set, as expected.
    
  \item Let be $E \subseteq X$.
    \begin{itemize}
    \item Let's show that $\inter(E)$ is the largest open set included
      in $E$. It has not clearly be proved in the main text that
      $\inter(E)$ is an open set, so we begin by proving it. Let be
      $x \in \inter(E)$. By definition, there exists $r > 0$ so that
      $B(x, r) \subseteq E$. But by (c), we know that $B(x, r)$ is an
      open set, so that any point $y$ of $B(x, r)$ is an interior
      point of this open ball, and thus an interior point of $E$.
      Thus, $\inter(E)$ is open.

      Now consider another open set $V \subseteq E$, and let's show
      that $V \subseteq \inter(E)$. If $x \in \inter(V)$, then there
      exists $r > 0$ such that $B(x,r) \subseteq V \subseteq E$, so
      that $x \in \inter(E)$. This shows that $V \subseteq \inter(E)$,
      as expected.

    \item Similarly, let's show that $\adh{E}$ is the smallest closed
      set that contains $E$. First we show that $\adh{E}$ is closed,
      i.e. that $\adh{\adh{E}} \subseteq \adh{E}$. (Hint: see Exercise
      9.1.6 for an intuition.) Let be $x \in \adh{\adh{E}}$. By
      definition, for all $r > 0$,
      $B(x,r) \cap \adh{E} \neq \emptyset$. Thus, there exists
      $y \in B(x,r)$ such that $y \in \adh{E}$. Thus, because $B(x,r)$
      is an open set and $y$ is adherent to $E$, there exists
      $\epsilon > 0$ such that $B(y, \epsilon) \subseteq B(x,r)$ and
      $B(y, \epsilon) \cap E \neq \emptyset$; i.e., there exists
      $z \in B(y, \epsilon) \subseteq B(x,r)$ such that $z \in E$. We
      have showed that whenever $x \in \adh{\adh{E}}$, we have $B(x,r)
      \cap E \neq \emptyset$ for all $r > 0$, i.e. that $x$ is an
      adherent point of $E$, as expected. Thus, $\adh{E}$ is closed.
      
      Now we consider a closed set $K$ such that $E \subseteq K$, and
      we have to show that $\adh{E} \subseteq K$. Let be
      $x \in \adh{E}$. By definition, for all $r > 0$, we have
      $B(x,r) \cap E \neq \emptyset$. But since $E \subseteq K$, we
      also have $B(x,r) \cap K \neq \emptyset$ for all $r > 0$. Thus,
      $x$ is an adherent point of $K$, i.e., $x \in \adh{K}$. But
      since $K$ is closed, we have $K = \adh{K}$, and thus $x \in K$.
      This shows that $\adh{E} \subseteq K$, as expected.
    \end{itemize}
  \end{enumerate}
\end{exo}

\begin{exo}{12.2.4}{Let $(X, d)$ be a metric space, $x_0$ be a point
    in $X$, and $r > 0$. Let $B$ be the open ball
    $B := B(x_0 , r) = \{x \in X : d(x, x_0 ) < r\}$, and let $C$ be
    the closed ball $C := \{x \in X : d(x, x_0 ) \leq r\}$.}

  Let's prove the following claims:

  \begin{enumerate}[label=(\alph*)]
  \item Show that $\adh{B} \subseteq C$.

    Let be $x \in \adh{B}$. By definition, since $x$ is an adherent
    point of $B$, for all $\epsilon > 0$ we have
    $B(x, \epsilon) \cap B \neq \emptyset$. In other words, there
    exists $y$ such that we have both $d(x, y) < \epsilon$ and
    $d(x_0, y) < r$. Thus, by the triangle inequality, we have
    \begin{align*}
      d(x, x_0) &\leq d(x,y) + d(y, x_0)\\
                &\leq \epsilon + r \; \; \text{ for all } \epsilon > 0
    \end{align*}
    which is equivalent (as a quick proof by contradiction would show)
    to $d(x,x_0) \leq r$. Thus, $x \in C$.

    We have indeed proved that $\adh{B} \subseteq C$.
    
  \item Give an example of a metric space $(X,d)$, a point $x_0$, and
    a radius $r > 0$ such that $\adh{B}$ is \emph{not} equal to $C$.

    Let's take $X = \rr$, $d = \ddisc$, $x = 0$ and $r = 1$. One the
    one hand, we have $B := \{0\}$ and $C := \rr$. Now let's work out
    $\adh{B}$. By Proposition 12.2.15(bd), $B$ is closed, so that we
    have $\adh{B} = B$. Thus, we clearly do not have $\adh{B} \neq C$
    here. (Note however that any $x_0 \in \rr$ would be convenient
    here; there is nothing special about $0$.)
  \end{enumerate}
\end{exo}

\begin{exo}{12.3.1}{Prove Proposition 12.3.4(b).}

  Let's show each direction of the equivalence.

  \begin{itemize}
  \item First suppose that $E$ is relatively closed w.r.t. $Y$, and
    let's show that there exists a closed subset $K \subseteq X$ such
    that $E = K \cap Y$.

    Since $E$ is closed w.r.t. $Y$, the set $Y \backslash E$ is open
    w.r.t. $Y$ (by Proposition 12.2.15(e)). Thus, by (a), there exists
    an open subset $V \subseteq X$ such that
    $Y \backslash E = V \cap Y$.

    Let be $K := X \backslash V$; this subset $K \subseteq X$ is
    closed w.r.t. $X$ by Proposition 12.2.15(e) since it is the
    complementary set of an open set. We have to show that
    $E = K \cap Y$.
    \begin{itemize}
    \item Let be $x \in E$. Thus, we have $x \in Y$, since $E
      \subseteq Y$. And since $x \in E$, by definition, we have $x
      \notin Y \backslash E$. Thus, $x \notin V \cap Y$, which implies
      that $x \notin V$ (since $x \in Y$). Thus, by definition, $x \in
      K$, and thus, $x \in K \cap Y$.
    \item Conversely, let be $x \in K \cap Y$. By definition, $x \in
      Y$ and $x \notin V$. Thus, $x \notin V \cap Y$, or, in other
      words, $x \notin Y \backslash E$. We finally get $x \in E$, as
      expected.
    \end{itemize}
    Thus, we have indeed $E = K \cap Y$, for some closed subset
    $K \subseteq X$, as expected.
    
  \item Now let's prove the converse implication: suppose that $E = K
    \cap Y$ for some closed subset $K \subseteq X$, and let's prove
    that $E$ is relatively closed w.r.t. $Y$.

    Still by Proposition 12.2.15(e), we know that the subset $V := X
    \backslash K$ is open w.r.t. $X$. Thus, by the previous result
    from this exercise, $V \cap Y$ is relatively open w.r.t. $Y$.
    Thus, its complementary set $Y \backslash (V \cap Y) = Y
    \backslash V$ is relatively closed w.r.t. $Y$. Now we want to show
    that $E = Y \backslash V$ to close the proof.

    \begin{itemize}
    \item First suppose that $x \in E$. Since $E = K \cap Y$, we thus
      have $x \in Y$ and $x \in K$, i.e. $x \notin V$. Thus, $x \in Y
      \backslash V$.
    \item Now suppose that $x \in Y \backslash V$. We thus have
      $x \in X$ (since $Y \subseteq X$) and $x \notin V$, so that we
      necessarily have $x \in K$. Thus $x \in Y \cap K$, i.e.
      $x \in E$.
    \end{itemize}
    Thus $E = Y \backslash V$ is relatively closed w.r.t. $Y$, as
    expected.
  \end{itemize}  
\end{exo}

\begin{exo}{12.4.1}{Prove Lemma 12.4.3.}

  We have to prove that any subsequence $(x^{(n_j)})_{j=1}^\infty$ of
  a convergent sequence $\seq{x^{(n)}}{m}$ converges to the same limit
  as the whole sequence itself.

  Suppose that the whole sequence $\seq{x^{(n)}}{m}$ converges to
  $x_0$. Let be $\epsilon > 0$. By definition, we have a positive
  integer $N \geq m$ such that $n \geq N \implies d(x^{(n)}, x_0) \leq
  \epsilon$. Our aim here is to show that there exists a positive
  integer $J \geq 1$ such that $j \geq J \implies d(x^{(n_j)}, x_0)
  \leq \epsilon$.

  By Definition 12.4.1, we know that we have $m \leq n_1 < n_2 < n_3 <
  \ldots$. Thus, as a quick induction would show, we have $n_j \geq m
  + j - 1$ for all $j \geq 1$. Let's take $J := N$. In this case, if
  $j \geq J$, i.e. if $j \geq N$, we have $n_j \geq m + N - 1 \geq
  N$. Thus:
  
  \[ j\geq J \implies n_j \geq N \implies d(x^{(n_j)}, x_0)
    \leq \epsilon.\]

  Since this is true for all $\epsilon > 0$, it means that
  $(x^{(n_j)})_{j=1}^\infty$ converges to $x_0$, as expected. 
\end{exo}

\bigskip
\begin{exo}{12.4.2}{Prove Proposition 12.4.5.}

  Let $\seq{x^{(n)}}{m}$ be a sequence of points in a metric space. We
  have to prove that the following two statements are equivalent:
  \begin{enumerate}[label=(\alph*)]
  \item $L$ is a limit point of $\seq{x^{(n)}}{m}$.
  \item There exists a subsequence $(x^{(n_j)})_{j=1}^\infty$ of the
    original sequence which converges to $L$.
  \end{enumerate}

  We will prove the two implications, but first, note that (with the
  notations from Definition 12.4.1) if we have $1 \leq m \leq n_1 <
  n_2 < n_3 < \ldots$, then a quick induction shows that we have $n_j
  \geq j$ for all $j \geq 1$.
  
  \begin{itemize}
  \item First we prove that (b) implies (a). If some subsequence
    $(x^{(n_j)})_{j=1}^\infty$ converges to $L$, then we have by
    definition:
    \begin{equation}
      \label{eq:12.4.2a}
      \forall \epsilon > 0, \, \exists J \geq 1 : \; j \geq J \implies
      d(x^{(n_j)}, L) \leq \epsilon
    \end{equation}
    Now, consider any $\epsilon > 0$ and any $N \geq m$. For this
    particular choice of $\epsilon$, consider the corresponding real
    number $J$ given by equation \eqref{eq:12.4.2a}, and let's define
    $p := \max(N, J)$. Thus, we have $n_p \geq p \geq J$, and by
    equation \eqref{eq:12.4.2a}, we thus have
    $d(x^{(n_p)}, L) \leq \epsilon$. If we set $n := n_p$, we have
    indeed found an $n \geq N$ such that
    $d(x^{(n)}, L) \leq \epsilon$. Thus, $L$ is a limit point of
    $\seq{x^{(n)}}{m}$, as required.
  \item Now we prove that (a) implies (b). Suppose that $L$ is a limit
    point of $\seq{x^{(n)}}{m}$. By definition, there exists a natural
    number $n_1 \geq m$ such that $d(x^{(n_1)}, L) \leq 1$. Now, for
    $j > 1$, let's define inductively
    $n_j := \min \{n > n_{j-1} : d(x^{(n)}, L) \leq 1/j\}$. This set
    is non-empty (by definition of a limit point), so that the
    well-ordering principle (Proposition 8.1.4) ensures that it has a
    (unique) minimal element, i.e. that $n_j$ indeed exists. Let's
    define the subsequence $(x^{(n_j)})_{j=1}^\infty$ obtained
    following this process. We thus have $d(x^{(n_j)}, L) \leq 1/j$
    for all $j \geq 1$, by construction.

    Thus, let be $\epsilon > 0$. There exists a $j \geq 1$ such that
    $0 < 1/j < \epsilon$ (Exercise 5.4.4). Thus, for this positive
    integer $j$, we have $d(x^{(n_j)}, L) \leq 1/j < \epsilon$. By
    construction, for all other natural numbers $k \geq j+1$, we have
    $d(x^{(n_k)}, L) \leq 1/k \leq 1/j \leq \epsilon$.

    In summary, for our arbitrary choice of $\epsilon$, we have showed
    that there exists $j \geq 1$ such that, for all $k \geq j$, we
    have $d(x^{(n_k)}, L) \leq \epsilon$. Thus, the subsequence
    $(x^{(n_j)})_{j=1}^\infty$ constructed in this way converges to
    $L$, as expected.
  \end{itemize}
\end{exo}

\begin{exo}{12.4.3}{Prove Lemma 12.4.7.}

  Suppose that $\seq{x^{(n)}}{m}$ is a convergent sequence of points
  in a metric space $(X,d)$, and that its limit is $x_0$. Let's show
  that it is a Cauchy sequence.

  By the triangle inequality, we know that for all $j,k \geq m$, we
  have:
  \[d(x^{(j)}, x^{(k)}) \leq d(x^{(j)}, x_0) + d(x^{(k)}, x_0).\]

  Let be $\epsilon > 0$. Since $\seq{x^{(n)}}{m}$ converges to $x_0$,
  there exists an $N \geq m$ such that we have $d(x^{(n)}, x_0) \leq
  \epsilon/3$ for all $n \geq N$. Thus, if we take $j,k \geq N$, we
  have:
  \begin{align*}
    d(x^{(j)}, x^{(k)}) &\leq d(x^{(j)}, x_0) + d(x^{(k)}, x_0) \\
                        &\leq \epsilon/3 + \epsilon/3\\
                        &< \epsilon
  \end{align*}
  which means that $\seq{x^{(n)}}{m}$ is a Cauchy sequence, as
  expected.  
\end{exo}

\bigskip
\begin{exo}{12.4.4}{Prove Lemma 12.4.9.}

  Let be an arbitrary $\epsilon > 0$. Since the subsequence
  $(x^{(n_j)})_{j=1}^\infty$ converges to $x_0$, there exists a
  $J \geq 1$ such that $d(x^{(n_j)}, x_0) \leq \epsilon/3$ whenever
  $j \geq J$.

  But the whole sequence $\seq{x^{(n)}}{m}$ is supposed to be a Cauchy
  sequence. Thus, there also exists a $N \geq m$ such that $d(x^{(j)},
  x^{(k)}) < \epsilon/3$ whenever $j,k \geq N$.

  Now, let be $K := \max(J,N)$. If $k \geq K$, we have
  \begin{align*}
    d(x^{(k)}, x_0) &\leq d(x^{(k)}, x^{(n_k)}) + d(x^{(n_k)}, x_0) \\
                    &< \epsilon/3 + \epsilon/3\\
                    &< \epsilon
  \end{align*}
  which means that $\seq{x^{(n)}}{m}$ converges to $x_0$, as expected.
\end{exo}

\bigskip
\begin{exo}{12.4.5}{Let $\seq{x^{(n)}}{m}$ be a sequence of points in
    a metric space $(X,d)$ and let $L \in X$. Show that if $L$ is a
    limit point of the sequence $\seq{x^{(n)}}{m}$, then $L$ is an
    adherent point of the set $\{x^{(n)} : n \geq m\}$. Is the
    converse true?}

  First suppose that $L$ is a limit point of $\seq{x^{(n)}}{m}$. By
  definition, it means that
  \begin{equation}
    \label{eq:12.4.5a}
    \forall \epsilon > 0, \, \forall N \geq m, \, \exists n \geq N \;
    : \; d(x^{(n)}, L) \leq \epsilon
  \end{equation}

  Let be an arbitrary $\epsilon > 0$, and let's take $N = m$. By
  formula \eqref{eq:12.4.5a} above, there exists an $n \geq N$ such
  that $d(x^{(n)}, L) \leq \epsilon$. Thus, this $x^{(n)}$ belongs to
  both sets $\{x^{(n)} : n \geq m\}$ and $B(L, \epsilon)$. We have
  just proved that for all $\epsilon > 0$, the intersection
  $B(L, \epsilon) \cap \{x^{(n)} : n \geq m\}$ is always non-empty. In
  other words, $L$ is thus an adherent point of
  $\{x^{(n)} : n \geq m\}$.

  However, the converse is not true. Indeed, consider the sequence
  $\seq{x^{(n)}}{1}$ defined in $(\rr, d)$ by $x^{(1)} = 1$ and
  $x^{(n)} = 0$ for all $n \geq 2$, i.e. the sequence
  $1, 0, 0, 0, \ldots$. It is clear that $L := 1$ is an adherent point
  of $\{x^{(n)} : n \geq 1\}$ (which is just the set $\{0, 1\}$). But
  $1$ is not a limit point of $\seq{x^{(n)}}{1}$, since we have
  $d(x^{(n)}, 1) > 1/2$ for all $n \geq 2$.
\end{exo}

\bigskip
\begin{exo}{12.4.6}{Show that every Cauchy sequence can have at most
    one limit point.}

  Suppose that $\seq{x^{(n)}}{m}$ is a Cauchy sequence in a metric
  space $(X,d)$, such that $L, L'$ are limit points. Then we have
  $L=L'$. We will give two different proofs for this fact.

  \begin{itemize}
  \item \textbf{Proof 1} (short proof using previous results). By
    Proposition 12.4.5, since $L$ is a limit point of
    $\seq{x^{(n)}}{m}$, there exists a subsequence that converges to
    $L$. But by Lemma 12.4.9, it means that the whole original
    sequence $\seq{x^{(n)}}{m}$ also converges to $L$. The same
    argument can be used to show that the whole sequence
    $\seq{x^{(n)}}{m}$ converges to $L'$. But by uniqueness of limits
    (Proposition 12.1.20), we must have $L=L'$, as expected.
  \item \textbf{Proof 2} (a more ``manual'' proof). Let be
    $\epsilon > 0$. Since $\seq{x^{(n)}}{m}$ is a Cauchy sequence,
    there exists $N \geq m$ such that
    $d(x^{(p)}, x^{(q)}) \leq \epsilon/3$ for all $p,q \geq N$.

    If $L$ is a limit point of $\seq{x^{(n)}}{m}$, then for this $N
    \geq m$, there exists $p \geq N$ such that $d(x^{(p)}, L) \leq
    \epsilon /3$. Similarly, there exists $q \geq N$ such that
    $d(x^{(q)}, L') \leq \epsilon/3$.

    We thus have, by triangle inequality:
    \begin{align*}
      d(L, L') &\leq d(L, x^{(p)}) + d(x^{(p)}, x^{(q)}) + d(x^{(q)},
                 L') \\
               &\leq \epsilon/3 + \epsilon/3 + \epsilon/3 \\
               &\leq \epsilon
    \end{align*}
    Thus, $d(L,L') \leq \epsilon$ for all $\epsilon > 0$, which
    implies $L=L'$.
  \end{itemize}  
\end{exo}

\begin{exo}{12.4.7}{Prove Proposition 12.4.12.}

  For statement (a), consider a convergent sequence $\seq{y^{(n)}}{m}$
  of elements of $Y \subseteq X$. Since it is convergent, it is a
  Cauchy sequence (Lemma 12.4.7). Saying that $(Y, d_{Y \times Y})$ is
  complete means that $\seq{y^{(n)}}{m}$ converges in
  $(Y, d_{Y \times Y})$. Thus, every convergent sequence in $Y$ has
  its limit in $Y$: this is exactly the characterization of closed
  sets given by Proposition 12.2.15(b).

  For statement (b), consider a Cauchy sequence $\seq{y^{(n)}}{m}$ of
  elements of a given closed subset $Y \subseteq X$. Since $(X,d)$ is
  complete, $\seq{y^{(n)}}{m}$ must converge to some value $L \in X$.
  But since $Y$ is closed, we have $L \in Y$ by Proposition
  12.2.15(b). Thus, every Cauchy sequence in $Y$ converges in $Y$.
  This means that $(Y, d_{Y \times Y})$ is complete, as expected.
\end{exo}

\bigskip
\begin{exo}{12.4.8}{The following construction generalizes the
    construction of the reals from the rationals in Chapter 5. In what
    follows, we let $(X,d)$ be a metric space.}
  
  We have to prove the following statements. Note that this is a
  generalization of the process of construction of the real numbers,
  so that we can use all results relative to the real numbers below.
  
  \begin{enumerate}[label=(\alph*)]
  \item Given any Cauchy sequence $\seq{x^{(n)}}{1}$ in $X$, we denote
    $\formallimit{x_n}$ its formal limit. We say that two formal
    limits $\formallimit{x_n}, \formallimit{y_n}$ are equal iff
    $\lim_{n \to \infty} d(x_n, y_n) = 0$. Then, this equality
    relation obeys the reflexive, symmetry and transitive axioms.
    \begin{itemize}
    \item This relation is reflexive: for every Cauchy sequence
      $\seq{x^{(n)}}{1}$, we have ${d(x_n, x_n) = 0}$ for all
      $n \geq 1$, by definition of a metric. Thus, $d(x_n, x_n)$ is
      constant and equal to zero, so that
      $\lim_{n \to \infty} d(x_n, x_n) = 0$.
    \item By the property of symmetry of the metric $d$, we have
      $d(x_n, y_n) = d(y_n, x_n)$ for all $n \geq 1$ and all Cauchy
      sequence $\seq{x^{(n)}}{1}, \seq{y^{(n)}}{1}$. Thus,
      $\formallimit{x_n} = \formallimit{y_n}$ iff $\lim_{n \to \infty}
      d(x_n, y_n) = 0$, iff $\lim_{n \to \infty} d(y_n, x_n) = 0$,
      which is equivalent to $\formallimit{y_n} = \formallimit{x_n}$.
    \item For transitivity, suppose that $\seq{x^{(n)}}{1}$,
      $\seq{y^{(n)}}{1}$ and $\seq{z^{(n)}}{1}$ are Cauchy sequences
      in $X$. If $\formallimit{x_n} = \formallimit{y_n}$ and
      $\formallimit{y_n} = \formallimit{z_n}$, then by definition we
      have $\lim_{n \to \infty} d(x_n, y_n) = 0$ and $\lim_{n \to
        \infty} d(y_n, z_n) = 0$. Let be $\epsilon > 0$. By
      definition, there exists $N_1 \geq 1$ such that $d(x_n, y_n)
      \leq \epsilon/2$ whenever $n \geq N_1$. Similarly, there exists
      $N_2 \geq 1$ such that $d(y_n, z_n) \leq \epsilon/2$ whenever $n
      \geq N_2$. Thus, if $n \geq N := \max(N_1, N_2)$, we have by the
      triangle inequality $d(x_n, z_n) \leq d(x_n, y_n) + d(y_n, z_n)
      \leq \epsilon$. It means that $\lim_{n \to \infty} d(x_n, z_n)$,
      i.e. that $\formallimit{x_n} = \formallimit{z_n}$, as expected.
    \end{itemize}
    
  \item Let $\adh{X}$ be the space of all formal limits of Cauchy
    sequences in $X$, with the above equality relation. Define a
    metric $d_{\adh{X}} : \adh{X} \times \adh{X} \to \rr^+$ by setting
    \[ d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n}) := \lim_{n
        \to \infty} d(x_n, y_n).\]
    Then this function is well-defined and gives $\adh{X}$ the
    structure of a metric space.
    \begin{itemize}
    \item First we have to show that the limit
      $\lim_{n \to \infty} d(x_n, y_n)$ exists (in $\rr^+$) for all Cauchy
      sequences $\seq{x^{(n)}}{1}, \seq{y^{(n)}}{1}$. We already know
      that $\rr$ is complete, thus $\rr^{+}$ is complete as a closed
      subset of the complete space $\rr$ (Proposition 12.4.12(b)).

      Let be the sequence defined by $u_{n} := d(x_{n}, y_{n})$ for
      all $n \geq 1$. Obviously, this sequence is in $\rr^{+}$, which is
      a complete space. Thus, to show that it converges, we just have
      to show that it is a Cauchy sequence.

      Consider the usual metric on $\rr^{+}$. We have, for all
      $p,q \geq 1$,
      \begin{align*}
        |u_{p} - u_q| &= |d(x_p, y_p) - d(x_q, y_q)| \\
                      &\leq |d(x_p, x_q) + d(x_q, y_q) + d(y_q, y_p) -
                        d(x_q, y_q)| \\
                      &\leq |d(x_p, x_q)| + |d(y_p, y_q)|.
      \end{align*}
      Now let be $\epsilon > 0$. Since $\seq{x^{(n)}}{1}$ and
      $\seq{y^{(n)}}{1}$ are Cauchy sequences, there exists
      $N_1, N_2 \geq 1$ such that $d(x_p, x_q) \leq \epsilon/2$ whenever $p,q \geq
      N_1$, and $d(y_p, y_q) \leq \epsilon/2$ whenever $p,q \geq N_2$. Thus, if
      $p,q \geq N:= \max(N_1, N_2)$, we have
      \[|u_p - u_q| \leq |d(x_p, x_q)| + |d(y_p, y_q)| \leq \epsilon.\]
      This shows that $\seq{u_n}{1}$ is a Cauchy sequence, and thus,
      that $\lim_{n \to \infty} d(x_n, y_n)$ exists.
    \item Now we must show that the axiom of substitution is obeyed.
      In other words, consider a Cauchy sequence $\seq{z^{(n)}}{1}$ in
      $(X,d)$ such that $\formallimit z_n = \formallimit x_n$. We must
      show that
      $d_{\adh{X}}(\formallimit z_n, \formallimit y_n) =
      d_{\adh{X}}(\formallimit x_n, \formallimit y_n)$, i.e. that
      \begin{equation}
        \label{eq:12.4.8b}
        \lim_{n \to \infty} d(z_n, y_n) = \lim_{n \to \infty} d(x_n, y_n)
      \end{equation}
      By the previous bullet point, we know that both limits in
      \eqref{eq:12.4.8b} do exist. Thus, the limit laws apply. We
      have:
      \[d(z_n, y_n) \leq d(z_n, x_n) + d(x_n, y_n)\]
      but since $\lim_{n \to \infty} d(z_n, x_n) = 0$ by definition, we
      obtain
      \[\lim_{n \to \infty} d(z_n, y_n) \leq \lim_{n \to \infty} d(x_n, y_n)\]
      if we take the limits of both sides in the previous inequality.

      But similarly, we have $d(x_n, y_n) \leq d(x_n, z_n) + d(z_n,
      y_n)$, so that a similar argument gives
      \[\lim_{n \to \infty} d(x_n, y_n) \leq \lim_{n \to \infty} d(z_n, y_n).\]

      Thus, we have indeed $\lim_{n \to \infty} d(z_n, y_n) = \lim_{n \to \infty}
      d(x_n, y_n)$, as expected.
    \item Finally, we must show that $d_{\adh{X}}$ is a metric on
      $\adh{X}$. To prove this statement, we must show that
      $d_{\adh{X}}$ obeys all four axioms that define a metric.
      \begin{itemize}
      \item First, it is clear that
        $d_{\adh{X}}(\formallimit{x_n}, \formallimit{x_n}) = \lim_{n \to
          \infty} d(x_n, x_n) = 0$ for all Cauchy sequence
        $\seq{x^{(n)}}{1}$ in $(X,d)$.
      \item Now let be two Cauchy sequences $\seq{x^{(n)}}{1}$,
        $\seq{y^{(n)}}{1}$ in $X$, such that $\formallimit x_n \neq
        \formallimit y_n$. This latest property implies that $\lim_{n
          \to \infty} d(x_n, y_n) > 0$, by definition. Thus,
        $d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n}) > 0$.
      \item Symmetry: we have
        \begin{align*}
          d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n})
          &= \lim_{n \to \infty} d(x_n, y_n) \\
          &= \lim_{n \to \infty} d(y_n, x_n) \text{ (symmetry of $d$ on
            $\rr^+$)}\\
          &= d_{\adh{X}}(\formallimit{y_n}, \formallimit{x_n})
        \end{align*}
        for all Cauchy sequences $\seq{x^{(n)}}{1}$,
        $\seq{y^{(n)}}{1}$.
      \item Triangle inequality: by the limit laws, we have
        \begin{align*}
          &d_{\adh{X}}(\formallimit{x_n}, \formallimit{z_n})
            = \lim_{n \to \infty} d(x_n, z_n) \\
          &\leq \lim_{n \to \infty} (d(x_n, y_n) + d(y_n, z_n)) \\
          &\leq \lim_{n \to \infty} d(x_n, y_n) + \lim_{n \to \infty} d(y_n, z_n) \\
          &\leq d_{\adh{X}}(\formallimit{x_n}, \formallimit{y_n}) +
            d_{\adh{X}}(\formallimit{y_n}, \formallimit{z_n})
        \end{align*}
        for all Cauchy sequences $\seq{x^{(n)}}{1}$,
        $\seq{y^{(n)}}{1}$ and $\seq{z^{(n)}}{1}$.
      \end{itemize}
    \end{itemize}
    Thus, $d_{\adh{X}}$ is indeed a metric on $\adh{X}$.

  \item The metric space $(\adh{X}, d_{\adh{X}})$ is complete.

    To prove this statement, consider a Cauchy sequence $\seq{u_n}{1}$
    in $\adh{X}$: we have to prove that this sequence converges in
    $(\adh{X}, d_{\adh{X}})$.
    
    By definition, $\seq{u_n}{1}$ is a Cauchy sequence of formal
    limits of Cauchy sequences that take their values in $X$; i.e.,
    for all $k \geq 1$, there exists a Cauchy sequence
    $\seq{x_n^{(k)}}{1}$ of elements of $X$ such that
    $u_k := \formallimit x_n^{(k)}$.

    Since all $\seq{x_n^{(k)}}{1}$ are Cauchy sequences, then for all
    $k \geq 1$, there exists a threshold $N_k$ such that
    $d(x_n^{(k)}, x_{N_k}^{(k)}) < 1/k$ whenever $n \geq N_k$. Thus,
    (using the countable axiom of choice) we can build a sequence
    $(z_k)_{k=1}^\infty$ defined by
    \[z_k := \left(x_{N_k}^{(k)}\right)\]
    for all $k \geq 1$. Now:
    \begin{itemize}
    \item We claim that $(z_k)_{k=1}^\infty$ is itself a Cauchy sequence.
      Indeed, consider an arbitrary positive real number $\epsilon > 0$. We
      must prove that $d(z_p, z_q) := d(x_{N_p}^{(p)}, x_{N_q}^{(q)})$
      is eventually lesser than $\epsilon$.

      Since $\seq{u_n}{1}$ is a Cauchy sequence in $\adh{X}$, there
      exists a $N \geq 1$ such that, if $p,q \geq N$, we have
      $d_{\adh{X}}(u_p, u_q) < \epsilon/3$, i.e.:
      \begin{align*}
        \epsilon/3 &> d_{\adh{X}}(u_p, u_q) \\
            &\geq d_{\adh{X}} (\formallimit x_n^{(p)}, \formallimit
              x_n^{(q)}) \\
            &\geq \lim_{n \to \infty} d(x_n^{(p)}, x_n^{(q)})
      \end{align*}
      Thus, there exists a $N' \geq 1$ such that, if $n \geq N'$, we have
      $d(x_n^{(p)}, x_n^{(q)}) \leq \epsilon/3$\footnote{Indeed, for any
        sequence $\seq{v_n}{1}$ that converges to $\ell$, if we have
        $0 \leq \ell < \epsilon$, then there exists an $N \geq 1$ such that
        $v_n \leq \epsilon$ whenever $n \geq N$ (why? use a proof by
        contradiction.).\label{ft.limsuites}}. Also, by Exercise
      5.4.4, there exists a $k > 0$ such that $1/k \leq \epsilon/3$. Thus, if
      $n,p,q \geq \max(k, N', N_p, N_q)$, we have
      \begin{align*}
        d(z_p, z_q)
        &= d(x_{N_p}^{(p)}, x_{N_q}^{(q)}) \\
        &\leq \underbrace{d(x_{N_p}^{(p)}, x_n^{(p)})}_{\leq 1/p \, \leq \, \epsilon/3}
          + \underbrace{d(x_n^{(p)}, x_n^{(q)})}_{\leq \epsilon/3}
          + \underbrace{d(x_n^{(q)}, x_{N_q}^{(q)})}_{\leq 1/q \, \leq \, \epsilon
          /3} \\
        &\leq \epsilon/3 + \epsilon/3 + \epsilon/3\\
        &\leq \epsilon
      \end{align*}
      Thus, $(z_k)_{k=1}^\infty$ is indeed a Cauchy sequence in $X$.
    \item Consequently, we can take the formal limit $L :=
      \formallimit z_n$, and this formal limit $L$ lies in $\adh{X}$
      by definition. We claim that $\lim_{n \to \infty} u_n = L \in \adh{X}$;
      proving this claim will close the proof of (c).

      Let be $\epsilon > 0$. Since $\seq{z_n}{1}$ is a Cauchy sequence in
      $X$, there exists a $N_1 \geq 1$ such that
      $d(z_p, z_q) \leq \epsilon/2$ whenever $p,q \geq N_1$.

      Once again, by Exercise 5.4.4, there exists a $K' \geq 1$ such that
      $1/K' < \epsilon/2$. Thus, if $k \geq K$ and $n > N_k$, we have
      \[d(x_n^{(k)}, z_k) := d(x_n^{(k)}, x_{N_k}^{(k)}) < \frac{1}{k}
        \leq \frac{1}{K} < \frac{\epsilon}{2}.\]

      Thus, by the triangle inequality, we have, for all $n >
      \max(N_k, N_1)$,
      \[d(x_n^{(k)}, z_n) \leq d(x_n^{(k)}, z_k) + d(z_k, z_n) \leq \epsilon/2 +
        \epsilon/2 \leq \epsilon.\]

      Consequently, we have, for all $k > K'$,
      \[d_{\adh{X}}(u_k, L) := \lim_{n \to \infty} d(x_n^{(k)}, b_n) < \epsilon.\]
      This shows that $\seq{u_n}{1} \to L$ in
      $(\adh{X}, d_{\adh{X}})$, which closes the proof.
    \end{itemize}
    
  \item We identify an element $x \in X$ with the corresponding formal
    limit $\formallimit x$ in $\adh{X}$.
    \begin{itemize}
    \item This is legitimate since we have
      $x = y \iff \formallimit x = \formallimit y$.
        
        Indeed, it is clear that if $x = y$, then we have
        $\formallimit x = \formallimit y$ by definition. Conversely,
        if $\formallimit x = \formallimit y$, then we have
        $\lim_{n \to \infty} d(x,y) = 0$, i.e. $d(x,y) = 0$, i.e.
        $x = y$. Thus, this identification is legitimate.
      \item With this identification, we have $d(x,y) =
        d_{\adh{X}}(x,y)$. Indeed:
        \begin{align*}
          d_{\adh{X}}(x,y)
          &= d_{\adh{X}}(\formallimit x, \formallimit y) \\
          &= \lim_{n \to \infty} d(x,y) \\
          &= d(x,y).
        \end{align*}
      \end{itemize}
      Thus, $(X,d)$ can be thought of as a subspace of
      $(\adh{X}, d_{\adh{X}})$.
      
    \item The closure of $X$ in $\adh{X}$ is $\adh{X}$.
      
      Indeed, let be $C$ the closure of $X$ in $\adh{X}$. We clearly
      have $C \subseteq \adh{X}$, by definition. Thus we just have to show
      that $\adh{X} \subseteq C$.

      Let be $x \in \adh{X}$, and let's show that $x \in C$. By
      definition, $x \in C$ means that $x$ is an adherent point of $X$
      in $\adh{X}$, i.e. that for all $\epsilon > 0$,
      $B_{(\adh{X}, d_{\adh{X}})}(x, \epsilon) \cap X \neq \emptyset$. In other words, for
      all $\epsilon > 0$, we must show that there exists a
      $y \in X$ such that $d_{\adh{X}}(x,y) < \epsilon$.

      Thus, let be $\epsilon > 0$. By definition, $x$ is the formal limit of
      a Cauchy sequence $\seq{x_n}{1}$ of elements of $X$, so that
      $x := \formallimit x_n$. Since $\seq{x_n}{1}$ is a Cauchy
      sequence, there exists an $N \geq 1$ such that
      $d(x_n, x_N) < \epsilon/2$ whenever $n \geq N$. Thus:
      \begin{align*}
        d_{\adh{X}}(x, x_N)
        &:= d_{\adh{X}}(\formallimit x_n, \formallimit x_N) \\
        &= \lim_{n \to \infty} d(x_n, x_N) \\
        &\leq \epsilon/2 < \epsilon
      \end{align*}
      so that $y := x_N$ is a convenient choice. This shows that $x$
      is an adherent point of $X$ in $\adh{X}$, as expected.
      
    \item Finally, the formal limit agrees with the actual limit,
      i.e., $\lim_{n \to \infty} x_n = \formallimit x_n \in \adh{X}$ for all
      Cauchy sequence $\seq{x_n}{1}$ in $X$.

      Indeed, let be $\seq{x_n}{1}$ a Cauchy sequence of elements of
      $X$. We know that $(X,d)$ can be thought of as a subspace of
      $(\adh{X}, d_{\adh{X}})$, so that $\seq{x_n}{1}$ can be thought
      of as a sequence of elements of $\adh{X}$. But we have showed
      that $(\adh{X}, d_{\adh{X}})$ is complete. Thus, the sequence
      $\seq{x_n}{1}$ converges in $\adh{X}$ to a certain limit $L \in
      \adh{X}$; i.e., we have $\lim_{n \to \infty} x_n = L$ for some $L \in
      \adh{X}$.

      Consider this limit $L$. By definition of $\adh{X}$, there
      exists a Cauchy sequence $\seq{a_n}{1}$ of elements of $X$ such
      that $L := \formallimit a_n$. What we need to prove is that we
      have
      \begin{equation}
        \label{eq:12.4.8f}
        L = \lim_{n \to \infty} x_n = \formallimit a_n = \formallimit x_n
      \end{equation}
      and thus, it is sufficient to show that
      $\formallimit a_n = \formallimit x_n$, since we already have the
      other equalities. And, by definition of the equality relation
      established in (a), in order to prove that
      $\formallimit a_n = \formallimit x_n$, we just have to show that
      $\lim_{n \to \infty} d(x_n, a_n) = 0$. Or, in yet another equivalent
      way, we have to show that for all $\epsilon > 0$, there exists an $N \geq
      1$ such that $d(x_n, a_n) \leq \epsilon$ whenever $n \geq N$.

      Thus, let be an arbitrary $\epsilon > 0$. Let's unfold our hypotheses.
      \begin{itemize}
      \item We know that the sequence $\seq{x_n}{1}$ converges to $L$
        in $\adh{X}$. Thus, by definition, there exists a $N_1 \geq 1$
        such that $d_{\adh{X}}(x_k, L) \leq \epsilon/2$ whenever
        $k \geq N_1$. In other words,
        $\lim_{n \to \infty} d(x_k, a_n) \leq \epsilon/3 < \epsilon/2$ whenever $k \geq N_1$.

        Thus, there exists a $N_2$ such that
        $d(x_k, a_n) \leq \epsilon/2$ whenever $k \geq N_1$ and
        $n \geq N_2$ (see footnote \ref{ft.limsuites} p.
        \pageref{ft.limsuites} from the present document).
      \item We also know that $\seq{x_n}{1}$ is a Cauchy sequence. It
        means that there exists a $N_3 \geq 1$ such that
        $d(x_p, x_q) \leq \epsilon/2$ for all $p,q \geq N_3$.
      \end{itemize}
      Let be $N := \max(N_1, N_2, N_3)$. Using the triangle
      inequality, we finally get, for all $n \geq N$,
      \begin{align*}
        d(x_n, a_n) &\leq d(x_n, x_N) + d(x_N, a_n) \\
                    &\leq \epsilon/2 + \epsilon/2 \\
                    &\leq \epsilon
      \end{align*}
      This closes the proof.
    \end{enumerate}
\end{exo}

\begin{exo}{12.5.1}{Show that Definitions 9.1.22 and 12.5.3 match when
    talking about subsets of the real line with the standard metric.}

  Consider $Y \subseteq \rr$ and the standard metric $d(x,y) = |x-y|$ for all
  $x,y \in \rr$. We have to show that both definitions of boundedness
  are equivalent in this case.

  \begin{itemize}
  \item First, suppose that $Y$ is bounded in the sense of Definition
    12.5.3. Thus, there exists a real number $x$ and a positive real
    number $r > 0$ such that $Y \subseteq B(x,r)$. In other words, we have
    $Y \subseteq \, ]x-r, x+r[ \, \subseteq [x-r, x+r]$. Let be
    $M := |x| + |r|$. We clearly have $x+r \leq M$, and $-M \leq x-r$. Thus,
    we have $Y \subseteq [-M, M]$, and $Y$ is bounded in the sense of
    Definition 9.1.22.
  \item Conversely, suppose that $Y$ is bounded in the sense of
    Definition 9.1.22. Thus, there exists a positive real $M > 0$ such
    that $Y \subseteq [-M, M] \subset ]-2M, 2M[$. But this later interval is simply
    $B(0, 2M)$, so that $Y$ is bounded in the sense of Definition
    12.5.1, taking $x := 0$ and $r := 2M$.
  \end{itemize}
\end{exo}

\begin{exo}{12.5.2}{Prove Proposition 12.5.5.}

  We must prove that any compact space $(X,d)$ is both complete and
  bounded. In both cases, we will use a proof by contradiction.

  \begin{itemize}
  \item First, let's prove completeness. Suppose, for the sake of
    contradiction, that the compact space $(X,d)$ is not complete.
    Since it is not complete, there exists a Cauchy sequence
    $\seq{x^{(n)}}{1}$ of elements of $X$ which does not converge in
    $(X,d)$. But since it is compact, there exists a subsequence
    $(x^{(n_k)})_{k=1}^\infty$ of this Cauchy sequence, which converges in
    $(X,d)$. But, by Lemma 12.4.9, if a Cauchy sequence has a
    convergent subsequence, then it is convergent itself; thus
    $\seq{x^{(n)}}{1}$ converges. It is a clear contradiction. Thus,
    $(X,d)$ must be complete.
  \item Now we show boundedness. Similarly, suppose for the sake of
    contradiction that $(X,d)$ is not bounded. It means that, for all
    positive real $r>0$ and all $x \in X$, we have
    $X \not \subseteq B(x,r)$. In particular, for any positive natural number
    $n \geq 1$ and an arbitrary $x \in X$, the set
    $X \backslash B(x,n)$ is not empty. Thus, using the (countable) axiom of
    choice, we can build a sequence $\seq{x^{(n)}}{1}$ such that
    $x^{(n)} \in X \backslash B(x,n)$ for all positive integer
    $n \geq 1$. Or, in other words, we have $d(x, x^{(n)}) \geq n$ for all
    $n \geq 1$.

    But recall that $(X,d)$ is compact. Thus, there must exist a
    convergent subsequence $(x^{(n_k)})_{k=1}^\infty$ of the original
    sequence. Say that this subsequence converges to some value $L$.
    Thus, by definition,
    \[\forall \epsilon > 0, \exists K \geq 1 \, : \, k \geq K \implies d(x^{(n_k)}, L) \leq \epsilon.\]
    Let's take $\epsilon := 1$ (there is nothing special about this value;
    this is just any arbitrary $\epsilon$ to obtain a contradiction). There
    must exist a $K_{1} \geq 1$ such that $d(x^{(n_k)}, L) \leq 1$ whenever
    $k \geq K_{1}$. But, at the same time, we have by the triangle
    inequality
    \begin{align*}
      d(x^{(n_k)}, x) &\leq d(x^{(n_k)}, L) + d(L, x)\\
      \implies d(x^{(n_k)}, L) &\geq d(x^{(n_k)}, x) - d(L,x)
    \end{align*}

    For instance by the Archimedean principle, there exists an $N \in
    \nn$ such that $N \geq d(L,x) + 3$. Let be $K_2 := \min\{ k \in \nn :
    n_k \geq N\}$ (this natural number exists simply because $n_N \geq N$,
    so that the set is not empty). We thus have
    \[d(x, x^{(n_k)}) \geq n_k \geq N \geq d(L,x) + 3\]
    for all $k \geq K_2$.

    Thus, for all $k \geq \max(K_1, K_2)$, we have both $d(x^{(n_k)}, x)
    \leq 1$ (because $k \geq K_1$), and $d(x^{(n_k)}, L) \geq d(x^{(n_k)}, x) -
    d(L,x) \geq d(L,x) + 3 - d(L,x) \geq 3$ (because $k \geq K_2$). This is a
    contradiction. Thus, $(X,d)$ is bounded.
  \end{itemize}
\end{exo}

\begin{exo}{12.5.3}{Prove Theorem 12.5.7.}

  Let be $(\rr^{n}, d)$ an Euclidean space, where $d$ is either the
  Euclidean, taxicab or sup norm metric. Also, let be
  $E \subseteq \rr^{n}$. We have to prove that $E$ is compact iff $E$ is
  closed and bounded. By Corollary 12.5.6, we already know that if $E$
  is compact, then it is closed and bounded. We thus have to prove the
  converse implication.

  Suppose that $E$ is both closed and bounded. Since $E$ is a subset
  of $\rr^{n}$, we can write $E := E_{1} \times \ldots \times E_{n}$, where
  $E_{j} \subseteq \rr$ for all $1 \leq j \leq n$.

  We have to prove that any sequence $(x^{(k)})_{k=1}^{\infty}$ in $E$ has
  a convergent subsequence in $(E, d)$. This sequence can be written
  as a sequence of vectors of length $n$, i.e., we have
  $x^{(k)} = (x_{1}^{(k)}, \ldots, x_{n}^{(k)})$, where
  $x_{j}^{(k)} \in E_j$ for all $k \geq 1$ and all $1 \leq j \leq n$.

 We will first need a lemma:

  \smallskip
  \textbf{Lemma}. If $E$ is bounded, then each $E_j \subseteq \rr$ is also
  bounded.

  \begin{proof}[Sketch of proof]
    Suppose that $d$ is the sup norm metric. If $E$ is bounded, we
    have $E \subseteq B(x, r)$ for some $x \in \rr^{n}$ and some $r > 0$
    (Definition 12.5.3). In other words, we have $d(x, y) < r$ for all
    $y \in E$. Since $d$ is the sup norm metric, this implies that
    \[\forall j \in \llbracket 1, n \rrbracket, \,
      |x_{j} - y_{j}| \leq \max_{j = 1, \ldots, n} |x_{j} - y_{j}| := d(x,y) <
      r.\] Thus, $E_j \subseteq B(x_{j}, r)$, i.e. $E_{j}$ is bounded for all
    $1 \leq j \leq n$.

    The proof is similar if $d$ is the Euclidean metric, or the
    taxicab metric.
  \end{proof}

  Now we go back to the main proof. Since each sequence
  $(x_{j}^{(k)})_{k=1}^{\infty}$ is a sequence of real numbers in the
  bounded subset $E_{j} \subseteq \rr$, then by Theorem 9.1.24 this sequence
  has a convergent subsequence $(x_{j}^{(k_{l})})_{l=1}^{\infty}$, which
  converges to $L_{j} \in \rr_{j}$. But by Proposition 12.1.18, this
  implies that the whole subsequence $(x^{(k_{l})})_{l=1}^{\infty}$ converges
  to $(L_{1}, \ldots, L_{n})$ (since it converges component-wise).

  Thus, $(x_{j}^{(k)})_{k=1}^{\infty}$ indeed has a convergent subsequence,
  as expected; and $E$ is compact.
\end{exo}

\bigskip
\begin{exo}{12.5.4}{Let $(\rr, d)$ be the real line with the standard
    metric. Give an example of a continuous function $f : \rr \to
    \rr$, and an open set $V \subseteq \rr$, such that the image $f(V) :=
    \{f(x) : x \in V\}$ of $V$ is not open.}

  As a simple example, consider the constant function $f(x) = 0$
  defined on $V := ]-1, 1[$. The interval $V$ is clearly open, but we
  have $f(V) = \{0\}$. This singleton (or more generally, any
  singleton) is not open in $(\rr, d)$, since for all $r>0$, there
  always exists a real number $x$ such that $x \in B(0, r) \backslash \{0\}$.
\end{exo}

\bigskip
\begin{exo}{12.5.5}{Let $(\rr, d)$ be the real line with the standard
    metric. Give an example of a continuous function $f: \rr \to \rr$, and
    closed set $F \subseteq \rr$, such that $f(F)$ is not closed.}

  One can give the example of the function $\tan^{-1}(x)$ defined on
  the closed set $F := \rr$, but this function has not really been
  defined so far in the book. So, let's use a simpler example.
  
  Consider the closed set $F := [1, +\infty[$ and the function $f(x) =
  1/x$. We have $f(F) = ]0, 1]$, which is not a closed set.  
\end{exo}

\pagebreak
\begin{exo}{12.5.6}{Prove Corollary 12.5.9.}

  Consider a sequence $K_1 \supset K_2 \supset K_3 \supset \ldots$ of non-empty compact sets
  in a metric space $(X,d)$. We have to show that $\bigcap_{n=1}^\infty K_n \neq \emptyset$.

  Let's work in the space $(K_1, d_{K_1 \times K_1})$. We define the sets
  $V_n := K_1 \setminus K_n$ for all $n \geq 1$, i.e.,
  \begin{align*}
    V_1 &:= K_1 \setminus K_1 = \emptyset \\
    V_2 &:= K_1 \setminus K_2 \\
    V_3 &:= K_1 \setminus K_3 \\
        & \ldots
  \end{align*}
  so that the $V_n$ clearly constitute an increasing sequence:
  \[V_1 \subseteq V_2 \subseteq V_3 \subseteq \ldots,\]
  so that $\bigcup_{k = 1}^n V_k = V_n$ for all $n \geq 1$.
  
  Furthermore, each set $V_n$ is open in $(K_1, d_{K_1 \times K_1})$, since
  it is the complementary set of a compact (and then closed) set
  (Proposition 12.2.15 (e)).

  Suppose, for the sake of contradiction, that we have $\bigcap_{n=1}^\infty K_n
  = \emptyset$. We would thus have:
  \begin{align*}
    \bigcup_{n=1}^\infty V_n &= \bigcup_{n=1}^\infty (K_1 \setminus K_n) \\
                  &= K_1 \setminus \left(\bigcap_{n=1}^\infty K_n\right) \text{ (Exercise 3.4.11)}\\
                  &= K_1 \setminus \emptyset \text{ (by hypothesis)} \\
                  &= K_1.
  \end{align*}

  But since $K_1$ is compact, then by Theorem 12.5.8, there exists a
  finite open cover of $K_1$, i.e., there exists a finite number $k$
  of indices $n_1 < \ldots < n_k\}$ such that
  \[\bigcup_{n \in \{n_1, \ldots, n_k\}} V_n = K_1.\]

  But since the $V_n$ form an increasing sequence, this implies
  $V_{n_k} = K_1$, i.e., $K_1 \setminus K_{n_k} = K_1$, so that we finally get
  $K_{n_k} = \emptyset$.

  But all the sets $K_n$ were supposed to be non empty: this is thus a
  contradiction, and we must have $\bigcap_{n=1}^\infty K_n \neq \emptyset$.
\end{exo}

\bigskip
\begin{exo}{12.5.7}{Prove Theorem 12.5.10.}

  Let be $(X,d)$ a metric space.

  \begin{enumerate}[label=(\alph*)]
  \item Let be $Z \subseteq Y \subseteq X$, with $Y$ compact. We have to show that $Z$
    is closed iff it is compact. We already know that if $Z$ is
    compact, then it is closed (Corollary 12.5.6); so that we just
    have to show the converse implication.

    Suppose that $Z$ is closed, and let be $(z^{(n)})_{n=1}^{\infty}$ a
    sequence of elements of $Z$. Since $Z \subseteq Y$, $(z^{(n)})_{n=1}^{\infty}$
    is also a sequence of elements of $Y$; and since $Y$ is compact,
    there exists a subsequence $(z^{(n_{k})})_{k=1}^{\infty}$ that
    converges to some $z \in Y$. But since $Z$ is closed, we must have
    $z \in Z$ (by Proposition 12.2.15(b)). Thus, any sequence of
    elements of $Z$ has a subsequence that converges in $Z$, i.e., $Z$
    is indeed compact.
    
  \item Let be $Y_1, \ldots, Y_n$ be $n$ compact subsets of $X$; we have to
    show that the finite union $Y_1 \cup \ldots \cup Y_n$ is compact. Let's
    use the topological characterization of compact sets: suppose that
    we have an open cover $\bigcup_{\alpha \in I} V_\alpha$ (possibly uncountable), i.e.
    that
    \[Y_1 \cup \ldots \cup Y_n \subseteq \bigcup_{\alpha \in I} V_\alpha.\]

    Clearly, we have $Y_1 \subseteq  \bigcup_{\alpha \in I} V_\alpha$, and since $V_1$ is
    compact, there exists a finite open cover, i.e. $Y_1 \subseteq  \bigcup_{i =
      1}^{s_1} V_{a_i}$. Similarly, there exist finite open covers for
    each other subset $Y_i$, i.e.,
    \begin{align*}
      Y_2 &\subseteq  \bigcup_{i = 1}^{s_2} V_{b_i} \\
          & \ldots \\
      Y_n &\subseteq \bigcup_{i = 1}^{s_n} V_{n_i}.
    \end{align*}
    Thus, there exists a finite open cover
    \[Y_1 \cup \ldots \cup Y_n \subseteq \bigcup_{\alpha \in \{a_1, \ldots, a_{s_1}, b_1, \ldots, b_{s_2}, \ldots,
        n_1, \ldots, n_{s_n}\}} V_\alpha\]
    so that $Y_1 \cup \ldots \cup Y_n$ is indeed compact.
    
  \item Let be $Y$ a finite subset of $X$; we have to show that $Y$ is
    compact.

    First, suppose that $Y$ is a singleton $\{a\}$. By definition, any
    sequence of elements of $Y$ can only be the constant sequence $a,
    a, a, \ldots$. Thus, any subsequence of this sequence is still the
    constant sequence $a, a, \ldots$, and still converges to $a$. Thus, any
    sequence of elements of $Y$ has a subsequence that converges in
    $Y$, i.e., $Y$ is compact.

    Now suppose that $Y$ is a finite subset of cardinality $n$. Let's
    write $Y := \{y_1, \ldots, y_n\}$. This can also be written
    $Y := \{y_1\} \cup \ldots \cup \{y_n\}$, so that we are back in the previous
    case (b): $Y$ is the finite union of compact subsets of $X$. Thus,
    $Y$ is itself compact.

    Note that for the limit case $Y = \emptyset$, we can say that the empty
    set is just a closed\footnote{See Remark 12.2.14.} subset of the
    compact set $\{a\}$, so that by the previous case (a), $Y = \emptyset$ is
    compact.
  \end{enumerate}
\end{exo}

\begin{exo}{12.5.8}{Let $(X, d_{l^1})$ be the metric space from
    Exercise 12.1.15. For each natural number $n$, let
    $e^{(n)} = (e^{(n)}_j)_{j=0}^\infty$ be the sequence in $X$ such that
    $e^{(n)}_j := 1$ when $n = j$ and $e^{(n)}_j := 0$ when
    $n \neq j$. Show that the set $\{e^{(n)} : n \in \nn\}$ is a closed and
    bounded subset of $X$, but is not compact.}

  Recall that $(X,d_{l^1})$ is the metric space of absolutely
  convergent sequences, with the metric defined by
  $d_{l^1}((a^{(n)}), (b^{(n)})) := \sum_{n=0}^\infty |a_n - b_n|$. Hereafter,
  we denote $E := \{e^{(n)} : n \in \nn\}$, with
  \begin{align*}
    e^{(0)} &:= 1, 0, 0, 0, \ldots \\
    e^{(1)} &:= 0, 1, 0, 0, \ldots \\
    e^{(2)} &:= 0, 0, 1, 0, \ldots \\
    &\ldots
  \end{align*}

  \begin{itemize}
  \item First, we show that $E$ is not compact. To prove this
    statement, we just have to find one sequence of elements of $E$
    that has no convergent subsequence in $E$.

    Consider the ``canonical'' sequence of elements of $E$ defined by
    $e^{(0)}, e^{(1)}, e^{(2)}, \ldots$. The distance between any two
    distinct elements of this sequence is
    \[d_{l^{(1)}}(e^{(j)}, e^{(k)}) := \sum_{i=0}^\infty |e_i^{(j)} -
      e_i^{(k)}| = 2 > 1.\]

    Thus, this sequence is not a Cauchy sequence itself, and it is
    clear that no subsequence can be a Cauchy sequence either. Thus,
    no subsequence of this sequence can converge in $E$, i.e., $E$ is
    not compact.
    
  \item However, $E$ is a closed subset of $X$. To prove this
    property, consider a convergent sequence of elements of $E$; we
    have to prove that its limit lies in $E$. We've just shown that
    the distance between any two distinct terms $e^{(j)}, e^{(k)}$ for
    $j \neq k$ is equal to $2$. Thus, if a sequence of elements of $E$
    converges, it must be eventually $0.5$-stable, and the only
    possibility for that is to be eventually constant. In other words,
    it must be eventually equal to $e^{(n_0)}$ for $n_0 \in \nn$, so
    that it necessarily converges to $e^{(n_0)}$, which is an element
    of $E$. This shows that $E$ is closed.
    
  \item Furthermore, $E$ is bounded. To show the boundedness of $E$,
    we have to show that $E \subseteq B_{(X, d_{l^1})}((x_j)_{j=0}^\infty, r)$
    for some $r > 0$ and some sequence $(x_j)_{j=0}^\infty \in X$.
    Consider the zero sequence $(z_j)_{j=0}^\infty := 0, 0, 0, \ldots$. This
    is clearly a sequence in $X$ (since it converges to $0$), and we
    have
    \[d_{l^1}\left((z_j)_{j=0}^\infty, (e^{(n)}_j)_{j=0}^\infty\right)
      = \sum_{j=0}^\infty |z_j - e^{(n)}_j| = 1 < 2\]
    for all $n \in \nn$. Thus, we have $E \subseteq B_{(X,
      d_{l^1})}((z_j)_{j=0}^\infty, 2)$, which shows that $E$ is bounded.
  \end{itemize}

  Thus, the case of the subset $E$ of the metric space $(X, d_{l^1})$
  shows that the Heine-Borel theorem (stated for the metric space
  $(\rr^n, d)$) is not valid in more general metric spaces.
\end{exo}

\bigskip
\begin{exo}{12.5.9}{Show that a metric space $(X, d)$ is compact if
    and only if every sequence in $X$ has at least one limit point.}

  A metric space $(X,d)$ is compact iff any sequence of elements of
  $X$ has a subsequence that converges in $(X,d)$. Thus, the statement
  is a direct consequence of Proposition 12.4.5, which says basically
  that ``having a convergent subsequence'' and ``having a limit
  point'' are synonymous.
\end{exo}

\bigskip
\begin{exo}{12.5.13}{Let $E$ and $F$ be two compact subsets of $\rr$
    (with the standard metric $d(x, y) = |x - y|$). Show that the
    Cartesian product $E \times F := \{(x, y) : x \in E, y \in F \}$ is a
    compact subset of $\rr^2$ (with the Euclidean metric $d_{l^2}$).}

  To prove that $E \times F$ is compact, we will show that it is both
  closed and bounded (by Heine-Borel theorem).

  \begin{itemize}
  \item First we show that $E \times F$ is bounded.

    Since $E$ and $F$ are compact, they are themselves bounded (by
    Heine-Borel theorem). Thus, there exist $a \in E$, $b \in F$ and $r_1,
    r_2 > 0$ such that $E \subseteq B_d(a, r_1)$ and $F \subseteq B_d(b, r_2)$, by
    Definition 12.5.3. In other words, we have:
    \begin{align*}
      \forall x \in E, & |x-a| < r_1 \\
      \forall y \in F, & |y-b| < r_2.
    \end{align*}
    Thus, let be $(x, y) \in E \times F$. We have:
    \begin{align*}
      d_{l^2}\left( (x,y), (a,b) \right)
      &= \sqrt{(x-a)^2 + (y-b)^2} \\
      &< \sqrt{r_1^2 + r_2^2}.
    \end{align*}
    This means that each $(x,y) \in E \times F$ lies in
    $B_{d_{l^2}}\left((a,b), \sqrt{r_1^2 + r_2^2}\right)$. Thus,
    $E \times F$ is indeed bounded.
    
  \item Now let's show that $E \times F$ is closed.

    Since $E$ and $F$ are compact, they are themselves closed (by
    Heine-Borel theorem). Consider a sequence
    $\seq{(x^{(n)}, y^{(n)})}{1}$ of elements of $E \times F$ which
    converges to $(x_0, y_0)$ with respect to $d_{l^2}$. By
    Proposition 12.1.18, this means that this sequence converges
    component-wise, i.e. that $\seq{x^{(n)}}{1}$ converges to $x_0$,
    and $\seq{y^{(n)}}{1}$ converges to $y_0$. By definition, we have
    $x_0 \in E$ and $y_0 \in F$, since $E$ and $F$ are closed. Thus,
    $(x_0, y_0) \in E \times F$. This shows that $E \times F$ is indeed bounded.
  \end{itemize}

  Thus, $E \times F$ is compact, as expected.
\end{exo}

\pagebreak
\section{Continuous functions on metric spaces}
\label{cha:cont-funct-metr}

\begin{exo}{13.1.1}{Prove Theorem 13.1.4.}

  Since the implication $(b) \implies (c)$ may be slightly more
  difficult to write, we will prove the implications $(a) \implies
  (c)$, $(c) \implies (b)$ and $(b) \implies (a)$ in this order.

  Let be $f : (X, d_X) \to (Y, d_Y)$, and $x_0 \in X$.

  \begin{itemize}
  \item First let's prove $(a) \implies (c)$. Suppose that $f$ is
    continuous at $x_0$, and let be $V \subseteq Y$ an open set that contains
    $f(x_0)$. By Proposition 12.2.15(a), there exists a $\epsilon > 0$ such
    that $B_Y(f(x_0), \epsilon) \subseteq V$. But since $f$ is continuous at
    $x_0$, we know that there exists a $\delta > 0$ such that
    $d_X(x, x_0) < \delta \implies d_Y(f(x), f(x_0)) < \epsilon$. Thus, if we set
    $U := B_X(x_0, \delta)$, we have found an open set $U \subseteq X$ such that
    $f(U) \subseteq B_Y(f(x_0), \epsilon) \subseteq V$, as required.
  \item Now we prove $(c) \implies (b)$. Consider a sequence
    $\seq{x^{(n)}}{1}$ in $X$ which converges to $x_0$ with respect to
    $d_X$. Let be an arbitrary $\epsilon > 0$; we set
    $V_\epsilon := B_Y(f(x_0), \epsilon)$. By $(c)$, we know that there exists an open
    set $U \subseteq X$ containing $x_0$ and such that $f(U) \subseteq V_\epsilon$. But since
    $U$ is open set, by Proposition 12.2.15(a), there exists a $\delta > 0$
    such that $B_X(x_0, \delta) \subseteq U$.

    Since $\seq{x^{(n)}}{1}$ converges to $x_0$, there exists a
    natural number $N \geq 1$ such that $d_X(x^{(n)}, x_0) < \delta$ whenever
    $n \geq N$. Or, in other words, we have $x^{(n)} \in B_X(x_0, \delta) \subseteq U$
    whenever $n \geq N$.

    But since $f(U) \subseteq V$ by hypothesis, we thus have $f(x^{(n)}) \in V_\epsilon$
    whenever $n \geq N$. Since this is true for any arbitrary $\epsilon > 0$,
    this shows that the sequence $\seq{f(x^{(n)})}{1}$ converges to
    $f(x_0)$ with respect to $d_Y$, as expected.
    
  \item Finally, we prove $(b) \implies (a)$. Suppose that
    $\seq{f(x^{(n)})}{1}$ converges to $f(x_0)$ whenever
    $\seq{x^{(n)}}{1}$ converges to $x_0$, and let's show that $f$ is
    continuous at $x_0$.

    Suppose, for the sake of contradiction, that $f$ is \emph{not}
    continuous at $x_0$. Thus, there exists an $\epsilon > 0$ such that for
    all $\delta > 0$, there exists an $x \in X$ such that
    $d_Y(f(x), f(x_0)) \geq \epsilon$ although $d_X(x, x_0) < \delta$.

    Thus, using the (countable) axiom of choice, we build a sequence
    $\seq{x^{(n)}}{1}$ such that, for all $n \geq 1$, we have
    $d_Y(f(x^{(n)}), f(x_0)) \geq \epsilon$ although
    $d_X(x^{(n)}, x_0) < \frac{1}{n}$. It is thus clear that
    $\seq{x^{(n)}}{1}$ converges to $x_0$, but that
    $\seq{f(x^{(n)})}{1}$ does not converge to $f(x_0)$, since
    $f(x^{(n)})$ and $f(x_0)$ are never $\epsilon/2$-close. This is a
    contradiction with $(c)$. Thus, $f$ must be continuous at $x_0$,
    as expected.
  \end{itemize}  
\end{exo}

\begin{exo}{13.1.2}{Prove Theorem 13.1.5.}

  We already know from Theorem 13.1.4 that $(a)$ and $(b)$ are
  equivalent. Let's prove the other implications.

  \begin{itemize}
  \item First we prove that $(a) \implies (c)$. Let be $V$ an open set
    in $Y$. We must show that $f^{-1}(V)$ is an open set in $X$. Thus,
    if we take an arbitrary $x_0 \in f^{-1}(V)$, we must show that there
    exists an $r_0 > 0$ such that $B_X(x_0, r_0) \subseteq f^{-1}(V)$ (cf.
    Theorem 12.2.15(a)).

    Consider this arbitrary $x_0 \in f^{-1}(V)$. By definition, we have
    $f(x_0) \in V$. But since $V$ is an open set, there exists an $\epsilon >
    0$ such that $B_Y(f(x_0), \epsilon) \subseteq V$.

    But $f$ is continuous: for this $\epsilon > 0$, there exists a
    $\delta > 0$ such that, for $x \in X$, we have
    $d_X(x_0, x) < \delta \implies d_Y(f(x_0), f(x)) < \epsilon$. In other words,
    we have $x \in B_X(x_0, \delta) \implies f(x) \in B_Y(f(x_0), \epsilon) \subseteq V$.

    Thus, if we set $r_0 := \delta$, we are done: for all $x \in B_X(x_0,
    r_0)$, we have $f(x) \in V$, i.e. $x \in f^{-1}(V)$. This shows that
    $B_X(x_0, \delta) \subseteq f^{-1}(V)$, and thus that $f^{-1}(V)$ is an open
    set, as expected.
  \item Now we show that $(c) \implies (d)$. By Theorem 12.2.15(e), we
    know that $F \subseteq X$ is closed iff $X \setminus F$ is open. Thus, consider
    $F \subseteq Y$ a closed set in $Y$. Let be $V := Y \setminus F$ its complementary
    set, which is thus an open set. By $(c)$, the set $f^{-1}(V)$ is
    an open set in $X$. But we have :
    \begin{align*}
      f^{-1}(F) &= \{x \in X : f(x) \in F\} \\
                &= \{x \in X : f(x) \in Y \setminus V\} \\
                &= \{x \in X : f(x) \notin V\}
    \end{align*}
    so that $f^{-1}(F) = X \setminus f^{-1}(V)$. Since $f^{-1}(F)$ is the
    complementary set of the open set $f^{-1}(V)$, it is closed in $X$,
    as expected.
  \item The implication $(d) \implies (c)$ can be shown in exactly the
    same way as above.
  \item Finally, let's show that $(c) \implies (a)$. Let be $\epsilon > 0$,
    let be $x_0 \in X$. Consider $V := B_Y(f(x_0), \epsilon)$, which is an open
    set in $Y$. By $(c)$, the set $f^{-1}(V)$ is open in $X$. Thus, by
    Theorem 12.2.15(a), there exists a $\delta > 0$ such that $B_X(x_0, \delta)
    \subseteq f^{-1}(V)$. Thus, if $x \in B_X(x_0, \delta)$, we have $f(x) \in V$.

    In other words, for any $\epsilon > 0$, there exists a $\delta > 0$ such that
    $d_X(x, x_0) < \delta \implies d_Y(f(x), f(x0)) < \epsilon$. This shows that
    $f$ is continuous at $x_0$, for any arbitrary $x_0 \in X$, as
    expected.
  \end{itemize}
\end{exo}

\begin{exo}{13.1.3}{Use Theorem 13.1.4 and Theorem 13.1.5 to prove
    Corollary 13.1.7.}
  
  To show (a), consider $\seq{x^{(n)}}{1}$ a sequence of elements of
  $X$ that converges to $x_0 \in X$. Since $f$ is continuous at
  $x_0$, then by Theorem 13.1.4(b), we know that
  $\seq{f(x^{(n)})}{1}$ converges to $f(x_0) \in Y$. But
  $\seq{f(x^{(n)})}{1}$ is a sequence of elements of $Y$. Since $g$
  is continuous at $f(x_0)$, then still by Theorem 13.1.4(b), we
  know that $\seq{g(f(x^{(n)}))}{1}$ converges to $g(f(x_0)) \in Z$.
  
  Thus, we have proved that for any sequence $\seq{x^{(n)}}{1}$ of
  elements of $X$ that converges to $x_0 \in X$, the sequence $\seq{g
    \circ f(x^{(n)})}{1}$ converges to $g \circ f(x_0)$. This shows that $g \circ
  f$ is continuous at $x_0$, as expected.
  
  Once (a) is proved, the result (b) is clear, since it is just (a)
  at any arbitrary $x_0 \in X$.
\end{exo}

\bigskip
\begin{exo}{13.1.4}{Give an example of functions $f : \rr \to \rr$ and
    $g : \rr \to \rr$ such that (a) $f$ is not continuous, but $g$ and
    $g \circ f$ are continuous; (b) $g$ is not continuous, but $f$ and
    $g \circ f$ are continuous; (c) $f$ and $g$ are not continuous, but
    $g \circ f$ is continuous. Explain brieﬂy why these examples do not
    contradict Corollary 13.1.7.}

  Here, the simplest way is to use piecewise constant functions, at
  least for one of the functions $f,g$.

  \begin{enumerate}[label=(\alph*)]
  \item Let be, for instance,
    \[f(x) :=
      \begin{cases}
        0  &\text{ if } x < 0 \\
        1 & \text{ if } x \geq 0
      \end{cases}
    \]
    and the constant function $g(x) := 3$. We thus have $g \circ f(x) = 3$
    for all $x \in \rr$, so that $g \circ f$ is continuous.
  \item Let be, for instance,
    \[g(x) :=
      \begin{cases}
        -1  &\text{ if } x < 0 \\
        1 & \text{ if } x \geq 0
      \end{cases}
    \]
    and $f(x) := x^2 + 1$. We thus have $g \circ f(x) = 1$ for all
    $x \in \rr$, so that $g \circ f$ is continuous.
  \item Let be, for instance,
    \[f(x) :=
      \begin{cases}
        0  &\text{ if } x < 0 \\
        3 & \text{ if } x \geq 0
      \end{cases}
    \]
    and
    \[g(x) :=
      \begin{cases}
        -1  &\text{ if } x < 0 \\
        1 & \text{ if } x \geq 0.
      \end{cases}
    \]
    We thus have $g \circ f(x) = 1$ for all $x \in \rr$, so that
    $g \circ f$ is continuous.
  \end{enumerate}

  This does not contradict Corollary 13.1.7, since the initial
  hypothesis of this corollary is that both functions $f,g$ are
  continuous, and it says nothing about non discontinuous functions.
\end{exo}

\bigskip
\begin{exo}{13.1.5}{Let $(X, d)$ be a metric space, and let
    $(E, d|_{E \times E})$ be a subspace of $(X, d)$. Let
    $\iota_{E \to X} : E \to X$ be the inclusion map, defined by setting
    $\iota_{E \to X} (x) := x$ for all $x \in E$. Show that
    $\iota_{E \to X}$ is continuous.}

  Let be $x_0 \in E$ an arbitrary point in $E$, and let be $\epsilon > 0$ a
  positive real number. Note that we have, for all $x \in E$, 
  \[d(\iota_{E \to X}(x_0), \iota_{E \to X}(x)) = d_{E \times E}(x_0, x).\]

  Thus, if we take $\delta := \epsilon$ in Definition 13.1.1 of continuity, we are
  done: if $d_{E \times E}(x_0, x) < \epsilon$, we automatically have $d(\iota_{E \to
    X}(x_0), \iota_{E \to X}(x)) < \epsilon$, so that $\iota_{E \to X}$ is continuous at
  any arbitrary $x_0 \in E$, as expected.
\end{exo}

\bigskip
\begin{exo}{13.1.6}{Let $f : X \to Y$ be a function from one metric
    space $(X, d_X)$ to another $(Y, d_Y)$. Let $E$ be a subset of $X$
    (which we give the induced metric $d_X|_{E \times E}$), and let
    $f|_E : E \to Y$ be the restriction of $f$ to $E$, thus
    $f|_E (x) := f (x)$ when $x \in E$. If $x_0 \in E$ and $f$ is
    continuous at $x_0$, show that $f|_E$ is also continuous at $x_0$.
    (Is the converse of this statement true? Explain.) Conclude that
    if $f$ is continuous, then $f|_E$ is continuous.}

  Let's use Exercise 13.1.5. First we note that we have
  $f|_E = f \circ \iota_{E \to X}$. Indeed, $f \circ \iota_{E \to X}$ is a function from
  $E$ to $Y$ just like $f$, and for all $x \in E$, we clearly have
  $f \circ \iota{E \to X} (x) = f(x) = f|_E(x)$.

  We have shown in Exercise 13.1.5 that $\iota_{E \to X}$ is continuous at
  any $x_0 \in E$, and $f$ is supposed to be continuous at $x_0 \in E$.
  Thus, by Corollary 13.1.7, $f|_E$ is continuous at $x_0$ since it is
  the composition of two continuous functions. Since this is true for
  any arbitrary $x_0 \in E$, the function $f|_E$ is continuous on $E$.

  The converse statement is not true: consider the piecewise constant
  function $f : \rr \to \rr$ defined by $f(x) = -1$ if $x < 0$ and
  $f(x) = 1$ if $x \geq 0$ for all $x \in \rr$; and let be
  $E := [0, +\infty[$. The restriction $f|_E$ is clearly continuous (as a
  constant function) at $0$, but the function $f$ itself is clearly
  not continuous at $0$.
\end{exo}

\bigskip
\begin{exo}{13.2.1}{Prove Lemma 13.2.1.}

  Here we just have to prove the statement (a), since the statement
  (b) is essentially (a) applied to any arbitrary $x_0 \in X$.

  \begin{itemize}
  \item First suppose that $f$ and $g$ are both continuous at
    $x_0 \in X$, and let be $\seq{x{^{(n)}}}{1}$ a sequence of elements
      of $X$. Then, by Theorem 13.1.4(b), the sequence
      $\seq{f(x^{(n)})}{1}$ converges to $f(x_0)$ in $\rr$, and the
      sequence $\seq{g(x^{(n)})}{1}$ converges to $g(x_0)$ in $\rr$.

      Thus, by Theorem 12.1.18, the sequence
      $\seq{(f(x^{(n)}), g(x^{(n)}))}{1}$ of elements of $\rr^2$
      converges to $(f(x_0), g(x_0))$ in $\rr^2$ with respect to the
      metric $d_{l^2}$ (since it converges component-wise). In other
      words, for any arbitrary sequence $\seq{x^{(n)}}{1}$ that
      converges to $x_0$, the sequence $\seq{f \oplus g(x^{(n)})}{1}$
      converges to $(f(x_0), g(x_0)) = f \oplus g (x_0)$. Thus,
      $f \oplus g$ is continuous at $x_0$, by Theorem 13.1.4.
      
    \item Conversely, if $f \oplus g$ is continuous at $x_0$, then for any
      sequence $\seq{x^{(n)}}{1}$ of elements of $X$, the sequence
      $\seq{f \oplus g(x^{(n)})}{1}$ converges to $(f(x_0), g(x_0))$, by
      Theorem 13.1.4. Thus, by Theorem 12.1.18, $\seq{f(x^{(n)})}{1}$
      converges to $f(x_0)$ in $\rr$, and $\seq{g(x^{(n)})}{1}$
      converges to $g(x_0)$ in $\rr$. Thus, $f$ and $g$ are both
      continuous at $x_0$.
  \end{itemize}
\end{exo}

\begin{exo}{13.2.2}{Prove Lemma 13.2.2.}

  First we prove that the addition function, $f : (x,y) \mapsto x+y$,
  defined from $\rr^2$ to $\rr$, is continuous. Consider a sequence
  $\seq{x_n, y_n}{1}$ of elements of $\rr^2$, that converges to
  $(x_0, y_0)$ with respect to the metric $d_{l^2}$. In particular,
  $\seq{x_n}{1}$ and $\seq{y_n}{1}$ are both sequences of real
  numbers, and we know from Proposition 12.1.8 that $\seq{x_n}{1}$
  converges to $x_0$, and $\seq{y_n}{1}$ converges to $y_0$. But we
  have $f(x_n, y_n) = x_n + y_n$, and we know by the limit laws
  (Proposition 6.1.19(a)) that
  $\lim_{n \to \infty} (x_n + y_n) = x_0 + y_0 =: f(x_0, y_0)$. Thus, for any
  sequence $\seq{(x_n, y_n)}{1}$ of elements of $\rr^2$ which
  converges to $(x_0, y_0)$, we have proved that
  $\seq{f(x_n, y_n)}{1}$ converges to $f(x_0, y_0)$. Thus, the
  addition function $f$ is continuous at any $(x_0, y_0) \in \rr^2$, and
  thus is continuous on $\rr^2$.

  A similar proof applies for the substraction function, the
  multiplication function, and the maximum and minimum functions.

  An additional precaution is required for the division function,
  $f : (x,y) \mapsto x/y$ defined from $\rr \times \rr \setminus\{0\}$ to
  $\rr$. Let be $(x_0, y_0) \in \rr \times \rr \setminus\{0\}$, and let be
  $\seq{x_n, y_n}{1}$ a sequence of elements in
  $\rr \times \rr \setminus\{0\}$ which converges to $(x_0, y_0)$. In the proof
  above, we can indeed apply Proposition 6.1.19(f) since $y_n \neq 0$ for
  all $n \geq 1$.  
\end{exo}

\bigskip
\begin{exo}{13.2.3}{Show that if $f : X \to \rr$ is a continuous
    function, so is the function $|f| : X \to \rr$ deﬁned by
    $|f|(x) := |f(x)|$.}

  The function $|f|$ is the composition of the functions $f$ and
  $x \mapsto |x|$. We already know (from Proposition 9.4.12) that the
  function $x \mapsto |x|$ is continuous on $\rr$ (basically because
  $|x| := \max(x, -x)$, and thus is the composition of two continuous
  functions). Thus, by Corollary 13.1.7, $|f|$ is continuous on $X$.  
\end{exo}

\bigskip
\begin{exo}{13.2.4}{Let $\pi_1 : \rr^2 \to \rr$ and
    $\pi_2 : \rr^2 \to \rr$ be the functions $\pi_1(x, y) := x$ and
    $\pi_2(x, y) := y$. Show that $\pi_1$ and $\pi_2$ are continuous.
    Conclude that if $f : \rr \to X$ is any continuous function into a
    metric space $(X, d)$, then the functions $g_1 : \rr^2 \to X$ and
    $g_2 : \rr^2 \to X$ deﬁned by $g_1(x, y) := f(x)$ and
    $g_2(x, y) := f(y)$ are also continuous.}

  Let be a sequence of elements of $\rr^2$, that we will note as
  $\seq{(x_n, y_n)}{1}$, and suppose that this sequence converges to
  $(x_0, y_0) \in \rr^2$. By Proposition 12.1.18, we know that this
  sequence converges component-wise, i.e. that the sequence of real
  numbers $\seq{x_n}{1}$ converges to $x_0$, and the sequence of real
  numbers $\seq{y_n}{1}$ converges to $y_0$. But we have
  $x_n = \pi_1(x_n, y_n)$, $x_0 = \pi_1(x_0, y_0)$,
  $y_n = \pi_2(x_n, y_n)$, and $y_0 = \pi_2(x_, y_0)$. To summarise, for
  any sequence $\seq{(x_n, y_n)}{1}$ of elements of $\rr^2$ that
  converges to $(x_0, y_0)$, the sequence $\pi_1(x_n, y_n)$ converges to
  $\pi_1(x_0, y_0)$, so that $\pi_1$ is continuous. A similar argument
  shows that $\pi_2$ is continuous.

  Furthermore, the function $g_1$ defined above can be expressed as
  $g_1 := f \circ \pi_1$, and is thus continuous as the composition of two
  continuous functions (Corollary 13.1.7). Similarly,
  $g_2 = f \circ \pi_2$ is continuous.
\end{exo}

\bigskip
\begin{exo}{13.2.5}{Let $n, m \geq 0$ be integers. Suppose that for
    every $0 \leq i \leq n$ and $0 \leq j \leq m$ we have a real number
    $c_{ij}$. Form the function $P : \rr^2 \to \rr$ defined by
    \[ P(x,y) := \sum_{i=0}^n \sum_{j=0}^m c_{ij} x^i y^j.\] Show that
    $P$ is continuous. Conclude that if $f : X \to \rr$ and
    $g : X \to \rr$ are continuous functions, then the function
    $P(f, g) : X \to \rr$ defined by $P(f, g)(x) := P (f(x), g(x))$ is
    also continuous.}

  By Exercise 13.2.4, we know that if $f : \rr \to \rr$ is continuous,
  then the function $f \circ \pi_1 : \rr^2 \to \rr$ is also continuous. It is
  clear that the function $f : x \mapsto x^i$ is continuous for all integer
  $i \geq 0$ (as a product of $i$ continuous functions: apply Corollary
  13.2.3 $i$ times to the identity function $x \mapsto x$). Thus, the
  function $f_1 = f \circ \pi_1$ defined by $f_1(x,y) = x^i$ is continuous
  on $\rr^2$. Similarly, the function $f_2(x,y) = y^j$ is continuous
  on $\rr^2$ for all $j \geq 0$.

  By Corollary 13.2.3 another time, the function
  $(x,y) \mapsto x^i y^j$ is thus continuous (since it is the product of two
  continuous functions), and still by Corollary 13.2.3, the function
  $(x, y) \mapsto c_{ij} x^i y^j$ is also continuous, for all real constant
  $c_{ij}$. And thus, $P(x,y)$ is also continuous as a sum of
  continuous functions.

  Now consider the function $H := P(f,g)$, such that
  $H(x) := P(f(x), g(x))$. We have $H = P \circ (f \oplus g)$. But we have just
  showed that $P$ is continuous, and we already know that $f \oplus g$ is
  continuous whenever $f$ and $g$ are continuous (Lemma 2.2.1). Thus,
  $H$ continuous (as the composition of two continuous functions), as
  expected.
\end{exo}

\bigskip
\begin{exo}{13.2.6}{Let $\rr^m$ and $\rr^n$ be Euclidean spaces. If
    $f : X \to \rr^m$ and $g : X \to \rr^n$ are continuous functions, show
    that $f \oplus g : X \to \rr^{m+n}$ is also continuous, where we have
    identified $\rr^m \times \rr^n$ with $\rr^{m+n}$ in the obvious manner.
    Is the converse statement true?}

  This exercise generalizes the result of Lemma 13.2.1. The proof will
  thus be very close to the approach adopted in Exercise 13.2.1.

  \begin{itemize}
  \item Let be an arbitrary $x_0 \in X$, and let's show that
    $f \oplus g$ is continuous at $x_0$.

    Let be $\seq{x^{(n)}}{1}$ a sequence of elements of $X$ that
    converges to $x_0$. We will use below the following notations:
    \begin{align*}
      f(x_0) &:= (x_1, \ldots, x_m) \\
      g(x_0) &:= (y_1, \ldots, y_m) \\
      f(x^{(k)}) &:= (x^{(k)}_1, \ldots, x^{(k)}_m) \text{ for all } k \geq 1 \\
      g(x^{(k)}) &:= (y^{(k)}_1, \ldots, y^{(k)}_n) \text{ for all } k \geq 1  
    \end{align*}

    First, note that by Proposition 12.1.18, the sequence
    $\seq{x^{(n)}}{1}$ converges component-wise, i.e., we have
    $\lim_{k \to \infty} x^{(k)}_p = x_p$ for all $1 \leq p \leq m$.

    Since $f$ is continuous, the sequence $\seq{f(x^{(n)})}{1}$
    converges to $f(x_0) \in \rr^m$. Similarly for $g$, the sequence
    $\seq{g(x^{(n)})}{1}$ converges to $g(x_0) \in \rr^n$.

    By definition, we have
    $f \oplus g (x_0) = (x_1, \ldots, x_m, y_1, \ldots, y_n) \in \rr^{m+n}$.

    But we also have, for all $k \geq 1$,
    \begin{align*}
      f \oplus g (x^{(k)})
      &:= \left(f(x^{(k)}), g(x^{(k)})\right) \\
      &= \left(x^{(k)}_1, \ldots, x^{(k)}_m, y^{(k)}_1, \ldots, y^{(k)}_n\right) 
    \end{align*}

    And since we already know that this sequence converges
    component-wise, we have by Proposition 12.1.18
    \[\lim_{k \to \infty} f \oplus g (x^{(k)}) = (x_1, \ldots, x_m, y_1, \ldots, y_m) =: f \oplus
      g (x_0).\]

    Thus, $f \oplus g$ is continuous at $x_0$. And since this is true for
    any arbitrary $x_0 \in X$, $f \oplus g$ is continuous on $X$.
    
  \item The converse statement is also true; this can also be proved
    in a very similar fashion as Exercise 13.2.1.
  \end{itemize}
\end{exo}

\begin{exo}{13.2.7}{Let $k \geq 1$, let $I$ be a ﬁnite subset of $\nn^k$,
    and let $c : I \to \rr$ be a function. Form the function $P : \rr^k
    \to \rr$ defined by
    \[P (x_1 , \ldots , x_k) := \sum_{(i_1, \ldots, i_k) \in I} c(i_1 , \ldots , i_k)
      x_1^{i_1} \ldots x_k^{i_k}.\] Show that $P$ is continuous.}

  Let's use induction on $k$.

  \begin{itemize}
  \item First, if $k=1$, we have $I \subset \nn$, and $P$ is simply of the
    form $P(x) := \sum_{i \in I} c_i x^i$. Such a function is clearly
    continuous by Lemma 13.2.2, as the product and sum of continuous
    functions.
  \item We can also note that, for $k=2$, the result corresponds
    exactly to Exercise 13.2.5.
  \item Now suppose that the property is true for a given positive
    integer $k$, and let's show that it is still true for $k+1$.

    Let be $I$ a finite subset of $\nn^{k+1}$, and a function $P(x_1,
    \ldots, x_k, x_{k+1})$ as defined above. Note that (for example by
    Corollary 3.6.14(e)), since $I$ is supposed to be finite, we have
    $I = I_1 \times \ldots \times I_k \times I_{k+1}$ where each $I_j$ is also a finite
    subset of $\nn$. In particular, $I_{k+1}$ is finite.

    We thus have:
    \begin{align*}
      P(x_1, \ldots, x_k, x_{k+1})
      &:= \sum_{(i_1, \ldots, i_k, i_{k+1}) \in I} c(i_1, \ldots, i_k, i_{k+1}) \,
        x_1^{i_1} \ldots x_k^{i_k} x_{k+1}^{i_{k+1}} \\
      &= \sum_{i_{k+1} \in I_{k+1}} \left(\sum_{(i_1, \ldots, i_k) \in I_1 \times \ldots \times I_k}  c(i_1, \ldots, i_k,
        i_{k+1}) x_1^{i_1} \ldots x_k^{i_k}\right) x_{k+1}^{i_{k+1}}
    \end{align*}
    By the induction hypothesis, the expression enclosed in the
    parentheses is a continuous function. Thus,
    $P(x_1, \ldots, x_{k+1})$ is also continuous, as a (finite) sum and
    product of continuous functions.
  \end{itemize}
\end{exo}

\begin{exo}{13.2.8}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces.
    Define the metric
    $d_{X \times Y} : (X \times Y ) \times (X \times Y) \to [0, \infty[$ by the formula
    \[d_{X \times Y} \left((x, y), (x', y')\right) := d_X (x, x') + d_Y (y,
      y').\] Show that $(X \times Y, d_{X \times Y})$ is a metric space, and
    deduce an analogue of Proposition 12.1.18 and Lemma 13.2.1.}

  \begin{enumerate}
  \item First we prove that $d_{X \times Y}$ is indeed a metric. In all the
    proofs below, we simply use the fact that $d_X$ and $d_Y$ are
    themselves metrics.
    \begin{itemize}
    \item For all $(x, y) \in X \times Y$, we have:
      \[
        d_{X \times Y} ((x,y), (x,y))
        = \underbrace{d_X(x,x)}_{=0} + \underbrace{d_Y(y,y)}_{=0}
        =0.
      \]
    \item For all distinct points $(x,y), (x',y') \in X \times Y$, we have
      \[d_{X \times Y} \left((x,y), (x', y')\right)
        = \underbrace{d_X(x, x')}_{>0} + \underbrace{d_Y(y, y')}_{>0}
        > 0.
      \]
    \item Symmetry: for all $(x,y), (x',y') \in X \times Y$, we have
      \begin{align*}
        d_{X \times Y} \left((x,y), (x', y')\right)
        &= d_X(x,x') + d_Y(y,y')\\
        &= d_X(x',x) + d_Y(y',y)\\
        &= d_{X \times Y}((x', y'), (x, y)).
      \end{align*}
    \item Triangle inequality: for all $(x,y), (x',y'), (x'', y'') \in X
      \times Y$, we have
      \begin{align*}
        &d_{X \times Y} \left((x,y), (x', y')\right) + d_{X \times Y}
        \left((x',y'), (x'', y'')\right)\\
        = \; &d_X(x, x') + d_X(x', x'') + d_Y(y, y') + d_Y(y', y'') \\
        \leq \; &d_X(x, x'') + d_Y(y, y'') \\
        \leq \; &d_{X \times Y}\left((x, x''), (y, y')\right).
      \end{align*}
    \end{itemize}
    Thus, $d_{X \times Y}$ is indeed a metric on $X \times Y$.
    
  \item Now we give an analogue of Proposition 12.1.18. Note that $X$
    and $Y$ are ``abstract'' metric spaces here, and $d_X, d_Y$ also
    are ``abstract'' distances, so that we cannot really give an
    analogue to the notion of the equivalence of metrics
    $d_{l^1}, d_{l^2}, d_{l^\infty}$. However, we can give an analogue of
    the notion of ``component-wise'' convergence. Indeed, let be
    $\seq{a^{(n)}}{1}$ a sequence of elements of $X \times Y$ (so that we
    can write $a^{(n)} := (x^{(n)}, y^{(n)})$ for all $n \geq 1$ with
    obvious notations), and let be $(x_0, y_0)$ a point in
    $X \times Y$. The following two statements are equivalent:
    \begin{enumerate}[label=(\roman*)]
    \item $\seq{a^{(n)}}{1} := \seq{x^{(n)}, y^{(n)}}{1}$ converges to
      $(x_0, y_0)$ with respect to the metric $d_{X \times Y}$
    \item $\seq{x^{(n)}}{1}$ converges to $x_0$ with respect to $d_X$
      and $\seq{y^{(n)}}{1}$ converges to $y_0$ with respect to $d_Y$.
    \end{enumerate}

    \begin{proof}
      First we show that (i) $\implies$ (ii). If
      $\seq{x^{(n)},y^{(n)}}{1}$ converges to $(x_0, y_0)$ with
      respect to $d_{X \times Y}$, then by definition we have
      $\lim_{n \to \infty} d_{X \times Y}((x^{(n)}, y^{(n)}), (x_0, y_0)) = 0$.
      Thus, for any arbitrary $\epsilon > 0$, there exists $N \geq 1$ such that
      for all $n \geq N$ we have
      \begin{align*}
        d_{X \times Y}((x^{(n)}, y^{(n)}), (x_0, y_0)) &< \epsilon \\
        d_X(x^{(n)}, x_0) + d_Y(y^{(n)}, y_0) &< \epsilon.
      \end{align*}
      In particular, we have both $d_X(x^{(n)}, x_0) < \epsilon$ and
      $d_Y(y^{(n)}, y_0) < \epsilon$ whenever $n \geq N$. Thus, we have
      $\lim_{n \to \infty} d_X(x^{(n)}, x_0) = 0$ and
      $\lim_{n \to \infty} d_Y(y^{(n)}, y_0) = 0$, which is precisely the
      statement (ii).

      Now we show that (ii) $\implies$ (i). Let be $\epsilon > 0$. Since we
      have both $\lim_{n \to \infty} d_X(x^{(n)}, x_0) = 0$ and
      $\lim_{n \to \infty} d_Y(y^{(n)}, y_0) = 0$, there exists $N_1, N_2 \geq
      1$ such that $d_X(x^{(n)}, x_0) < \epsilon/2$ whenever $n \geq N_1$, and
      $d_Y(y^{(n)}, y_0) < \epsilon/2$ whenever $n \ge N_2$. Thus, for all $n \geq
      \max(N_1, N_2)$, we have
      \begin{align*}
        d_{X \times Y}((x^{(n)}, y^{(n)}), (x_0, y_0))
        &= d_X(x^{(n)}, x_0) + d_Y(y^{(n)}, y_0) \\
        &< \epsilon/2 + \epsilon/2\\
        &< \epsilon
      \end{align*}
      which means that  $\lim_{n \to \infty} d_{X \times Y}((x^{(n)}, y^{(n)}),
      (x_0, y_0)) = 0$. This is the statement (i).     
    \end{proof}
    
  \item Similarly, we can give an analogue for Lemma 13.2.1:
    
    Let be $S$ an arbitrary domain for the functions $f : S \to X$ and
    $g : S \to Y$. Consider the metric spaces $(X, d_X)$, $(Y, d_Y)$ and
    $(X \times Y, d_{X \times Y})$. Then the function
    $f \oplus g : S \to X \times Y$ is continuous at $s_0 \in S$ iff both
    $f$ and $g$ are continuous at $s_0$.

    \begin{proof}
      Suppose that both $f$ and $g$ are continuous at $s_0 \in S$. Let be
      $\epsilon > 0$, and let be $\seq{s^{(n)}}{1}$ a sequence of elements of $S$
      that converges to $s_0$. By definition, there exists $N_1 \geq 1$
      such that $d_X(f(s^{(n)}), f(s_0)) < \epsilon/2$ whenever $n \geq N_1$;
      and there exists $N_2 \geq 1$ such that $d_Y(g(s^{(n)}), g(s_0)) <
      \epsilon/2$ whenever $n \geq N_2$. Thus, for $n \geq N := \max(N_1, N_2)$,
      \[
        d_{X \times Y}(f \oplus g(s^{(n)}), f \oplus g(s_0))
        := \underbrace{d_X(f(s^{(n)}), f(s_0))}_{< \epsilon/2}
          + \underbrace{d_Y(g(s^{(n)}), g(s_0))}_{< \epsilon/2}
        < \epsilon
      \]
      which means that $f \oplus g$ is continuous at $s_0$, as expected.

      The converse statement can be proved similarly (see also the
      proof of 2. above).
    \end{proof}
  \end{enumerate}
\end{exo}

\begin{exo}{13.2.10}{Let $f : \rr^2 \to \rr$ be a continuous function.
    Show that for each $x \in \rr$, the function $y \mapsto f(x, y)$ is
    continuous on $\rr$, and for each $y \in \rr$, the function
    $x \mapsto f (x, y)$ is continuous on $\rr$. Thus a function
    $f(x, y)$ which is jointly continuous in $(x, y)$ is also
    continuous in each variable $x, y$ separately.}

  Let be $x_0 \in \rr$ a real number. We will just prove the result for
  the function $g : y \mapsto f(x_0, y)$, since the result for the other
  function can be shown in the same way. We will work below with the
  metric space $(\rr^2, d_{l^2})$ but the proof can easily be adapted
  for the metrics $d_{l^1}$ or $d_{l^\infty}$.

  Let be $y_0 \in \rr$ an arbitrary real number: we have to prove that
  $g$ is continuous at $y_0$. Let be $\epsilon > 0$ an arbitrary positive
  real number. Since $f$ is continuous on $\rr^2$, it is continuous in
  particular at $(x_0, y_0) \in \rr^2$. Thus, there exists a
  $\delta > 0$ such that $|f(x, y) - f(x_0, y_0)| < \epsilon$ whenever
  $d((x,y), (x_0, y_0)) < \delta$.

  If we set $x := x_0$ and choose any $y \in \rr$ such that
  $|y-y_0| < \delta$, we will have
  $d((x_0, y), (x_0, y_0)) = |y-y_0| < \delta$. Thus, we will have
  $\epsilon > |f(x_0, y) - f(x_0, y_0)| = |g(y) - g(y_0)|$. This means that
  $g$ is continuous at $y_0$, as expected.
\end{exo}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
